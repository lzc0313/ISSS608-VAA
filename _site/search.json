[
  {
    "objectID": "Take-home_Ex/Take-home_ex03/Take-home_ex3.html",
    "href": "Take-home_Ex/Take-home_ex03/Take-home_ex3.html",
    "title": "Take Home Exercise 3: Be Weatherwise or Otherwise",
    "section": "",
    "text": "According to an office report as shown in the infographic below,\n\nDaily mean temperature are projected to increase by 1.4 to 4.6, and\nThe contrast between the wet months (November to January) and dry month (February and June to September) is likely to be more pronounced.\n\n\n\n\nAs a visual analytics greenhorn, you are keen to apply newly acquired visual interactivity and visualising uncertainty methods to validate the claims presented above.\n\n\n\n\nIn this take-home exercise, you are required to:\n\nSelect a weather station and download historical daily temperature or rainfall data from Meteorological Service Singapore website,\nSelect either daily temperature or rainfall records of a month of the year 1983, 1993, 2003, 2013 and 2023 and create an analytics-driven data visualisation,\nApply appropriate interactive techniques to enhance the user experience in data discovery and/or visual story-telling.\n\nFrom the above requirements for this exercise, i have selected the Changi weather station, and downloaded the records of December of the year 1983, 1993, 2003, 2013 and 2023.\n\n\n\n\n\nThe code chunk below are the R packages used in this exercise:\n\npacman::p_load(readxl, ggiraph, plotly, \n               patchwork, DT, tidyverse, gifski, gapminder, gganimate, ggplot2,lubridate, transformr) \n\n\n\n\nBased on the objective, the temperature records from Changi weather station is used from (link), from Dec of the year 1983, 1993, 2003, 2013 and 2023.\n\nDec_temp &lt;- c(\"data/DAILYDATA_S24_198312.csv\", \"data/DAILYDATA_S24_199312.csv\", \n                \"data/DAILYDATA_S24_200312.csv\", \"data/DAILYDATA_S24_201312.csv\", \n                \"data/DAILYDATA_S24_202312.csv\")\n\nlist_of_temp &lt;- lapply(Dec_temp, read.csv)\n\ntemp_combined &lt;- do.call(rbind, list_of_temp)\n\nAfter examine the list_of_temp data, for the purpose of the exercise the five data sets are combined and named temp_combined before analyzing."
  },
  {
    "objectID": "Take-home_Ex/Take-home_ex03/Take-home_ex3.html#objective",
    "href": "Take-home_Ex/Take-home_ex03/Take-home_ex3.html#objective",
    "title": "Take Home Exercise 3: Be Weatherwise or Otherwise",
    "section": "",
    "text": "In this take-home exercise, you are required to:\n\nSelect a weather station and download historical daily temperature or rainfall data from Meteorological Service Singapore website,\nSelect either daily temperature or rainfall records of a month of the year 1983, 1993, 2003, 2013 and 2023 and create an analytics-driven data visualisation,\nApply appropriate interactive techniques to enhance the user experience in data discovery and/or visual story-telling.\n\nFrom the above requirements for this exercise, i have selected the Changi weather station, and downloaded the records of December of the year 1983, 1993, 2003, 2013 and 2023."
  },
  {
    "objectID": "Take-home_Ex/Take-home_ex03/Take-home_ex3.html#getting-started",
    "href": "Take-home_Ex/Take-home_ex03/Take-home_ex3.html#getting-started",
    "title": "Take Home Exercise 3: Be Weatherwise or Otherwise",
    "section": "",
    "text": "The code chunk below are the R packages used in this exercise:\n\npacman::p_load(readxl, ggiraph, plotly, \n               patchwork, DT, tidyverse, gifski, gapminder, gganimate, ggplot2,lubridate, transformr) \n\n\n\n\nBased on the objective, the temperature records from Changi weather station is used from (link), from Dec of the year 1983, 1993, 2003, 2013 and 2023.\n\nDec_temp &lt;- c(\"data/DAILYDATA_S24_198312.csv\", \"data/DAILYDATA_S24_199312.csv\", \n                \"data/DAILYDATA_S24_200312.csv\", \"data/DAILYDATA_S24_201312.csv\", \n                \"data/DAILYDATA_S24_202312.csv\")\n\nlist_of_temp &lt;- lapply(Dec_temp, read.csv)\n\ntemp_combined &lt;- do.call(rbind, list_of_temp)\n\nAfter examine the list_of_temp data, for the purpose of the exercise the five data sets are combined and named temp_combined before analyzing."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hand-on_Ex06.html",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hand-on_Ex06.html",
    "title": "Hand-on exercise 6 Visual Analytics",
    "section": "",
    "text": "By the end of this hands-on exercise you will be able create the followings data visualisation by using R packages:\n\nplotting a calender heatmap by using ggplot2 functions,\nplotting a cycle plot by using ggplot2 function,\nplotting a slopegraph\nplotting a horizon chart\n\n\n\n\n\n\n\nWrite a code chunk to check, install and launch the following R packages: scales, viridis, lubridate, ggthemes, gridExtra, readxl, knitr, data.table and tidyverse.\n\npacman::p_load(scales, viridis, lubridate, ggthemes, gridExtra, readxl, knitr, data.table, CGPfunctions, ggHoriPlot, tidyverse)\n\n\n\n\nIn this section, you will learn how to plot a calender heatmap programmatically by using ggplot2 package.\n\n\n\nBy the end of this section, you will be able to:\n\n\n\nplot a calender heatmap by using ggplot2 functions and extension,\nto write function using R programming,\nto derive specific date and time related field by using base R and lubridate packages\nto perform data preparation task by using tidyr and dplyr packages.\n\n\n\nFor the purpose of this hands-on exercise, eventlog.csv file will be used. This data file consists of 199,999 rows of time-series cyber attack records by country.\n\n\n\nFirst, you will use the code chunk below to import eventlog.csv file into R environment and called the data frame as attacks.\n\nattacks &lt;- read_csv(\"data/eventlog.csv\")\n\n\n\n\nIt is always a good practice to examine the imported data frame before further analysis is performed.\nFor example, kable() can be used to review the structure of the imported data frame.\n\nkable(head(attacks))\n\n\n\n\ntimestamp\nsource_country\ntz\n\n\n\n\n2015-03-12 15:59:16\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:00:48\nFR\nEurope/Paris\n\n\n2015-03-12 16:02:26\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:02:38\nUS\nAmerica/Chicago\n\n\n2015-03-12 16:03:22\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:03:45\nCN\nAsia/Shanghai\n\n\n\n\n\nThere are three columns, namely timestamp, source_country and tz.\n\ntimestamp field stores date-time values in POSIXct format.\nsource_country field stores the source of the attack. It is in ISO 3166-1 alpha-2 country code.\ntz field stores time zone of the source IP address.\n\n\n\n\nStep 1: Deriving weekday and hour of day fields\nBefore we can plot the calender heatmap, two new fields namely wkday and hour need to be derived. In this step, we will write a function to perform the task.\n\nmake_hr_wkday &lt;- function(ts, sc, tz) {\n  real_times &lt;- ymd_hms(ts, \n                        tz = tz[1], \n                        quiet = TRUE)\n  dt &lt;- data.table(source_country = sc,\n                   wkday = weekdays(real_times),\n                   hour = hour(real_times))\n  return(dt)\n  }\n\n\n\n\n\n\n\nImportant\n\n\n\nNote\n\nymd_hms() and hour() are from lubridate package, and\nweekdays() is a base R function.\n\n\n\nStep 2: Deriving the attacks tibble data frame\n\nwkday_levels &lt;- c('Saturday', 'Friday', \n                  'Thursday', 'Wednesday', \n                  'Tuesday', 'Monday', \n                  'Sunday')\n\nattacks &lt;- attacks %&gt;%\n  group_by(tz) %&gt;%\n  do(make_hr_wkday(.$timestamp, \n                   .$source_country, \n                   .$tz)) %&gt;% \n  ungroup() %&gt;% \n  mutate(wkday = factor(\n    wkday, levels = wkday_levels),\n    hour  = factor(\n      hour, levels = 0:23))\n\n\n\n\n\n\n\nImportant\n\n\n\nNote\nBeside extracting the necessary data into attacks data frame, mutate() of dplyr package is used to convert wkday and hour fields into factor so they’ll be ordered when plotting\n\n\nTable below shows the tidy tibble table after processing.\n\nkable(head(attacks))\n\n\n\n\ntz\nsource_country\nwkday\nhour\n\n\n\n\nAfrica/Cairo\nBG\nSaturday\n20\n\n\nAfrica/Cairo\nTW\nSunday\n6\n\n\nAfrica/Cairo\nTW\nSunday\n8\n\n\nAfrica/Cairo\nCN\nSunday\n11\n\n\nAfrica/Cairo\nUS\nSunday\n15\n\n\nAfrica/Cairo\nCA\nMonday\n11\n\n\n\n\n\n\n\n\n\ngrouped &lt;- attacks %&gt;% \n  count(wkday, hour) %&gt;% \n  ungroup() %&gt;%\n  na.omit()\n\nggplot(grouped, \n       aes(hour, \n           wkday, \n           fill = n)) + \ngeom_tile(color = \"white\", \n          size = 0.1) + \ntheme_tufte(base_family = \"Helvetica\") + \ncoord_equal() +\nscale_fill_gradient(name = \"# of attacks\",\n                    low = \"sky blue\", \n                    high = \"dark blue\") +\nlabs(x = NULL, \n     y = NULL, \n     title = \"Attacks by weekday and time of day\") +\ntheme(axis.ticks = element_blank(),\n      plot.title = element_text(hjust = 0.5),\n      legend.title = element_text(size = 8),\n      legend.text = element_text(size = 6) )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThings to learn from the code chunk\n\na tibble data table called grouped is derived by aggregating the attack by wkday and hour fields.\na new field called n is derived by using group_by() and count() functions.\nna.omit() is used to exclude missing value.\ngeom_tile() is used to plot tiles (grids) at each x and y position. color and size arguments are used to specify the border color and line size of the tiles.\ntheme_tufte() of ggthemes package is used to remove unnecessary chart junk. To learn which visual components of default ggplot2 have been excluded, you are encouraged to comment out this line to examine the default plot.\ncoord_equal() is used to ensure the plot will have an aspect ratio of 1:1.\nscale_fill_gradient() function is used to creates a two colour gradient (low-high).\n\n\n\nThen we can simply group the count by hour and wkday and plot it, since we know that we have values for every combination there’s no need to further preprocess the data.\n\n\n\nChallenge: Building multiple heatmaps for the top four countries with the highest number of attacks.\n\n\n\nStep 1: Deriving attack by country object\nIn order to identify the top 4 countries with the highest number of attacks, you are required to do the followings:\n\ncount the number of attacks by country,\ncalculate the percent of attackes by country, and\nsave the results in a tibble data frame.\n\nattacks_by_country &lt;- count(\n  attacks, source_country) %&gt;%\n  mutate(percent = percent(n/sum(n))) %&gt;%\n  arrange(desc(n))\n\n\nStep 2: Preparing the tidy data frame\nIn this step, you are required to extract the attack records of the top 4 countries from attacks data frame and save the data in a new tibble data frame (i.e. top4_attacks).\n\ntop4 &lt;- attacks_by_country$source_country[1:4]\ntop4_attacks &lt;- attacks %&gt;%\n  filter(source_country %in% top4) %&gt;%\n  count(source_country, wkday, hour) %&gt;%\n  ungroup() %&gt;%\n  mutate(source_country = factor(\n    source_country, levels = top4)) %&gt;%\n  na.omit()\n\n\n\n\nStep 3: Plotting the Multiple Calender Heatmap by using ggplot2 package.\n\nggplot(top4_attacks, \n       aes(hour, \n           wkday, \n           fill = n)) + \n  geom_tile(color = \"white\", \n          size = 0.1) + \n  theme_tufte(base_family = \"Helvetica\") + \n  coord_equal() +\n  scale_fill_gradient(name = \"# of attacks\",\n                    low = \"sky blue\", \n                    high = \"dark blue\") +\n  facet_wrap(~source_country, ncol = 2) +\n  labs(x = NULL, y = NULL, \n     title = \"Attacks on top 4 countries by weekday and time of day\") +\n  theme(axis.ticks = element_blank(),\n        axis.text.x = element_text(size = 7),\n        plot.title = element_text(hjust = 0.5),\n        legend.title = element_text(size = 8),\n        legend.text = element_text(size = 6) )\n\n\n\n\n\n\n\n\n\n\n\n\nIn this section, you will learn how to plot a cycle plot showing the time-series patterns and trend of visitor arrivals from Vietnam programmatically by using ggplot2 functions.\n\n\nFor the purpose of this hands-on exercise, arrivals_by_air.xlsx will be used.\nThe code chunk below imports arrivals_by_air.xlsx by using read_excel() of readxl package and save it as a tibble data frame called air.\n\nair &lt;- read_excel(\"data/arrivals_by_air.xlsx\")\n\n\n\n\nNext, two new fields called month and year are derived from Month-Year field.\n\nair$month &lt;- factor(month(air$`Month-Year`), \n                    levels=1:12, \n                    labels=month.abb, \n                    ordered=TRUE) \nair$year &lt;- year(ymd(air$`Month-Year`))\n\n\n\n\nNext, the code chunk below is use to extract data for the target country (i.e. Vietnam)\n\nVietnam &lt;- air %&gt;% \n  select(`Vietnam`, \n         month, \n         year) %&gt;%\n  filter(year &gt;= 2010)\n\n\n\n\nThe code chunk below uses group_by() and summarise() of dplyr to compute year average arrivals by month.\n\nhline.data &lt;- Vietnam %&gt;% \n  group_by(month) %&gt;%\n  summarise(avgvalue = mean(`Vietnam`))\n\n\n\n\nThe code chunk below is used to plot the cycle plot as shown in Slide 12/23.\n\nggplot() + \n  geom_line(data=Vietnam,\n            aes(x=year, \n                y=`Vietnam`, \n                group=month), \n            colour=\"black\") +\n  geom_hline(aes(yintercept=avgvalue), \n             data=hline.data, \n             linetype=6, \n             colour=\"red\", \n             size=0.5) + \n  facet_grid(~month) +\n  labs(axis.text.x = element_blank(),\n       title = \"Visitor arrivals from Vietnam by air, Jan 2010-Dec 2019\") +\n  xlab(\"\") +\n  ylab(\"No. of Visitors\") +\n  theme_tufte(base_family = \"Helvetica\")\n\n\n\n\n\n\n\n\n\n\n\n\nIn this section you will learn how to plot a slopegraph by using R.\nBefore getting start, make sure that CGPfunctions has been installed and loaded onto R environment. Then, refer to Using newggslopegraph to learn more about the function. Lastly, read more about newggslopegraph() and its arguments by referring to this link.\n\n\nImport the rice data set into R environment by using the code chunk below.\n\nrice &lt;- read_csv(\"data/rice.csv\")\n\n\n\n\nNext, code chunk below will be used to plot a basic slopegraph as shown below.\n\nrice %&gt;% \n  mutate(Year = factor(Year)) %&gt;%\n  filter(Year %in% c(1961, 1980)) %&gt;%\n  newggslopegraph(Year, Yield, Country,\n                Title = \"Rice Yield of Top 11 Asian Counties\",\n                SubTitle = \"1961-1980\",\n                Caption = \"Prepared by: Dr. Kam Tin Seong\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThing to learn from the code chunk above\nFor effective data visualisation design, factor() is used convert the value type of Year field from numeric to factor."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hand-on_Ex06.html#learning-outcome",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hand-on_Ex06.html#learning-outcome",
    "title": "Hand-on exercise 6 Visual Analytics",
    "section": "",
    "text": "By the end of this hands-on exercise you will be able create the followings data visualisation by using R packages:\n\nplotting a calender heatmap by using ggplot2 functions,\nplotting a cycle plot by using ggplot2 function,\nplotting a slopegraph\nplotting a horizon chart"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hand-on_Ex06.html#do-it-yourself",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hand-on_Ex06.html#do-it-yourself",
    "title": "Hand-on exercise 6 Visual Analytics",
    "section": "",
    "text": "Write a code chunk to check, install and launch the following R packages: scales, viridis, lubridate, ggthemes, gridExtra, readxl, knitr, data.table and tidyverse.\n\npacman::p_load(scales, viridis, lubridate, ggthemes, gridExtra, readxl, knitr, data.table, CGPfunctions, ggHoriPlot, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hand-on_Ex06.html#plotting-calendar-heatmap",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hand-on_Ex06.html#plotting-calendar-heatmap",
    "title": "Hand-on exercise 6 Visual Analytics",
    "section": "",
    "text": "In this section, you will learn how to plot a calender heatmap programmatically by using ggplot2 package.\n\n\n\nBy the end of this section, you will be able to:\n\n\n\nplot a calender heatmap by using ggplot2 functions and extension,\nto write function using R programming,\nto derive specific date and time related field by using base R and lubridate packages\nto perform data preparation task by using tidyr and dplyr packages.\n\n\n\nFor the purpose of this hands-on exercise, eventlog.csv file will be used. This data file consists of 199,999 rows of time-series cyber attack records by country.\n\n\n\nFirst, you will use the code chunk below to import eventlog.csv file into R environment and called the data frame as attacks.\n\nattacks &lt;- read_csv(\"data/eventlog.csv\")\n\n\n\n\nIt is always a good practice to examine the imported data frame before further analysis is performed.\nFor example, kable() can be used to review the structure of the imported data frame.\n\nkable(head(attacks))\n\n\n\n\ntimestamp\nsource_country\ntz\n\n\n\n\n2015-03-12 15:59:16\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:00:48\nFR\nEurope/Paris\n\n\n2015-03-12 16:02:26\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:02:38\nUS\nAmerica/Chicago\n\n\n2015-03-12 16:03:22\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:03:45\nCN\nAsia/Shanghai\n\n\n\n\n\nThere are three columns, namely timestamp, source_country and tz.\n\ntimestamp field stores date-time values in POSIXct format.\nsource_country field stores the source of the attack. It is in ISO 3166-1 alpha-2 country code.\ntz field stores time zone of the source IP address.\n\n\n\n\nStep 1: Deriving weekday and hour of day fields\nBefore we can plot the calender heatmap, two new fields namely wkday and hour need to be derived. In this step, we will write a function to perform the task.\n\nmake_hr_wkday &lt;- function(ts, sc, tz) {\n  real_times &lt;- ymd_hms(ts, \n                        tz = tz[1], \n                        quiet = TRUE)\n  dt &lt;- data.table(source_country = sc,\n                   wkday = weekdays(real_times),\n                   hour = hour(real_times))\n  return(dt)\n  }\n\n\n\n\n\n\n\nImportant\n\n\n\nNote\n\nymd_hms() and hour() are from lubridate package, and\nweekdays() is a base R function.\n\n\n\nStep 2: Deriving the attacks tibble data frame\n\nwkday_levels &lt;- c('Saturday', 'Friday', \n                  'Thursday', 'Wednesday', \n                  'Tuesday', 'Monday', \n                  'Sunday')\n\nattacks &lt;- attacks %&gt;%\n  group_by(tz) %&gt;%\n  do(make_hr_wkday(.$timestamp, \n                   .$source_country, \n                   .$tz)) %&gt;% \n  ungroup() %&gt;% \n  mutate(wkday = factor(\n    wkday, levels = wkday_levels),\n    hour  = factor(\n      hour, levels = 0:23))\n\n\n\n\n\n\n\nImportant\n\n\n\nNote\nBeside extracting the necessary data into attacks data frame, mutate() of dplyr package is used to convert wkday and hour fields into factor so they’ll be ordered when plotting\n\n\nTable below shows the tidy tibble table after processing.\n\nkable(head(attacks))\n\n\n\n\ntz\nsource_country\nwkday\nhour\n\n\n\n\nAfrica/Cairo\nBG\nSaturday\n20\n\n\nAfrica/Cairo\nTW\nSunday\n6\n\n\nAfrica/Cairo\nTW\nSunday\n8\n\n\nAfrica/Cairo\nCN\nSunday\n11\n\n\nAfrica/Cairo\nUS\nSunday\n15\n\n\nAfrica/Cairo\nCA\nMonday\n11\n\n\n\n\n\n\n\n\n\ngrouped &lt;- attacks %&gt;% \n  count(wkday, hour) %&gt;% \n  ungroup() %&gt;%\n  na.omit()\n\nggplot(grouped, \n       aes(hour, \n           wkday, \n           fill = n)) + \ngeom_tile(color = \"white\", \n          size = 0.1) + \ntheme_tufte(base_family = \"Helvetica\") + \ncoord_equal() +\nscale_fill_gradient(name = \"# of attacks\",\n                    low = \"sky blue\", \n                    high = \"dark blue\") +\nlabs(x = NULL, \n     y = NULL, \n     title = \"Attacks by weekday and time of day\") +\ntheme(axis.ticks = element_blank(),\n      plot.title = element_text(hjust = 0.5),\n      legend.title = element_text(size = 8),\n      legend.text = element_text(size = 6) )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThings to learn from the code chunk\n\na tibble data table called grouped is derived by aggregating the attack by wkday and hour fields.\na new field called n is derived by using group_by() and count() functions.\nna.omit() is used to exclude missing value.\ngeom_tile() is used to plot tiles (grids) at each x and y position. color and size arguments are used to specify the border color and line size of the tiles.\ntheme_tufte() of ggthemes package is used to remove unnecessary chart junk. To learn which visual components of default ggplot2 have been excluded, you are encouraged to comment out this line to examine the default plot.\ncoord_equal() is used to ensure the plot will have an aspect ratio of 1:1.\nscale_fill_gradient() function is used to creates a two colour gradient (low-high).\n\n\n\nThen we can simply group the count by hour and wkday and plot it, since we know that we have values for every combination there’s no need to further preprocess the data.\n\n\n\nChallenge: Building multiple heatmaps for the top four countries with the highest number of attacks.\n\n\n\nStep 1: Deriving attack by country object\nIn order to identify the top 4 countries with the highest number of attacks, you are required to do the followings:\n\ncount the number of attacks by country,\ncalculate the percent of attackes by country, and\nsave the results in a tibble data frame.\n\nattacks_by_country &lt;- count(\n  attacks, source_country) %&gt;%\n  mutate(percent = percent(n/sum(n))) %&gt;%\n  arrange(desc(n))\n\n\nStep 2: Preparing the tidy data frame\nIn this step, you are required to extract the attack records of the top 4 countries from attacks data frame and save the data in a new tibble data frame (i.e. top4_attacks).\n\ntop4 &lt;- attacks_by_country$source_country[1:4]\ntop4_attacks &lt;- attacks %&gt;%\n  filter(source_country %in% top4) %&gt;%\n  count(source_country, wkday, hour) %&gt;%\n  ungroup() %&gt;%\n  mutate(source_country = factor(\n    source_country, levels = top4)) %&gt;%\n  na.omit()\n\n\n\n\nStep 3: Plotting the Multiple Calender Heatmap by using ggplot2 package.\n\nggplot(top4_attacks, \n       aes(hour, \n           wkday, \n           fill = n)) + \n  geom_tile(color = \"white\", \n          size = 0.1) + \n  theme_tufte(base_family = \"Helvetica\") + \n  coord_equal() +\n  scale_fill_gradient(name = \"# of attacks\",\n                    low = \"sky blue\", \n                    high = \"dark blue\") +\n  facet_wrap(~source_country, ncol = 2) +\n  labs(x = NULL, y = NULL, \n     title = \"Attacks on top 4 countries by weekday and time of day\") +\n  theme(axis.ticks = element_blank(),\n        axis.text.x = element_text(size = 7),\n        plot.title = element_text(hjust = 0.5),\n        legend.title = element_text(size = 8),\n        legend.text = element_text(size = 6) )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hand-on_Ex06.html#plotting-cycle-plot",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hand-on_Ex06.html#plotting-cycle-plot",
    "title": "Hand-on exercise 6 Visual Analytics",
    "section": "",
    "text": "In this section, you will learn how to plot a cycle plot showing the time-series patterns and trend of visitor arrivals from Vietnam programmatically by using ggplot2 functions.\n\n\nFor the purpose of this hands-on exercise, arrivals_by_air.xlsx will be used.\nThe code chunk below imports arrivals_by_air.xlsx by using read_excel() of readxl package and save it as a tibble data frame called air.\n\nair &lt;- read_excel(\"data/arrivals_by_air.xlsx\")\n\n\n\n\nNext, two new fields called month and year are derived from Month-Year field.\n\nair$month &lt;- factor(month(air$`Month-Year`), \n                    levels=1:12, \n                    labels=month.abb, \n                    ordered=TRUE) \nair$year &lt;- year(ymd(air$`Month-Year`))\n\n\n\n\nNext, the code chunk below is use to extract data for the target country (i.e. Vietnam)\n\nVietnam &lt;- air %&gt;% \n  select(`Vietnam`, \n         month, \n         year) %&gt;%\n  filter(year &gt;= 2010)\n\n\n\n\nThe code chunk below uses group_by() and summarise() of dplyr to compute year average arrivals by month.\n\nhline.data &lt;- Vietnam %&gt;% \n  group_by(month) %&gt;%\n  summarise(avgvalue = mean(`Vietnam`))\n\n\n\n\nThe code chunk below is used to plot the cycle plot as shown in Slide 12/23.\n\nggplot() + \n  geom_line(data=Vietnam,\n            aes(x=year, \n                y=`Vietnam`, \n                group=month), \n            colour=\"black\") +\n  geom_hline(aes(yintercept=avgvalue), \n             data=hline.data, \n             linetype=6, \n             colour=\"red\", \n             size=0.5) + \n  facet_grid(~month) +\n  labs(axis.text.x = element_blank(),\n       title = \"Visitor arrivals from Vietnam by air, Jan 2010-Dec 2019\") +\n  xlab(\"\") +\n  ylab(\"No. of Visitors\") +\n  theme_tufte(base_family = \"Helvetica\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hand-on_Ex06.html#plotting-slopegraph",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hand-on_Ex06.html#plotting-slopegraph",
    "title": "Hand-on exercise 6 Visual Analytics",
    "section": "",
    "text": "In this section you will learn how to plot a slopegraph by using R.\nBefore getting start, make sure that CGPfunctions has been installed and loaded onto R environment. Then, refer to Using newggslopegraph to learn more about the function. Lastly, read more about newggslopegraph() and its arguments by referring to this link.\n\n\nImport the rice data set into R environment by using the code chunk below.\n\nrice &lt;- read_csv(\"data/rice.csv\")\n\n\n\n\nNext, code chunk below will be used to plot a basic slopegraph as shown below.\n\nrice %&gt;% \n  mutate(Year = factor(Year)) %&gt;%\n  filter(Year %in% c(1961, 1980)) %&gt;%\n  newggslopegraph(Year, Yield, Country,\n                Title = \"Rice Yield of Top 11 Asian Counties\",\n                SubTitle = \"1961-1980\",\n                Caption = \"Prepared by: Dr. Kam Tin Seong\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThing to learn from the code chunk above\nFor effective data visualisation design, factor() is used convert the value type of Year field from numeric to factor."
  },
  {
    "objectID": "Take-home_Ex/Take-home_ex02/Take-home_ex2.html",
    "href": "Take-home_Ex/Take-home_ex02/Take-home_ex2.html",
    "title": "Take Home Exercise 2: DataVis Makeover",
    "section": "",
    "text": "For this Data Visualization Makeover exercise, i have choose my classmates Xu Lin’s work of his take home exercise 1, and i will try to make improvements on his work so that it can be an better visual analysis."
  },
  {
    "objectID": "Take-home_Ex/Take-home_ex02/Take-home_ex2.html#objective",
    "href": "Take-home_Ex/Take-home_ex02/Take-home_ex2.html#objective",
    "title": "Take Home Exercise 2: DataVis Makeover",
    "section": "Objective",
    "text": "Objective\nFor this exercise the objective is to look at the distribution of Singapore students’ performance in mathematics, reading, and science, and the relationship between these subjects performances with schools, gender and socioeconomic status of the students."
  },
  {
    "objectID": "Take-home_Ex/Take-home_ex02/Take-home_ex2.html#methods",
    "href": "Take-home_Ex/Take-home_ex02/Take-home_ex2.html#methods",
    "title": "Take Home Exercise 2: DataVis Makeover",
    "section": "Methods",
    "text": "Methods\nFor this Analysis, the average score of mathematics, reading, and science is used as benchmark to do the analysis and visualization. and also for schools, gender and socioeconomic status variables."
  },
  {
    "objectID": "Take-home_Ex/Take-home_ex02/Take-home_ex2.html#limitations",
    "href": "Take-home_Ex/Take-home_ex02/Take-home_ex2.html#limitations",
    "title": "Take Home Exercise 2: DataVis Makeover",
    "section": "Limitations",
    "text": "Limitations\nThe mean serves as merely one benchmark and might not be universally applicable due to its sensitivity to extreme values, which can skew the representation of a data set’s central tendency. Moreover, our choice of data is informed by subjective interpretation, potentially leading to biased datasets because it reflects our preconceptions and possible exclusion of relevant data points."
  },
  {
    "objectID": "Take-home_Ex/Take-home_ex02/Take-home_ex2.html#install-r-package",
    "href": "Take-home_Ex/Take-home_ex02/Take-home_ex2.html#install-r-package",
    "title": "Take Home Exercise 2: DataVis Makeover",
    "section": "Install R package",
    "text": "Install R package\nInstalling necessary R packages that will be needed for this exercise. To load the requried packages the code chunk below use pacman::p_load() function is used to unsure that the packages are load to the current R work environment.\n\npacman::p_load(ggrepel, patchwork, \n               ggthemes, hrbrthemes,\n               tidyverse, haven, ggplot2, dplyr,ggstatsplot,tidyr)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_ex02/Take-home_ex2.html#import-data",
    "href": "Take-home_Ex/Take-home_ex02/Take-home_ex2.html#import-data",
    "title": "Take Home Exercise 2: DataVis Makeover",
    "section": "Import Data",
    "text": "Import Data\nFor this exercise, we are only examining the Singapore students. So the data load in the below code chunk only contains\n\nstu_qqq_SG &lt;- readRDS(\"C:/lzc0313/ISSS608-VAA/In-class_Ex/In-class_ex1/data/stu_qqq_SG.rds\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_ex02/Take-home_ex2.html#prepare-data",
    "href": "Take-home_Ex/Take-home_ex02/Take-home_ex2.html#prepare-data",
    "title": "Take Home Exercise 2: DataVis Makeover",
    "section": "Prepare Data",
    "text": "Prepare Data\nAfter examing the data, found that the subject scores are named as PV1MATH to PV10MATCH and also same for other two subjects. The below code chunks calculates the average of PV1 to PV10 for all three subject.\n\nstu_qqq_SG_MRS &lt;- stu_qqq_SG %&gt;%\n  mutate(AVEMATH = (PV1MATH + PV2MATH + PV3MATH + PV4MATH + PV5MATH + PV6MATH + PV7MATH + PV8MATH + PV9MATH + PV10MATH ) / 10,\n         AVEREAD = (PV1READ + PV2READ + PV3READ + PV4READ + PV5READ + PV6READ + PV7READ + PV8READ + PV9READ + PV10READ )/ 10,\n         AVESCIE = (PV1SCIE + PV2SCIE + PV3SCIE + PV4SCIE + PV5SCIE + PV6SCIE + PV7SCIE + PV8SCIE + PV9SCIE + PV10SCIE )/ 10)"
  },
  {
    "objectID": "In-class_Ex/In-class_ex1/In-class_ex1.html",
    "href": "In-class_Ex/In-class_ex1/In-class_ex1.html",
    "title": "In-class_ex1",
    "section": "",
    "text": "This is the In Class Exercise 1 Lets GOoOoOoO!"
  },
  {
    "objectID": "In-class_Ex/In-class_ex1/In-class_ex1.html#loading-r-packages",
    "href": "In-class_Ex/In-class_ex1/In-class_ex1.html#loading-r-packages",
    "title": "In-class_ex1",
    "section": "Loading R packages",
    "text": "Loading R packages\nIn this in-class exercise, two R packages will be used.\nThey are:\n\ntidyverse,\nhaven\n\nThe code chunk used is as follows:\n\npacman::p_load(tidyverse,haven)\n\n##Import PISA Data\nThe code chunk below uses read_sas()of haven to import PISA data in R environment.\n\nstu_qqq &lt;-read_sas(\"data/cy08msp_stu_qqq.sas7bdat\")\n\n\nstu_qqq_SG &lt;- stu_qqq %&gt;% \n  filter(CNT == \"SGP\")\n\n\nwrite_rds(stu_qqq_SG,\n          \"data/stu_qqq_SG.rds\")\n\n\nstu_qqq_SG &lt;- \n  read_rds(\"data/stu_qqq_SG.rds\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hand-on_Ex04.html",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hand-on_Ex04.html",
    "title": "Hands-on Exercise 4 Visual Analytics",
    "section": "",
    "text": "Visualising distribution is not new in statistical analysis. In chapter 1 we have shared with you some of the popular statistical graphics methods for visualising distribution are histogram, probability density curve (pdf), boxplot, notch plot and violin plot and how they can be created by using ggplot2. In this chapter, we are going to share with you two relatively new statistical graphic methods for visualising distribution, namely ridgeline plot and raincloud plot by using ggplot2 and its extensions."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hand-on_Ex04.html#learning-outcome",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hand-on_Ex04.html#learning-outcome",
    "title": "Hands-on Exercise 4 Visual Analytics",
    "section": "",
    "text": "Visualising distribution is not new in statistical analysis. In chapter 1 we have shared with you some of the popular statistical graphics methods for visualising distribution are histogram, probability density curve (pdf), boxplot, notch plot and violin plot and how they can be created by using ggplot2. In this chapter, we are going to share with you two relatively new statistical graphic methods for visualising distribution, namely ridgeline plot and raincloud plot by using ggplot2 and its extensions."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hand-on_Ex04.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hand-on_Ex04.html#getting-started",
    "title": "Hands-on Exercise 4 Visual Analytics",
    "section": "1.2 Getting Started",
    "text": "1.2 Getting Started\n\n1.2.1 Installing and loading the packages\nFor the purpose of this exercise, the following R packages will be used, they are:\n\ntidyverse, a family of R packages for data science process,\nggridges, a ggplot2 extension specially designed for plotting ridgeline plots, and\nggdist for visualising distribution and uncertainty.\n\n\npacman::p_load(ggdist, ggridges, ggthemes,\n               colorspace, tidyverse)\n\npackage 'distributional' successfully unpacked and MD5 sums checked package 'quadprog' successfully unpacked and MD5 sums checked package 'ggdist' successfully unpacked and MD5 sums checked  The downloaded binary packages are in     C:\\Users\\tskam\\AppData\\Local\\Temp\\RtmpcJX3aB\\downloaded_packages package 'ggridges' successfully unpacked and MD5 sums checked  The downloaded binary packages are in     C:\\Users\\tskam\\AppData\\Local\\Temp\\RtmpcJX3aB\\downloaded_packages\n\n\n1.2.2 Data import\nFor the purpose of this exercise, Exam_data.csv will be used.\n\nexam &lt;- read_csv(\"C:/lzc0313/ISSS608-VAA/Hands-on_Ex/Hands-on_Ex01/data/Exam_data.csv\")\n\n##9.3 Visualising Distribution with Ridgeline Plot Ridgeline plot (sometimes called Joyplot) is a data visualisation technique for revealing the distribution of a numeric value for several groups. Distribution can be represented using histograms or density plots, all aligned to the same horizontal scale and presented with a slight overlap.\nFigure below is a ridgelines plot showing the distribution of English score by class.\n\n\n\n\n\n\nNotes\n\n\n\n\nRidgeline plots make sense when the number of group to represent is medium to high, and thus a classic window separation would take to much space. Indeed, the fact that groups overlap each other allows to use space more efficiently. If you have less than 5 groups, dealing with other distribution plots is probably better.\nIt works well when there is a clear pattern in the result, like if there is an obvious ranking in groups. Otherwise group will tend to overlap each other, leading to a messy plot not providing any insight.\n\n\n\n\n\n1.3.1 Plotting ridgeline graph: ggridges method\nThere are several ways to plot ridgeline plot with R. In this section, you will learn how to plot ridgeline plot by using ggridges package.\nggridges package provides two main geom to plot gridgeline plots, they are: geom_ridgeline() and geom_density_ridges(). The former takes height values directly to draw the ridgelines, and the latter first estimates data densities and then draws those using ridgelines.\nThe ridgeline plot below is plotted by using geom_density_ridges().\n\nggplot(exam, \n       aes(x = ENGLISH, \n           y = CLASS)) +\n  geom_density_ridges(\n    scale = 3,\n    rel_min_height = 0.01,\n    bandwidth = 3.4,\n    fill = lighten(\"#7097BB\", .3),\n    color = \"white\"\n  ) +\n  scale_x_continuous(\n    name = \"English grades\",\n    expand = c(0, 0)\n    ) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat can see from the plot?\n\n\n\n\n3A,3B,3D tend to follow a normal distribution\nlower bond of 3A is slightly lower compare to students in 3B, but at the higher bond 3A students perform much better compare to 3B students.\n3B students tend to be perform similarly well.\n\n\n\n\n\n1.3.2 Varying fill colors along the x axis\nSometimes we would like to have the area under a ridgeline not filled with a single solid color but rather with colors that vary in some form along the x axis. This effect can be achieved by using either geom_ridgeline_gradient() or geom_density_ridges_gradient(). Both geoms work just like geom_ridgeline() and geom_density_ridges(), except that they allow for varying fill colors. However, they do not allow for alpha transparency in the fill. For technical reasons, we can have changing fill colors or transparency but not both.\n\nggplot(exam, \n       aes(x = ENGLISH, \n           y = CLASS,\n           fill = stat(x))) +\n  geom_density_ridges_gradient(\n    scale = 3,\n    rel_min_height = 0.01) +\n  scale_fill_viridis_c(name = \"Temp. [F]\",\n                       option = \"C\") +\n  scale_x_continuous(\n    name = \"English grades\",\n    expand = c(0, 0)\n  ) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges()\n\n\n\n\n\n\n\n\n\n\n1.3.3 Mapping the probabilities directly onto colour\nBeside providing additional geom objects to support the need to plot ridgeline plot, ggridges package also provides a stat function called stat_density_ridges() that replaces stat_density() of ggplot2.\nFigure below is plotted by mapping the probabilities calculated by using stat(ecdf) which represent the empirical cumulative density function for the distribution of English score.\n\nggplot(exam,\n       aes(x = ENGLISH, \n           y = CLASS, \n           fill = 0.5 - abs(0.5-stat(ecdf)))) +\n  stat_density_ridges(geom = \"density_ridges_gradient\", \n                      calc_ecdf = TRUE) +\n  scale_fill_viridis_c(name = \"Tail probability\",\n                       direction = -1) +\n  theme_ridges()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIt is important include the argument calc_ecdf = TRUE in stat_density_ridges().\n\n\n\n\n1.3.4 Ridgeline plots with quantile lines\nBy using geom_density_ridges_gradient(), we can colour the ridgeline plot by quantile, via the calculated stat(quantile) aesthetic as shown in the figure below.\n\nggplot(exam,\n       aes(x = ENGLISH, \n           y = CLASS, \n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  theme_ridges()\n\n\n\n\n\n\n\n\nInstead of using number to define the quantiles, we can also specify quantiles by cut points such as 2.5% and 97.5% tails to colour the ridgeline plot as shown in the figure below.\n\nggplot(exam,\n       aes(x = ENGLISH, \n           y = CLASS, \n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = c(0.025, 0.975)\n    ) +\n  scale_fill_manual(\n    name = \"Probability\",\n    values = c(\"#FF0000A0\", \"#A0A0A0A0\", \"#0000FFA0\"),\n    labels = c(\"(0, 0.025]\", \"(0.025, 0.975]\", \"(0.975, 1]\")\n  ) +\n  theme_ridges()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hand-on_Ex04.html#visualising-distribution-with-raincloud-plot",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hand-on_Ex04.html#visualising-distribution-with-raincloud-plot",
    "title": "Hands-on Exercise 4 Visual Analytics",
    "section": "1.4 Visualising Distribution with Raincloud Plot",
    "text": "1.4 Visualising Distribution with Raincloud Plot\nRaincloud Plot is a data visualisation techniques that produces a half-density to a distribution plot. It gets the name because the density plot is in the shape of a “raincloud”. The raincloud (half-density) plot enhances the traditional box-plot by highlighting multiple modalities (an indicator that groups may exist). The boxplot does not show where densities are clustered, but the raincloud plot does!\nIn this section, you will learn how to create a raincloud plot to visualise the distribution of English score by race. It will be created by using functions provided by ggdist and ggplot2 packages.\n\n1.4.1 Plotting a Half Eye graph\nFirst, we will plot a Half-Eye graph by using stat_halfeye() of ggdist package.\nThis produces a Half Eye visualization, which is contains a half-density and a slab-interval.\n\nggplot(exam, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above\n\n\n\nWe remove the slab interval by setting .width = 0 and point_colour = NA.\n\n\n\n\n1.4.2 Adding the boxplot with geom_boxplot()\nNext, we will add the second geometry layer using geom_boxplot() of ggplot2. This produces a narrow boxplot. We reduce the width and adjust the opacity.\n\nggplot(exam, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               outlier.shape = NA)\n\n\n\n\n\n\n\n\n\n\n1.4.3 Adding the Dot Plots with stat_dots()\nNext, we will add the third geometry layer using stat_dots() of ggdist package. This produces a half-dotplot, which is similar to a histogram that indicates the number of samples (number of dots) in each bin. We select side = “left” to indicate we want it on the left-hand side.\n\nggplot(exam, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               outlier.shape = NA) +\n  stat_dots(side = \"left\", \n            justification = 1.2, \n            binwidth = .5,\n            dotsize = 2)\n\n\n\n\n\n\n\n\n\n\n1.4.4 Finishing touch\nLastly, coord_flip() of ggplot2 package will be used to flip the raincloud chart horizontally to give it the raincloud appearance. At the same time, theme_economist() of ggthemes package is used to give the raincloud chart a professional publishing standard look.\n\nggplot(exam, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               outlier.shape = NA) +\n  stat_dots(side = \"left\", \n            justification = 1.2, \n            binwidth = .5,\n            dotsize = 1.5) +\n  coord_flip() +\n  theme_economist()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hand-on_Ex04.html#learning-outcome-1",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hand-on_Ex04.html#learning-outcome-1",
    "title": "Hands-on Exercise 4 Visual Analytics",
    "section": "2.1 Learning Outcome",
    "text": "2.1 Learning Outcome\nIn this hands-on exercise, you will gain hands-on experience on using:\n\nggstatsplot package to create visual graphics with rich statistical information,\nperformance package to visualise model diagnostics, and\nparameters package to visualise model parameters"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hand-on_Ex04.html#visual-statistical-analysis-with-ggstatsplot",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hand-on_Ex04.html#visual-statistical-analysis-with-ggstatsplot",
    "title": "Hands-on Exercise 4 Visual Analytics",
    "section": "2.2 Visual Statistical Analysis with ggstatsplot",
    "text": "2.2 Visual Statistical Analysis with ggstatsplot\n\nggstatsplot is an extension of ggplot2 package for creating graphics with details from statistical tests included in the information-rich plots themselves.\n\nTo provide alternative statistical inference methods by default.\nTo follow best practices for statistical reporting. For all statistical tests reported in the plots, the default template abides by the APA gold standard for statistical reporting. For example, here are results from a robust t-test:"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hand-on_Ex04.html#getting-started-1",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hand-on_Ex04.html#getting-started-1",
    "title": "Hands-on Exercise 4 Visual Analytics",
    "section": "2.3 Getting Started",
    "text": "2.3 Getting Started\n\n2.3.1 Installing and launching R packages\nIn this exercise, ggstatsplot and tidyverse will be used.\n\npacman::p_load(ggstatsplot, tidyverse,vctrs, dplyr)\n\n\n\n2.3.2 Importing data\n\nexam &lt;- read_csv(\"C:/lzc0313/ISSS608-VAA/Hands-on_Ex/Hands-on_Ex01/data/Exam_data.csv\")\n\n# A tibble: 322 × 7 ID CLASS GENDER RACE ENGLISH MATHS SCIENCE &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Student321 3I Male Malay 21 9 15 2 Student305 3I Female Malay 24 22 16 3 Student289 3H Male Chinese 26 16 16 4 Student227 3F Male Chinese 27 77 31 5 Student318 3I Male Malay 27 11 25 6 Student306 3I Female Malay 31 16 16 7 Student313 3I Male Chinese 31 21 25 8 Student316 3I Male Malay 31 18 27 9 Student312 3I Male Malay 33 19 15 10 Student297 3H Male Indian 34 49 37 # ℹ 312 more rows\n\n\n2.3.3 One-sample test: gghistostats() method\nIn the code chunk below, gghistostats() is used to to build an visual of one-sample test on English scores.\n\nlibrary(dplyr)             # Load the updated dplyr package\n\nlibrary(ggstatsplot)\nset.seed(1234)\n\ngghistostats(\n  data = exam,\n  x = ENGLISH,\n  type = \"bayes\",\n  test.value = 60,\n  xlab = \"English scores\"\n)\n\n\n\n\n\n\n\n\nDefault information: - statistical details - Bayes Factor - sample sizes - distribution summary\n\n\n2.3.4 Unpacking the Bayes Factor\n\nA Bayes factor is the ratio of the likelihood of one particular hypothesis to the likelihood of another. It can be interpreted as a measure of the strength of evidence in favor of one theory among two competing theories.\nThat’s because the Bayes factor gives us a way to evaluate the data in favor of a null hypothesis, and to use external information to do so. It tells us what the weight of the evidence is in favor of a given hypothesis.\nWhen we are comparing two hypotheses, H1 (the alternate hypothesis) and H0 (the null hypothesis), the Bayes Factor is often written as B10. It can be defined mathematically as\n\n\n\nThe Schwarz criterion is one of the easiest ways to calculate rough approximation of the Bayes Factor.\n\n\n\n2.3.5 How to interpret Bayes Factor\nA Bayes Factor can be any positive number. One of the most common interpretations is this one—first proposed by Harold Jeffereys (1961) and slightly modified by Lee and Wagenmakers in 2013:\n\n\n2.3.6 Two-sample mean test: ggbetweenstats()\nIn the code chunk below, ggbetweenstats() is used to build a visual for two-sample mean test of Maths scores by gender.\n\nggbetweenstats(\n  data = exam,\n  x = GENDER, \n  y = MATHS,\n  type = \"np\",\n  messages = FALSE\n)\n\n\n\n\n\n\n\n\nDefault information: - statistical details - Bayes Factor - sample sizes - distribution summary\n\n\n2.3.7 Oneway ANOVA Test: ggbetweenstats() method\nIn the code chunk below, ggbetweenstats() is used to build a visual for One-way ANOVA test on English score by race.\n\nggbetweenstats(\n  data = exam,\n  x = RACE, \n  y = ENGLISH,\n  type = \"p\",\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE\n)\n\n\n\n\n\n\n\n\n\n“ns” → only non-significant\n“s” → only significant\n“all” → everything\n\n\n\n2.3.8 Significant Test of Correlation: ggscatterstats()\nIn the code chunk below, ggscatterstats() is used to build a visual for Significant Test of Correlation between Maths scores and English scores.\n\nggscatterstats(\n  data = exam,\n  x = MATHS,\n  y = ENGLISH,\n  marginal = FALSE,\n  )\n\n\n\n\n\n\n\n\n\n\n10.3.9 Significant Test of Association (Depedence) : ggbarstats() methods\nIn the code chunk below, the Maths scores is binned into a 4-class variable by using cut().\n\nexam1 &lt;- exam %&gt;% \n  mutate(MATHS_bins = \n           cut(MATHS, \n               breaks = c(0,60,75,85,100))\n)\n\nIn this code chunk below ggbarstats() is used to build a visual for Significant Test of Association\n\nggbarstats(exam1, \n           x = MATHS_bins, \n           y = GENDER)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hand-on_Ex04.html#visualising-models",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hand-on_Ex04.html#visualising-models",
    "title": "Hands-on Exercise 4 Visual Analytics",
    "section": "2.4 Visualising Models",
    "text": "2.4 Visualising Models\nIn this section, you will learn how to visualise model diagnostic and model parameters by using parameters package.\n\nToyota Corolla case study will be used. The purpose of study is to build a model to discover factors affecting prices of used-cars by taking into consideration a set of explanatory variables."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hand-on_Ex04.html#getting-started-2",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hand-on_Ex04.html#getting-started-2",
    "title": "Hands-on Exercise 4 Visual Analytics",
    "section": "2.5 Getting Started",
    "text": "2.5 Getting Started"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hand-on_Ex04.html#installing-and-loading-the-required-libraries",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hand-on_Ex04.html#installing-and-loading-the-required-libraries",
    "title": "Hands-on Exercise 4 Visual Analytics",
    "section": "2.6 Installing and loading the required libraries",
    "text": "2.6 Installing and loading the required libraries\n\npacman::p_load(readxl, performance, parameters, see)\n\n\n2.6.1 Importing Excel file: readxl methods\nIn the code chunk below, read_xls() of readxl package is used to import the data worksheet of ToyotaCorolla.xls workbook into R.\n\ncar_resale &lt;- read_xls(\"C:/lzc0313/ISSS608-VAAA/In-class_Ex/In-class_ex1/data/data/ToyotaCorolla.xls\", \"data\")\ncar_resale\n\n# A tibble: 1,436 × 38\n      Id Model    Price Age_08_04 Mfg_Month Mfg_Year     KM Quarterly_Tax Weight\n   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt;\n 1    81 TOYOTA … 18950        25         8     2002  20019           100   1180\n 2     1 TOYOTA … 13500        23        10     2002  46986           210   1165\n 3     2 TOYOTA … 13750        23        10     2002  72937           210   1165\n 4     3  TOYOTA… 13950        24         9     2002  41711           210   1165\n 5     4 TOYOTA … 14950        26         7     2002  48000           210   1165\n 6     5 TOYOTA … 13750        30         3     2002  38500           210   1170\n 7     6 TOYOTA … 12950        32         1     2002  61000           210   1170\n 8     7  TOYOTA… 16900        27         6     2002  94612           210   1245\n 9     8 TOYOTA … 18600        30         3     2002  75889           210   1245\n10    44 TOYOTA … 16950        27         6     2002 110404           234   1255\n# ℹ 1,426 more rows\n# ℹ 29 more variables: Guarantee_Period &lt;dbl&gt;, HP_Bin &lt;chr&gt;, CC_bin &lt;chr&gt;,\n#   Doors &lt;dbl&gt;, Gears &lt;dbl&gt;, Cylinders &lt;dbl&gt;, Fuel_Type &lt;chr&gt;, Color &lt;chr&gt;,\n#   Met_Color &lt;dbl&gt;, Automatic &lt;dbl&gt;, Mfr_Guarantee &lt;dbl&gt;,\n#   BOVAG_Guarantee &lt;dbl&gt;, ABS &lt;dbl&gt;, Airbag_1 &lt;dbl&gt;, Airbag_2 &lt;dbl&gt;,\n#   Airco &lt;dbl&gt;, Automatic_airco &lt;dbl&gt;, Boardcomputer &lt;dbl&gt;, CD_Player &lt;dbl&gt;,\n#   Central_Lock &lt;dbl&gt;, Powered_Windows &lt;dbl&gt;, Power_Steering &lt;dbl&gt;, …\n\n\nNotice that the output object car_resale is a tibble data frame.\n\n\n2.6.2 Multiple Regression Model using lm()\nThe code chunk below is used to calibrate a multiple linear regression model by using lm() of Base Stats of R.\n\nmodel &lt;- lm(Price ~ Age_08_04 + Mfg_Year + KM + \n              Weight + Guarantee_Period, data = car_resale)\nmodel\n\n\nCall:\nlm(formula = Price ~ Age_08_04 + Mfg_Year + KM + Weight + Guarantee_Period, \n    data = car_resale)\n\nCoefficients:\n     (Intercept)         Age_08_04          Mfg_Year                KM  \n      -2.637e+06        -1.409e+01         1.315e+03        -2.323e-02  \n          Weight  Guarantee_Period  \n       1.903e+01         2.770e+01  \n\n\n\n\n2.6.3 Model Diagnostic: checking for multicolinearity:\nIn the code chunk, check_collinearity() of performance package.\n\ncheck_collinearity(model)\n\n# Check for Multicollinearity\n\nLow Correlation\n\n             Term  VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n               KM 1.46 [ 1.37,  1.57]         1.21      0.68     [0.64, 0.73]\n           Weight 1.41 [ 1.32,  1.51]         1.19      0.71     [0.66, 0.76]\n Guarantee_Period 1.04 [ 1.01,  1.17]         1.02      0.97     [0.86, 0.99]\n\nHigh Correlation\n\n      Term   VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n Age_08_04 31.07 [28.08, 34.38]         5.57      0.03     [0.03, 0.04]\n  Mfg_Year 31.16 [28.16, 34.48]         5.58      0.03     [0.03, 0.04]\n\n\n\ncheck_c &lt;- check_collinearity(model)\nplot(check_c)\n\n\n\n\n\n\n\n\n\n\n2.6.4 Model Diagnostic: checking normality assumption\nIn the code chunk, check_normality() of performance package.\n\nmodel1 &lt;- lm(Price ~ Age_08_04 + KM + \n              Weight + Guarantee_Period, data = car_resale)\n\n\ncheck_n &lt;- check_normality(model1)\n\n\nplot(check_n)\n\n\n\n\n\n\n\n\n\n\n2.6.5 Model Diagnostic: Check model for homogeneity of variances\nIn the code chunk, check_heteroscedasticity() of performance package.\n\ncheck_h &lt;- check_heteroscedasticity(model1)\n\n\nplot(check_h)\n\n\n\n\n\n\n\n\n\n\n2.6.6 Model Diagnostic: Complete check\nWe can also perform the complete by using check_model().\n\ncheck_model(model1)\n\n\n\n\n\n\n\n\n\n\n2.6.7 Visualising Regression Parameters: see methods\nIn the code below, plot() of see package and parameters() of parameters package is used to visualise the parameters of a regression model.\n\nplot(parameters(model1))\n\n\n\n\n\n\n\n\n\n\n2.6.8 Visualising Regression Parameters: ggcoefstats() methods\nIn the code below, ggcoefstats() of ggstatsplot package to visualise the parameters of a regression model.\n\nggcoefstats(model1, \n            output = \"plot\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hand-on_Ex04.html#learning-outcome-2",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hand-on_Ex04.html#learning-outcome-2",
    "title": "Hands-on Exercise 4 Visual Analytics",
    "section": "3.1 Learning Outcome",
    "text": "3.1 Learning Outcome\nVisualising uncertainty is relatively new in statistical graphics. In this chapter, you will gain hands-on experience on creating statistical graphics for visualising uncertainty. By the end of this chapter you will be able:\n\nto plot statistics error bars by using ggplot2,\nto plot interactive error bars by combining ggplot2, plotly and DT,\nto create advanced by using ggdist, and\nto create hypothetical outcome plots (HOPs) by using ungeviz package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hand-on_Ex04.html#getting-started-3",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hand-on_Ex04.html#getting-started-3",
    "title": "Hands-on Exercise 4 Visual Analytics",
    "section": "3.2 Getting Started",
    "text": "3.2 Getting Started\n\n3.2.1 Installing and loading the packages\nFor the purpose of this exercise, the following R packages will be used, they are:\n\ntidyverse, a family of R packages for data science process,\nplotly for creating interactive plot,\ngganimate for creating animation plot,\nDT for displaying interactive html table,\ncrosstalk for for implementing cross-widget interactions (currently, linked brushing and filtering), and\nggdist for visualising distribution and uncertainty.\n\n\npacman::p_load(ungeviz, plotly, crosstalk,\n               DT, ggdist, ggridges,\n               colorspace, gganimate, tidyverse)\n\n\n\n3.2.2 Data import\nFor the purpose of this exercise, Exam_data.csv will be used.\nexam &lt;- read_csv(\"data/Exam_data.csv\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hand-on_Ex04.html#visualizing-the-uncertainty-of-point-estimates-ggplot2-methods",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hand-on_Ex04.html#visualizing-the-uncertainty-of-point-estimates-ggplot2-methods",
    "title": "Hands-on Exercise 4 Visual Analytics",
    "section": "3.3 Visualizing the uncertainty of point estimates: ggplot2 methods",
    "text": "3.3 Visualizing the uncertainty of point estimates: ggplot2 methods\nA point estimate is a single number, such as a mean. Uncertainty, on the other hand, is expressed as standard error, confidence interval, or credible interval.\nImportant\n\nDon’t confuse the uncertainty of a point estimate with the variation in the sample\n\n\nmy_sum &lt;- exam %&gt;%\n  group_by(RACE) %&gt;%\n  summarise(\n    n=n(),\n    mean=mean(MATHS),\n    sd=sd(MATHS)\n    ) %&gt;%\n  mutate(se=sd/sqrt(n-1))\n\n\ngroup_by() of dplyr package is used to group the observation by RACE,\nsummarise() is used to compute the count of observations, mean, standard deviation\nmutate() is used to derive standard error of Maths by RACE, and\nthe output is save as a tibble data table called my_sum.\n\nNext, the code chunk below will be used to display my_sum tibble data frame in an html table format.\n\nknitr::kable(head(my_sum), format = 'html')\n\n\n\n\nRACE\nn\nmean\nsd\nse\n\n\n\n\nChinese\n193\n76.50777\n15.69040\n1.132357\n\n\nIndian\n12\n60.66667\n23.35237\n7.041005\n\n\nMalay\n108\n57.44444\n21.13478\n2.043177\n\n\nOthers\n9\n69.66667\n10.72381\n3.791438\n\n\n\n\n\n\n\n\n3.3.1 Plotting standard error bars of point estimates\nNow we are ready to plot the standard error bars of mean maths score by race as shown below.\n\nggplot(my_sum) +\n  geom_errorbar(\n    aes(x=RACE, \n        ymin=mean-se, \n        ymax=mean+se), \n    width=0.2, \n    colour=\"black\", \n    alpha=0.9, \n    size=0.5) +\n  geom_point(aes\n           (x=RACE, \n            y=mean), \n           stat=\"identity\", \n           color=\"red\",\n           size = 1.5,\n           alpha=1) +\n  ggtitle(\"Standard error of mean maths score by rac\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThings to learn from the code chunk above\n\nThe error bars are computed by using the formula mean+/-se.\nFor geom_point(), it is important to indicate stat=“identity”.\n\n\n\n\n\n3.3.2 Plotting confidence interval of point estimates\nInstead of plotting the standard error bar of point estimates, we can also plot the confidence intervals of mean maths score by race.\n\nggplot(my_sum) +\n  geom_errorbar(\n    aes(x=reorder(RACE, -mean), \n        ymin=mean-1.96*se, \n        ymax=mean+1.96*se), \n    width=0.2, \n    colour=\"black\", \n    alpha=0.9, \n    size=0.5) +\n  geom_point(aes\n           (x=RACE, \n            y=mean), \n           stat=\"identity\", \n           color=\"red\",\n           size = 1.5,\n           alpha=1) +\n  labs(x = \"Maths score\",\n       title = \"95% confidence interval of mean maths score by race\")\n\n\n\n\n\n\n\n\n\n\n3.3.3 Visualizing the uncertainty of point estimates with interactive error bars\nIn this section, you will learn how to plot interactive error bars for the 99% confidence interval of mean maths score by race as shown in the figure below.\n\nshared_df = SharedData$new(my_sum)\n\nbscols(widths = c(4,8),\n       ggplotly((ggplot(shared_df) +\n                   geom_errorbar(aes(\n                     x=reorder(RACE, -mean),\n                     ymin=mean-2.58*se, \n                     ymax=mean+2.58*se), \n                     width=0.2, \n                     colour=\"black\", \n                     alpha=0.9, \n                     size=0.5) +\n                   geom_point(aes(\n                     x=RACE, \n                     y=mean, \n                     text = paste(\"Race:\", `RACE`, \n                                  \"&lt;br&gt;N:\", `n`,\n                                  \"&lt;br&gt;Avg. Scores:\", round(mean, digits = 2),\n                                  \"&lt;br&gt;95% CI:[\", \n                                  round((mean-2.58*se), digits = 2), \",\",\n                                  round((mean+2.58*se), digits = 2),\"]\")),\n                     stat=\"identity\", \n                     color=\"red\", \n                     size = 1.5, \n                     alpha=1) + \n                   xlab(\"Race\") + \n                   ylab(\"Average Scores\") + \n                   theme_minimal() + \n                   theme(axis.text.x = element_text(\n                     angle = 45, vjust = 0.5, hjust=1)) +\n                   ggtitle(\"99% Confidence interval of average /&lt;br&gt;maths scores by race\")), \n                tooltip = \"text\"), \n       DT::datatable(shared_df, \n                     rownames = FALSE, \n                     class=\"compact\", \n                     width=\"100%\", \n                     options = list(pageLength = 10,\n                                    scrollX=T), \n                     colnames = c(\"No. of pupils\", \n                                  \"Avg Scores\",\n                                  \"Std Dev\",\n                                  \"Std Error\")) %&gt;%\n         formatRound(columns=c('mean', 'sd', 'se'),\n                     digits=2))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hand-on_Ex04.html#visualising-uncertainty-ggdist-package",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hand-on_Ex04.html#visualising-uncertainty-ggdist-package",
    "title": "Hands-on Exercise 4 Visual Analytics",
    "section": "3.4 Visualising Uncertainty: ggdist package",
    "text": "3.4 Visualising Uncertainty: ggdist package\n\nggdist is an R package that provides a flexible set of ggplot2 geoms and stats designed especially for visualising distributions and uncertainty.\nIt is designed for both frequentist and Bayesian uncertainty visualization, taking the view that uncertainty visualization can be unified through the perspective of distribution visualization:\n\nfor frequentist models, one visualises confidence distributions or bootstrap distributions (see vignette(“freq-uncertainty-vis”));\nfor Bayesian models, one visualises probability distributions (see the tidybayes package, which builds on top of ggdist).\n\n\n\n3.4.1 Visualizing the uncertainty of point estimates: ggdist methods\nIn the code chunk below, stat_pointinterval() of ggdist is used to build a visual for displaying distribution of maths scores by race.\n\nexam %&gt;%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_pointinterval() +\n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Mean Point + Multiple-interval plot\")\n\n\n\n\n\n\n\n\nFor example, in the code chunk below the following arguments are used:\n\n.width = 0.95\n.point = median\n.interval = qi\n\n\nexam %&gt;%\n  ggplot(aes(x = RACE, y = MATHS)) +\n  stat_pointinterval(.width = 0.95,\n  .point = median,\n  .interval = qi) +\n  labs(\n    title = \"Visualising confidence intervals of median math score\",\n    subtitle = \"Median Point + Multiple-interval plot\")\n\n\n\n\n\n\n\n\n\n\n3.4.2 Visualizing the uncertainty of point estimates: ggdist methods\n\nexam %&gt;%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_pointinterval(\n    show.legend = FALSE) +   \n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Mean Point + Multiple-interval plot\")\n\n\n\n\n\n\n\n\nGentle advice: This function comes with many arguments, students are advised to read the syntax reference for more detail.\n\n\n3.4.3 Visualizing the uncertainty of point estimates: ggdist methods\nIn the code chunk below, stat_gradientinterval() of ggdist is used to build a visual for displaying distribution of maths scores by race.\n\nexam %&gt;%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_gradientinterval(   \n    fill = \"skyblue\",      \n    show.legend = TRUE     \n  ) +                        \n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Gradient + interval plot\")\n\n\n\n\n\n\n\n\nGentle advice: This function comes with many arguments, students are advised to read the syntax reference for more detail."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hand-on_Ex04.html#visualising-uncertainty-with-hypothetical-outcome-plots-hops",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hand-on_Ex04.html#visualising-uncertainty-with-hypothetical-outcome-plots-hops",
    "title": "Hands-on Exercise 4 Visual Analytics",
    "section": "3.5 Visualising Uncertainty with Hypothetical Outcome Plots (HOPs)",
    "text": "3.5 Visualising Uncertainty with Hypothetical Outcome Plots (HOPs)\nStep 1: Installing ungeviz package\n\n\n\nggplot(data = exam, \n       (aes(x = factor(RACE), y = MATHS))) +\n  geom_point(position = position_jitter(\n    height = 0.3, width = 0.05), \n    size = 0.4, color = \"#0072B2\", alpha = 1/2) +\n  geom_hpline(data = sampler(25, group = RACE), height = 0.6, color = \"#D55E00\") +\n  theme_bw() + \n  # `.draw` is a generated column indicating the sample draw\n  transition_states(.draw, 1, 3)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hand-on_Ex04.html#visualising-uncertainty-with-hypothetical-outcome-plots-hops-1",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hand-on_Ex04.html#visualising-uncertainty-with-hypothetical-outcome-plots-hops-1",
    "title": "Hands-on Exercise 4 Visual Analytics",
    "section": "3.6 Visualising Uncertainty with Hypothetical Outcome Plots (HOPs)",
    "text": "3.6 Visualising Uncertainty with Hypothetical Outcome Plots (HOPs)\n\nggplot(data = exam, \n       (aes(x = factor(RACE), \n            y = MATHS))) +\n  geom_point(position = position_jitter(\n    height = 0.3, \n    width = 0.05), \n    size = 0.4, \n    color = \"#0072B2\", \n    alpha = 1/2) +\n  geom_hpline(data = sampler(25, \n                             group = RACE), \n              height = 0.6, \n              color = \"#D55E00\") +\n  theme_bw() + \n  transition_states(.draw, 1, 3)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hand-on_Ex04.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hand-on_Ex04.html#overview",
    "title": "Hands-on Exercise 4 Visual Analytics",
    "section": "4.1 Overview",
    "text": "4.1 Overview\nFunnel plot is a specially designed data visualisation for conducting unbiased comparison between outlets, stores or business entities. By the end of this hands-on exercise, you will gain hands-on experience on:\n\nplotting funnel plots by using funnelPlotR package,\nplotting static funnel plot by using ggplot2 package, and\nplotting interactive funnel plot by using both plotly R and ggplot2 packages."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hand-on_Ex04.html#installing-and-launching-r-packages-1",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hand-on_Ex04.html#installing-and-launching-r-packages-1",
    "title": "Hands-on Exercise 4 Visual Analytics",
    "section": "4.2 Installing and Launching R Packages",
    "text": "4.2 Installing and Launching R Packages\nIn this exercise, four R packages will be used. They are:\n\nreadr for importing csv into R.\nFunnelPlotR for creating funnel plot.\nggplot2 for creating funnel plot manually.\nknitr for building static html table.\nplotly for creating interactive funnel plot.\n\npacman::p_load(tidyverse, FunnelPlotR, plotly, knitr)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hand-on_Ex04.html#importing-data-1",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hand-on_Ex04.html#importing-data-1",
    "title": "Hands-on Exercise 4 Visual Analytics",
    "section": "4.3 Importing Data",
    "text": "4.3 Importing Data\nIn this section, COVID-19_DKI_Jakarta will be used. The data was downloaded from Open Data Covid-19 Provinsi DKI Jakarta portal. For this hands-on exercise, we are going to compare the cumulative COVID-19 cases and death by sub-district (i.e. kelurahan) as at 31st July 2021, DKI Jakarta.\nThe code chunk below imports the data into R and save it into a tibble data frame object called covid19.\n\ncovid19 &lt;- read_csv(\"C:/lzc0313/ISSS608-VAAA/In-class_Ex/In-class_ex1/data/data/COVID-19_DKI_Jakarta.csv\") %&gt;%\n  mutate_if(is.character, as.factor)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hand-on_Ex04.html#funnelplotr-methods",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hand-on_Ex04.html#funnelplotr-methods",
    "title": "Hands-on Exercise 4 Visual Analytics",
    "section": "4.4 FunnelPlotR methods",
    "text": "4.4 FunnelPlotR methods\nFunnelPlotR package uses ggplot to generate funnel plots. It requires a numerator (events of interest), denominator (population to be considered) and group. The key arguments selected for customisation are:\n\nlimit: plot limits (95 or 99).\nlabel_outliers: to label outliers (true or false).\nPoisson_limits: to add Poisson limits to the plot.\nOD_adjust: to add overdispersed limits to the plot.\nxrange and yrange: to specify the range to display for axes, acts like a zoom function.\nOther aesthetic components such as graph title, axis labels etc.\n\n\n4.4.1 FunnelPlotR methods: The basic plot\nThe code chunk below plots a funnel plot.\n\nfunnel_plot(\n  numerator = covid19$Positive,\n  denominator = covid19$Death,\n  group = covid19$`Sub-district`\n)\n\n\n\n\n\n\n\n\nA funnel plot object with 267 points of which 0 are outliers. \nPlot is adjusted for overdispersion. \n\n\nA funnel plot object with 267 points of which 0 are outliers.  Plot is adjusted for overdispersion. \nThings to learn from the code chunk above.\n\ngroup in this function is different from the scatterplot. Here, it defines the level of the points to be plotted i.e. Sub-district, District or City. If Cityc is chosen, there are only six data points.\nBy default, data_typeargument is “SR”.\nlimit: Plot limits, accepted values are: 95 or 99, corresponding to 95% or 99.8% quantiles of the distribution.\n\n\n\n4.4.2 FunnelPlotR methods: Makeover 1\nThe code chunk below plots a funnel plot.\n\nfunnel_plot(\n  numerator = covid19$Death,\n  denominator = covid19$Positive,\n  group = covid19$`Sub-district`,\n  data_type = \"PR\",     #&lt;&lt;\n  xrange = c(0, 6500),  #&lt;&lt;\n  yrange = c(0, 0.05)   #&lt;&lt;\n)\n\n\n\n\n\n\n\n\nA funnel plot object with 267 points of which 7 are outliers. \nPlot is adjusted for overdispersion. \n\n\nA funnel plot object with 267 points of which 7 are outliers.  Plot is adjusted for overdispersion. \nThings to learn from the code chunk above. + data_type argument is used to change from default “SR” to “PR” (i.e. proportions). + xrange and yrange are used to set the range of x-axis and y-axis\n\n\n4.4.3 FunnelPlotR methods: Makeover 2\nThe code chunk below plots a funnel plot.\n\nfunnel_plot(\n  numerator = covid19$Death,\n  denominator = covid19$Positive,\n  group = covid19$`Sub-district`,\n  data_type = \"PR\",   \n  xrange = c(0, 6500),  \n  yrange = c(0, 0.05),\n  label = NA,\n  title = \"Cumulative COVID-19 Fatality Rate by Cumulative Total Number of COVID-19 Positive Cases\", #&lt;&lt;           \n  x_label = \"Cumulative COVID-19 Positive Cases\", #&lt;&lt;\n  y_label = \"Cumulative Fatality Rate\"  #&lt;&lt;\n)\n\n\n\n\n\n\n\n\nA funnel plot object with 267 points of which 7 are outliers. \nPlot is adjusted for overdispersion. \n\n\nA funnel plot object with 267 points of which 7 are outliers.  Plot is adjusted for overdispersion. \nThings to learn from the code chunk above.\n\nlabel = NA argument is to removed the default label outliers feature.\ntitle argument is used to add plot title.\nx_label and y_label arguments are used to add/edit x-axis and y-axis titles."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hand-on_Ex04.html#funnel-plot-for-fair-visual-comparison-ggplot2-methods",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hand-on_Ex04.html#funnel-plot-for-fair-visual-comparison-ggplot2-methods",
    "title": "Hands-on Exercise 4 Visual Analytics",
    "section": "4.5 Funnel Plot for Fair Visual Comparison: ggplot2 methods",
    "text": "4.5 Funnel Plot for Fair Visual Comparison: ggplot2 methods\nIn this section, you will gain hands-on experience on building funnel plots step-by-step by using ggplot2. It aims to enhance you working experience of ggplot2 to customise speciallised data visualisation like funnel plot.\n\n4.5.1 Computing the basic derived fields\nTo plot the funnel plot from scratch, we need to derive cumulative death rate and standard error of cumulative death rate.\n\ndf &lt;- covid19 %&gt;%\n  mutate(rate = Death / Positive) %&gt;%\n  mutate(rate.se = sqrt((rate*(1-rate)) / (Positive))) %&gt;%\n  filter(rate &gt; 0)\n\nNext, the fit.mean is computed by using the code chunk below.\n\nfit.mean &lt;- weighted.mean(df$rate, 1/df$rate.se^2)\n\n\n\n4.5.2 Calculate lower and upper limits for 95% and 99.9% CI\nThe code chunk below is used to compute the lower and upper limits for 95% confidence interval.\n\nnumber.seq &lt;- seq(1, max(df$Positive), 1)\nnumber.ll95 &lt;- fit.mean - 1.96 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ul95 &lt;- fit.mean + 1.96 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ll999 &lt;- fit.mean - 3.29 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ul999 &lt;- fit.mean + 3.29 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \ndfCI &lt;- data.frame(number.ll95, number.ul95, number.ll999, \n                   number.ul999, number.seq, fit.mean)\n\n\n\n4.5.3 Plotting a static funnel plot\nIn the code chunk below, ggplot2 functions are used to plot a static funnel plot.\n\np &lt;- ggplot(df, aes(x = Positive, y = rate)) +\n  geom_point(aes(label=`Sub-district`), \n             alpha=0.4) +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ll95), \n            size = 0.4, \n            colour = \"grey40\", \n            linetype = \"dashed\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ul95), \n            size = 0.4, \n            colour = \"grey40\", \n            linetype = \"dashed\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ll999), \n            size = 0.4, \n            colour = \"grey40\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ul999), \n            size = 0.4, \n            colour = \"grey40\") +\n  geom_hline(data = dfCI, \n             aes(yintercept = fit.mean), \n             size = 0.4, \n             colour = \"grey40\") +\n  coord_cartesian(ylim=c(0,0.05)) +\n  annotate(\"text\", x = 1, y = -0.13, label = \"95%\", size = 3, colour = \"grey40\") + \n  annotate(\"text\", x = 4.5, y = -0.18, label = \"99%\", size = 3, colour = \"grey40\") + \n  ggtitle(\"Cumulative Fatality Rate by Cumulative Number of COVID-19 Cases\") +\n  xlab(\"Cumulative Number of COVID-19 Cases\") + \n  ylab(\"Cumulative Fatality Rate\") +\n  theme_light() +\n  theme(plot.title = element_text(size=12),\n        legend.position = c(0.91,0.85), \n        legend.title = element_text(size=7),\n        legend.text = element_text(size=7),\n        legend.background = element_rect(colour = \"grey60\", linetype = \"dotted\"),\n        legend.key.height = unit(0.3, \"cm\"))\np\n\n\n\n\n\n\n\n\n\n\n4.5.4 Interactive Funnel Plot: plotly + ggplot2\nThe funnel plot created using ggplot2 functions can be made interactive with ggplotly() of plotly r package.\n\nfp_ggplotly &lt;- ggplotly(p,\n  tooltip = c(\"label\", \n              \"x\", \n              \"y\"))\nfp_ggplotly"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hand-on_Ex04.html#references",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hand-on_Ex04.html#references",
    "title": "Hands-on Exercise 4 Visual Analytics",
    "section": "4.6 References",
    "text": "4.6 References\n\nfunnelPlotR package.\nFunnel Plots for Indirectly-standardised ratios.\nChanging funnel plot options\nggplot2 package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hand-on_Ex05.html",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hand-on_Ex05.html",
    "title": "Hand-on exercise 5 Visual Analytics",
    "section": "",
    "text": "Ternary plots are a way of displaying the distribution and variability of three-part compositional data. (For example, the proportion of aged, economy active and young population or sand, silt, and clay in soil.) It’s display is a triangle with sides scaled from 0 to 1. Each side represents one of the three components. A point is plotted so that a line drawn perpendicular from the point to each leg of the triangle intersect at the component values of the point.\nIn this hands-on, you will learn how to build ternary plot programmatically using R for visualising and analysing population structure of Singapore.\nThe hands-on exercise consists of four steps:\n\nInstall and launch tidyverse and ggtern packages.\nDerive three new measures using mutate() function of dplyr package.\nBuild a static ternary plot using ggtern() function of ggtern package.\nBuild an interactive ternary plot using plot-ly() function of Plotly R package.\n\n\n\n\nFor this exercise, two main R packages will be used in this hands-on exercise, they are:\n\nggtern, a ggplot extension specially designed to plot ternary diagrams. The package will be used to plot static ternary plots.\nPlotly R, an R package for creating interactive web-based graphs via plotly’s JavaScript graphing library, plotly.js . The plotly R libary contains the ggplotly function, which will convert ggplot2 figures into a Plotly object.\n\nWe will also need to ensure that selected tidyverse family packages namely: readr, dplyr and tidyr are also installed and loaded.\nIn this exercise, version 3.2.1 of ggplot2 will be installed instead of the latest version of ggplot2. This is because the current version of ggtern package is not compatible to the latest version of ggplot2.\nThe code chunks below will accomplish the task.\n\npacman::p_load(plotly, ggtern, tidyverse)\n\n\n\n\n\n\nFor the purpose of this hands-on exercise, the Singapore Residents by Planning AreaSubzone, Age Group, Sex and Type of Dwelling, June 2000-2018 data will be used. The data set has been downloaded and included in the data sub-folder of the hands-on exercise folder. It is called respopagsex2000to2018_tidy.csv and is in csv file format.\n\n\n\nTo important respopagsex2000to2018_tidy.csv into R, read_csv() function of readr package will be used.\n\n#Reading the data into R environment\npop_data &lt;- read_csv(\"data/respopagsex2000to2018_tidy.csv\") \n\n\n\n\nNext, use the mutate() function of dplyr package to derive three new measures, namely: young, active, and old.\n\n#Deriving the young, economy active and old measures\nagpop_mutated &lt;- pop_data %&gt;%\n  mutate(`Year` = as.character(Year))%&gt;%\n  spread(AG, Population) %&gt;%\n  mutate(YOUNG = rowSums(.[4:8]))%&gt;%\n  mutate(ACTIVE = rowSums(.[9:16]))  %&gt;%\n  mutate(OLD = rowSums(.[17:21])) %&gt;%\n  mutate(TOTAL = rowSums(.[22:24])) %&gt;%\n  filter(Year == 2018)%&gt;%\n  filter(TOTAL &gt; 0)\n\n\n\n\n\n\n\nUse ggtern() function of ggtern package to create a simple ternary plot.\n\n#Building the static ternary plot\nggtern(data=agpop_mutated,aes(x=YOUNG,y=ACTIVE, z=OLD)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n#Building the static ternary plot \nggtern(data=agpop_mutated, aes(x=YOUNG,y=ACTIVE, z=OLD)) + geom_point() + labs(title=\"Population structure, 2015\") + theme_rgbw()\n\n\n\n\n\n\n\n\n\n\n\nThe code below create an interactive ternary plot using plot_ly() function of Plotly R.\n\n# reusable function for creating annotation object\nlabel &lt;- function(txt) {\n  list(\n    text = txt, \n    x = 0.1, y = 1,\n    ax = 0, ay = 0,\n    xref = \"paper\", yref = \"paper\", \n    align = \"center\",\n    font = list(family = \"serif\", size = 15, color = \"white\"),\n    bgcolor = \"#b3b3b3\", bordercolor = \"black\", borderwidth = 2\n  )\n}\n\n# reusable function for axis formatting\naxis &lt;- function(txt) {\n  list(\n    title = txt, tickformat = \".0%\", tickfont = list(size = 10)\n  )\n}\n\nternaryAxes &lt;- list(\n  aaxis = axis(\"Young\"), \n  baxis = axis(\"Active\"), \n  caxis = axis(\"Old\")\n)\n\n# Initiating a plotly visualization \nplot_ly(\n  agpop_mutated, \n  a = ~YOUNG, \n  b = ~ACTIVE, \n  c = ~OLD, \n  color = I(\"black\"), \n  type = \"scatterternary\"\n) %&gt;%\n  layout(\n    annotations = label(\"Ternary Markers\"), \n    ternary = ternaryAxes\n  )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hand-on_Ex05.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hand-on_Ex05.html#overview",
    "title": "Hand-on exercise 5 Visual Analytics",
    "section": "",
    "text": "Ternary plots are a way of displaying the distribution and variability of three-part compositional data. (For example, the proportion of aged, economy active and young population or sand, silt, and clay in soil.) It’s display is a triangle with sides scaled from 0 to 1. Each side represents one of the three components. A point is plotted so that a line drawn perpendicular from the point to each leg of the triangle intersect at the component values of the point.\nIn this hands-on, you will learn how to build ternary plot programmatically using R for visualising and analysing population structure of Singapore.\nThe hands-on exercise consists of four steps:\n\nInstall and launch tidyverse and ggtern packages.\nDerive three new measures using mutate() function of dplyr package.\nBuild a static ternary plot using ggtern() function of ggtern package.\nBuild an interactive ternary plot using plot-ly() function of Plotly R package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hand-on_Ex05.html#installing-and-launching-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hand-on_Ex05.html#installing-and-launching-r-packages",
    "title": "Hand-on exercise 5 Visual Analytics",
    "section": "",
    "text": "For this exercise, two main R packages will be used in this hands-on exercise, they are:\n\nggtern, a ggplot extension specially designed to plot ternary diagrams. The package will be used to plot static ternary plots.\nPlotly R, an R package for creating interactive web-based graphs via plotly’s JavaScript graphing library, plotly.js . The plotly R libary contains the ggplotly function, which will convert ggplot2 figures into a Plotly object.\n\nWe will also need to ensure that selected tidyverse family packages namely: readr, dplyr and tidyr are also installed and loaded.\nIn this exercise, version 3.2.1 of ggplot2 will be installed instead of the latest version of ggplot2. This is because the current version of ggtern package is not compatible to the latest version of ggplot2.\nThe code chunks below will accomplish the task.\n\npacman::p_load(plotly, ggtern, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hand-on_Ex05.html#data-preparation",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hand-on_Ex05.html#data-preparation",
    "title": "Hand-on exercise 5 Visual Analytics",
    "section": "",
    "text": "For the purpose of this hands-on exercise, the Singapore Residents by Planning AreaSubzone, Age Group, Sex and Type of Dwelling, June 2000-2018 data will be used. The data set has been downloaded and included in the data sub-folder of the hands-on exercise folder. It is called respopagsex2000to2018_tidy.csv and is in csv file format.\n\n\n\nTo important respopagsex2000to2018_tidy.csv into R, read_csv() function of readr package will be used.\n\n#Reading the data into R environment\npop_data &lt;- read_csv(\"data/respopagsex2000to2018_tidy.csv\") \n\n\n\n\nNext, use the mutate() function of dplyr package to derive three new measures, namely: young, active, and old.\n\n#Deriving the young, economy active and old measures\nagpop_mutated &lt;- pop_data %&gt;%\n  mutate(`Year` = as.character(Year))%&gt;%\n  spread(AG, Population) %&gt;%\n  mutate(YOUNG = rowSums(.[4:8]))%&gt;%\n  mutate(ACTIVE = rowSums(.[9:16]))  %&gt;%\n  mutate(OLD = rowSums(.[17:21])) %&gt;%\n  mutate(TOTAL = rowSums(.[22:24])) %&gt;%\n  filter(Year == 2018)%&gt;%\n  filter(TOTAL &gt; 0)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hand-on_Ex05.html#plotting-ternary-diagram-with-r",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hand-on_Ex05.html#plotting-ternary-diagram-with-r",
    "title": "Hand-on exercise 5 Visual Analytics",
    "section": "",
    "text": "Use ggtern() function of ggtern package to create a simple ternary plot.\n\n#Building the static ternary plot\nggtern(data=agpop_mutated,aes(x=YOUNG,y=ACTIVE, z=OLD)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n#Building the static ternary plot \nggtern(data=agpop_mutated, aes(x=YOUNG,y=ACTIVE, z=OLD)) + geom_point() + labs(title=\"Population structure, 2015\") + theme_rgbw()\n\n\n\n\n\n\n\n\n\n\n\nThe code below create an interactive ternary plot using plot_ly() function of Plotly R.\n\n# reusable function for creating annotation object\nlabel &lt;- function(txt) {\n  list(\n    text = txt, \n    x = 0.1, y = 1,\n    ax = 0, ay = 0,\n    xref = \"paper\", yref = \"paper\", \n    align = \"center\",\n    font = list(family = \"serif\", size = 15, color = \"white\"),\n    bgcolor = \"#b3b3b3\", bordercolor = \"black\", borderwidth = 2\n  )\n}\n\n# reusable function for axis formatting\naxis &lt;- function(txt) {\n  list(\n    title = txt, tickformat = \".0%\", tickfont = list(size = 10)\n  )\n}\n\nternaryAxes &lt;- list(\n  aaxis = axis(\"Young\"), \n  baxis = axis(\"Active\"), \n  caxis = axis(\"Old\")\n)\n\n# Initiating a plotly visualization \nplot_ly(\n  agpop_mutated, \n  a = ~YOUNG, \n  b = ~ACTIVE, \n  c = ~OLD, \n  color = I(\"black\"), \n  type = \"scatterternary\"\n) %&gt;%\n  layout(\n    annotations = label(\"Ternary Markers\"), \n    ternary = ternaryAxes\n  )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hand-on_Ex05.html#overview-1",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hand-on_Ex05.html#overview-1",
    "title": "Hand-on exercise 5 Visual Analytics",
    "section": "2.1 Overview",
    "text": "2.1 Overview\nHeatmaps visualise data through variations in colouring. When applied to a tabular format, heatmaps are useful for cross-examining multivariate data, through placing variables in the columns and observation (or records) in rowa and colouring the cells within the table. Heatmaps are good for showing variance across multiple variables, revealing any patterns, displaying whether any variables are similar to each other, and for detecting if any correlations exist in-between them.\nIn this hands-on exercise, you will gain hands-on experience on using R to plot static and interactive heatmap for visualising and analysing multivariate data."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hand-on_Ex05.html#installing-and-launching-r-packages-1",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hand-on_Ex05.html#installing-and-launching-r-packages-1",
    "title": "Hand-on exercise 5 Visual Analytics",
    "section": "2.2 Installing and Launching R Packages",
    "text": "2.2 Installing and Launching R Packages\nBefore you get started, you are required to open a new Quarto document. Keep the default html as the authoring format.\nNext, you will use the code chunk below to install and launch seriation, heatmaply, dendextend and tidyverse in RStudio.\n\npacman::p_load(seriation, dendextend, heatmaply, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hand-on_Ex05.html#importing-and-preparing-the-data-set",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hand-on_Ex05.html#importing-and-preparing-the-data-set",
    "title": "Hand-on exercise 5 Visual Analytics",
    "section": "2.3 Importing and Preparing The Data Set",
    "text": "2.3 Importing and Preparing The Data Set\nIn this hands-on exercise, the data of World Happines 2018 report will be used. The data set is downloaded from here. The original data set is in Microsoft Excel format. It has been extracted and saved in csv file called WHData-2018.csv.\n\n2.3.1 Importing the data set\nIn the code chunk below, read_csv() of readr is used to import WHData-2018.csv into R and parsed it into tibble R data frame format.\n\nwh &lt;- read_csv(\"data/WHData-2018.csv\")\n\nThe output tibbled data frame is called wh.\n\n\n2.3.2 Preparing the data\nNext, we need to change the rows by country name instead of row number by using the code chunk below\n\nrow.names(wh) &lt;- wh$Country\n\nNotice that the row number has been replaced into the country name.\n\n\n2.3.3 Transforming the data frame into a matrix\nThe data was loaded into a data frame, but it has to be a data matrix to make your heatmap.\nThe code chunk below will be used to transform wh data frame into a data matrix.\n\nwh1 &lt;- dplyr::select(wh, c(3, 7:12))\nwh_matrix &lt;- data.matrix(wh)\n\nNotice that wh_matrix is in R matrix format."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hand-on_Ex05.html#static-heatmap",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hand-on_Ex05.html#static-heatmap",
    "title": "Hand-on exercise 5 Visual Analytics",
    "section": "2.4 Static Heatmap",
    "text": "2.4 Static Heatmap\nThere are many R packages and functions can be used to drawing static heatmaps, they are:\n\nheatmap()of R stats package. It draws a simple heatmap.\nheatmap.2() of gplots R package. It draws an enhanced heatmap compared to the R base function.\npheatmap() of pheatmap R package. pheatmap package also known as Pretty Heatmap. The package provides functions to draws pretty heatmaps and provides more control to change the appearance of heatmaps.\nComplexHeatmap package of R/Bioconductor package. The package draws, annotates and arranges complex heatmaps (very useful for genomic data analysis). The full reference guide of the package is available here.\nsuperheat package: A Graphical Tool for Exploring Complex Datasets Using Heatmaps. A system for generating extendable and customizable heatmaps for exploring complex datasets, including big data and data with multiple data types. The full reference guide of the package is available here.\n\nIn this section, you will learn how to plot static heatmaps by using heatmap() of R Stats package.\n\n2.4.1 heatmap() of R Stats\nIn this sub-section, we will plot a heatmap by using heatmap() of Base Stats. The code chunk is given below.\n\nwh_heatmap &lt;- heatmap(wh_matrix,\n                      Rowv=NA, Colv=NA)\n\n\n\n\n\n\n\n\nNote:\n\nBy default, heatmap() plots a cluster heatmap. The arguments Rowv=NA and Colv=NA are used to switch off the option of plotting the row and column dendrograms.\n\nTo plot a cluster heatmap, we just have to use the default as shown in the code chunk below.\n\nwh_heatmap &lt;- heatmap(wh_matrix)\n\n\n\n\n\n\n\n\nNote:\n\nThe order of both rows and columns is different compare to the native wh_matrix. This is because heatmap do a reordering using clusterisation: it calculates the distance between each pair of rows and columns and try to order them by similarity. Moreover, the corresponding dendrogram are provided beside the heatmap.\n\nHere, red cells denotes small values, and red small ones. This heatmap is not really informative. Indeed, the Happiness Score variable have relatively higher values, what makes that the other variables with small values all look the same. Thus, we need to normalize this matrix. This is done using the scale argument. It can be applied to rows or to columns following your needs.\nThe code chunk below normalises the matrix column-wise.\n\nwh_heatmap &lt;- heatmap(wh_matrix,\n                      scale=\"column\",\n                      cexRow = 0.6, \n                      cexCol = 0.8,\n                      margins = c(10, 4))\n\n\n\n\n\n\n\n\nNotice that the values are scaled now. Also note that margins argument is used to ensure that the entire x-axis labels are displayed completely and, cexRow and cexCol arguments are used to define the font size used for y-axis and x-axis labels respectively."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hand-on_Ex05.html#creating-interactive-heatmap",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hand-on_Ex05.html#creating-interactive-heatmap",
    "title": "Hand-on exercise 5 Visual Analytics",
    "section": "2.5 Creating Interactive Heatmap",
    "text": "2.5 Creating Interactive Heatmap\nheatmaply is an R package for building interactive cluster heatmap that can be shared online as a stand-alone HTML file. It is designed and maintained by Tal Galili.\nBefore we get started, you should review the Introduction to Heatmaply to have an overall understanding of the features and functions of Heatmaply package. You are also required to have the user manualof the package handy with you for reference purposes.\nIn this section, you will gain hands-on experience on using heatmaply to design an interactive cluster heatmap. We will still use the wh_matrix as the input data.\n\n2.5.1 Working with heatmaply\n\nheatmaply(mtcars)\n\n\n\n\n\nThe code chunk below shows the basic syntax needed to create n interactive heatmap by using heatmaply package.\n\nheatmaply(wh_matrix[, -c(1, 2, 4, 5)])\n\n\n\n\n\nNote that:\n\nDifferent from heatmap(), for heatmaply() the default horizontal dendrogram is placed on the left hand side of the heatmap.\nThe text label of each raw, on the other hand, is placed on the right hand side of the heat map.\nWhen the x-axis marker labels are too long, they will be rotated by 135 degree from the north.\n\n\n\n2.5.2 Data trasformation\nWhen analysing multivariate data set, it is very common that the variables in the data sets includes values that reflect different types of measurement. In general, these variables’ values have their own range. In order to ensure that all the variables have comparable values, data transformation are commonly used before clustering.\nThree main data transformation methods are supported by heatmaply(), namely: scale, normalise and percentilse.\n\n2.5.2.1 Scaling method\n\nWhen all variables are came from or assumed to come from some normal distribution, then scaling (i.e.: subtract the mean and divide by the standard deviation) would bring them all close to the standard normal distribution.\nIn such a case, each value would reflect the distance from the mean in units of standard deviation.\nThe scale argument in heatmaply() supports column and row scaling.\n\nThe code chunk below is used to scale variable values columewise.\n\nheatmaply(wh_matrix[, -c(1, 2, 4, 5)],\n          scale = \"column\")\n\n\n\n\n\n\n\n2.5.2.2 Normalising method\n\nWhen variables in the data comes from possibly different (and non-normal) distributions, the normalize function can be used to bring data to the 0 to 1 scale by subtracting the minimum and dividing by the maximum of all observations.\nThis preserves the shape of each variable’s distribution while making them easily comparable on the same “scale”.\n\nDifferent from Scaling, the normalise method is performed on the input data set i.e. wh_matrix as shown in the code chunk below.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]))\n\n\n\n\n\n\n\n2.5.2.3 Percentising method\n\nThis is similar to ranking the variables, but instead of keeping the rank values, divide them by the maximal rank.\nThis is done by using the ecdf of the variables on their own values, bringing each value to its empirical percentile.\nThe benefit of the percentize function is that each value has a relatively clear interpretation, it is the percent of observations that got that value or below it.\n\nSimilar to Normalize method, the Percentize method is also performed on the input data set i.e. wh_matrix as shown in the code chunk below.\n\nheatmaply(percentize(wh_matrix[, -c(1, 2, 4, 5)]))\n\n\n\n\n\n\n\n\n2.5.3 Clustering algorithm\nheatmaply supports a variety of hierarchical clustering algorithm. The main arguments provided are:\n\ndistfun: function used to compute the distance (dissimilarity) between both rows and columns. Defaults to dist. The options “pearson”, “spearman” and “kendall” can be used to use correlation-based clustering, which uses as.dist(1 - cor(t(x))) as the distance metric (using the specified correlation method).\nhclustfun: function used to compute the hierarchical clustering when Rowv or Colv are not dendrograms. Defaults to hclust.\ndist_method default is NULL, which results in “euclidean” to be used. It can accept alternative character strings indicating the method to be passed to distfun. By default distfun is “dist”” hence this can be one of “euclidean”, “maximum”, “manhattan”, “canberra”, “binary” or “minkowski”.\nhclust_method default is NULL, which results in “complete” method to be used. It can accept alternative character strings indicating the method to be passed to hclustfun. By default hclustfun is hclust hence this can be one of “ward.D”, “ward.D2”, “single”, “complete”, “average” (= UPGMA), “mcquitty” (= WPGMA), “median” (= WPGMC) or “centroid” (= UPGMC).\n\nIn general, a clustering model can be calibrated either manually or statistically.\n\n\n2.5.4 Manual approach\nIn the code chunk below, the heatmap is plotted by using hierachical clustering algorithm with “Euclidean distance” and “ward.D” method.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          dist_method = \"euclidean\",\n          hclust_method = \"ward.D\")\n\n\n\n\n\n\n\n2.5.5 Statistical approach\nIn order to determine the best clustering method and number of cluster the dend_expend() and find_k() functions of dendextend package will be used.\nFirst, the dend_expend() will be used to determine the recommended clustering method to be used.\n\nwh_d &lt;- dist(normalize(wh_matrix[, -c(1, 2, 4, 5)]), method = \"euclidean\")\ndend_expend(wh_d)[[3]]\n\n  dist_methods hclust_methods     optim\n1      unknown         ward.D 0.6137851\n2      unknown        ward.D2 0.6289186\n3      unknown         single 0.4774362\n4      unknown       complete 0.6434009\n5      unknown        average 0.6701688\n6      unknown       mcquitty 0.5020102\n7      unknown         median 0.5901833\n8      unknown       centroid 0.6338734\n\n\nThe output table shows that “average” method should be used because it gave the high optimum value.\nNext, find_k() is used to determine the optimal number of cluster.\n\nwh_clust &lt;- hclust(wh_d, method = \"average\")\nnum_k &lt;- find_k(wh_clust)\nplot(num_k)\n\n\n\n\n\n\n\n\nFigure above shows that k=3 would be good.\nWith reference to the statistical analysis results, we can prepare the code chunk as shown below.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          dist_method = \"euclidean\",\n          hclust_method = \"average\",\n          k_row = 3)\n\n\n\n\n\n\n\n2.5.6 Seriation\nOne of the problems with hierarchical clustering is that it doesn’t actually place the rows in a definite order, it merely constrains the space of possible orderings. Take three items A, B and C. If you ignore reflections, there are three possible orderings: ABC, ACB, BAC. If clustering them gives you ((A+B)+C) as a tree, you know that C can’t end up between A and B, but it doesn’t tell you which way to flip the A+B cluster. It doesn’t tell you if the ABC ordering will lead to a clearer-looking heatmap than the BAC ordering.\nheatmaply uses the seriation package to find an optimal ordering of rows and columns. Optimal means to optimize the Hamiltonian path length that is restricted by the dendrogram structure. This, in other words, means to rotate the branches so that the sum of distances between each adjacent leaf (label) will be minimized. This is related to a restricted version of the travelling salesman problem.\nHere we meet our first seriation algorithm: Optimal Leaf Ordering (OLO). This algorithm starts with the output of an agglomerative clustering algorithm and produces a unique ordering, one that flips the various branches of the dendrogram around so as to minimize the sum of dissimilarities between adjacent leaves. Here is the result of applying Optimal Leaf Ordering to the same clustering result as the heatmap above.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"OLO\")\n\n\n\n\n\nThe default options is “OLO” (Optimal leaf ordering) which optimizes the above criterion (in O(n^4)). Another option is “GW” (Gruvaeus and Wainer) which aims for the same goal but uses a potentially faster heuristic.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"GW\")\n\n\n\n\n\nThe option “mean” gives the output we would get by default from heatmap functions in other packages such as gplots::heatmap.2.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"mean\")\n\n\n\n\n\nThe option “none” gives us the dendrograms without any rotation that is based on the data matrix.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"none\")\n\n\n\n\n\n\n\n2.5.7 Working with colour palettes\nThe default colour palette uses by heatmaply is viridis. heatmaply users, however, can use other colour palettes in order to improve the aestheticness and visual friendliness of the heatmap.\nIn the code chunk below, the Blues colour palette of rColorBrewer is used\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"none\",\n          colors = Blues)\n\n\n\n\n\n\n\n2.5.8 The finishing touch\nBeside providing a wide collection of arguments for meeting the statistical analysis needs, heatmaply also provides many plotting features to ensure cartographic quality heatmap can be produced.\nIn the code chunk below the following arguments are used:\n\nk_row is used to produce 5 groups.\nmargins is used to change the top margin to 60 and row margin to 200.\nfontsizw_row and fontsize_col are used to change the font size for row and column labels to 4.\nmain is used to write the main title of the plot.\nxlab and ylab are used to write the x-axis and y-axis labels respectively.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          Colv=NA,\n          seriate = \"none\",\n          colors = Blues,\n          k_row = 5,\n          margins = c(NA,200,60,NA),\n          fontsize_row = 4,\n          fontsize_col = 5,\n          main=\"World Happiness Score and Variables by Country, 2018 \\nDataTransformation using Normalise Method\",\n          xlab = \"World Happiness Indicators\",\n          ylab = \"World Countries\"\n          )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hand-on_Ex05.html#overview-2",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hand-on_Ex05.html#overview-2",
    "title": "Hand-on exercise 5 Visual Analytics",
    "section": "3.1 Overview",
    "text": "3.1 Overview\nParallel coordinates plot is a data visualisation specially designed for visualising and analysing multivariate, numerical data. It is ideal for comparing multiple variables together and seeing the relationships between them. For example, the variables contribute to Happiness Index. Parallel coordinates was invented by Alfred Inselberg in the 1970s as a way to visualize high-dimensional data. This data visualisation technique is more often found in academic and scientific communities than in business and consumer data visualizations. As pointed out by Stephen Few(2006), “This certainly isn’t a chart that you would present to the board of directors or place on your Web site for the general public. In fact, the strength of parallel coordinates isn’t in their ability to communicate some truth in the data to others, but rather in their ability to bring meaningful multivariate patterns and comparisons to light when used interactively for analysis.” For example, parallel coordinates plot can be used to characterise clusters detected during customer segmentation.\nBy the end of this hands-on exercise, you will gain hands-on experience on:\n\nplotting statistic parallel coordinates plots by using ggparcoord() of GGally package,\nplotting interactive parallel coordinates plots by using parcoords package, and\nplotting interactive parallel coordinates plots by using parallelPlot package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hand-on_Ex05.html#installing-and-launching-r-packages-2",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hand-on_Ex05.html#installing-and-launching-r-packages-2",
    "title": "Hand-on exercise 5 Visual Analytics",
    "section": "3.2 Installing and Launching R Packages",
    "text": "3.2 Installing and Launching R Packages\nFor this exercise, the GGally, parcoords, parallelPlot and tidyverse packages will be used.\nThe code chunks below are used to install and load the packages in R.\n\npacman::p_load(GGally, parallelPlot, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hand-on_Ex05.html#data-preparation-1",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hand-on_Ex05.html#data-preparation-1",
    "title": "Hand-on exercise 5 Visual Analytics",
    "section": "3.3 Data Preparation",
    "text": "3.3 Data Preparation\nIn this hands-on exercise, the World Happinees 2018 (http://worldhappiness.report/ed/2018/) data will be used. The data set is download at https://s3.amazonaws.com/happiness-report/2018/WHR2018Chapter2OnlineData.xls. The original data set is in Microsoft Excel format. It has been extracted and saved in csv file called WHData-2018.csv.\nIn the code chunk below, read_csv() of readr package is used to import WHData-2018.csv into R and save it into a tibble data frame object called wh.\n\nwh &lt;- read_csv(\"data/WHData-2018.csv\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hand-on_Ex05.html#plotting-static-parallel-coordinates-plot",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hand-on_Ex05.html#plotting-static-parallel-coordinates-plot",
    "title": "Hand-on exercise 5 Visual Analytics",
    "section": "3.4 Plotting Static Parallel Coordinates Plot",
    "text": "3.4 Plotting Static Parallel Coordinates Plot\nIn this section, you will learn how to plot static parallel coordinates plot by using ggparcoord() of GGally package. Before getting started, it is a good practice to read the function description in detail.\n\n3.4.1 Plotting a simple parallel coordinates\nCode chunk below shows a typical syntax used to plot a basic static parallel coordinates plot by using ggparcoord().\n\nggparcoord(data = wh, \n           columns = c(7:12))\n\n\n\n\n\n\n\n\nNotice that only two argument namely data and columns is used. Data argument is used to map the data object (i.e. wh) and columns is used to select the columns for preparing the parallel coordinates plot.\n\n\n3.4.2 Plotting a parallel coordinates with boxplot\nThe basic parallel coordinates failed to reveal any meaning understanding of the World Happiness measures. In this section, you will learn how to makeover the plot by using a collection of arguments provided by ggparcoord().\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Parallel Coordinates Plot of World Happines Variables\")\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above.\n\ngroupColumn argument is used to group the observations (i.e. parallel lines) by using a single variable (i.e. Region) and colour the parallel coordinates lines by region name.\nscale argument is used to scale the variables in the parallel coordinate plot by using uniminmax method. The method univariately scale each variable so the minimum of the variable is zero and the maximum is one.\nalphaLines argument is used to reduce the intensity of the line colour to 0.2. The permissible value range is between 0 to 1.\nboxplot argument is used to turn on the boxplot by using logical TRUE. The default is FALSE.\ntitle argument is used to provide the parallel coordinates plot a title.\n\n\n\n3.4.3 Parallel coordinates with facet\nSince ggparcoord() is developed by extending ggplot2 package, we can combination use some of the ggplot2 function when plotting a parallel coordinates plot.\nIn the code chunk below, facet_wrap() of ggplot2 is used to plot 10 small multiple parallel coordinates plots. Each plot represent one geographical region such as East Asia.\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region)\n\n\n\n\n\n\n\n\nOne of the aesthetic defect of the current design is that some of the variable names overlap on x-axis.\n\n\n3.4.4 Rotating x-axis text label\nTo make the x-axis text label easy to read, let us rotate the labels by 30 degrees. We can rotate axis text labels using theme() function in ggplot2 as shown in the code chunk below\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region) + \n  theme(axis.text.x = element_text(angle = 30))\n\n\n\n\n\n\n\n\nThing to learn from the code chunk above:\n\nTo rotate x-axis text labels, we use axis.text.x as argument to theme() function. And we specify element_text(angle = 30) to rotate the x-axis text by an angle 30 degree.\n\n\n\n3.4.5 Adjusting the rotated x-axis text label\nRotating x-axis text labels to 30 degrees makes the label overlap with the plot and we can avoid this by adjusting the text location using hjust argument to theme’s text element with element_text(). We use axis.text.x as we want to change the look of x-axis text.\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region) + \n  theme(axis.text.x = element_text(angle = 30, hjust=1))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hand-on_Ex05.html#plotting-interactive-parallel-coordinates-plot-parallelplot-methods",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hand-on_Ex05.html#plotting-interactive-parallel-coordinates-plot-parallelplot-methods",
    "title": "Hand-on exercise 5 Visual Analytics",
    "section": "3.5 Plotting Interactive Parallel Coordinates Plot: parallelPlot methods",
    "text": "3.5 Plotting Interactive Parallel Coordinates Plot: parallelPlot methods\nparallelPlot is an R package specially designed to plot a parallel coordinates plot by using ‘htmlwidgets’ package and d3.js. In this section, you will learn how to use functions provided in parallelPlot package to build interactive parallel coordinates plot.\n\n3.5.1 The basic plot\nThe code chunk below plot an interactive parallel coordinates plot by using parallelPlot().\n\nwh &lt;- wh %&gt;%\n  select(\"Happiness score\", c(7:12))\nparallelPlot(wh,\n             width = 320,\n             height = 250)\n\n\n\n\n\nNotice that some of the axis labels are too long. You will learn how to overcome this problem in the next step.\n\n\n3.5.2 Rotate axis label\nIn the code chunk below, rotateTitle argument is used to avoid overlapping axis labels.\n\nparallelPlot(wh,\n             rotateTitle = TRUE)\n\n\n\n\n\nOne of the useful interactive feature of parallelPlot is we can click on a variable of interest, for example Happiness score, the monotonous blue colour (default) will change a blues with different intensity colour scheme will be used.\n\n\n3.5.3 Changing the colour scheme\nWe can change the default blue colour scheme by using continousCS argument as shown in the code chunl below.\n\nparallelPlot(wh,\n             continuousCS = \"YlOrRd\",\n             rotateTitle = TRUE)\n\n\n\n\n\n\n\n3.5.4 Parallel coordinates plot with histogram\nIn the code chunk below, histoVisibility argument is used to plot histogram along the axis of each variables.\n\nhistoVisibility &lt;- rep(TRUE, ncol(wh))\nparallelPlot(wh,\n             rotateTitle = TRUE,\n             histoVisibility = histoVisibility)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hand-on_Ex05.html#references",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hand-on_Ex05.html#references",
    "title": "Hand-on exercise 5 Visual Analytics",
    "section": "3.6 References",
    "text": "3.6 References\n\nggparcoord() of GGally package\nparcoords user guide\nparallelPlot"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hand-on_Ex05.html#overview-3",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hand-on_Ex05.html#overview-3",
    "title": "Hand-on exercise 5 Visual Analytics",
    "section": "4.1 Overview",
    "text": "4.1 Overview\nIn this hands-on exercise, you will gain hands-on experiences on designing treemap using appropriate R packages. The hands-on exercise consists of three main section. First, you will learn how to manipulate transaction data into a treemap strcuture by using selected functions provided in dplyr package. Then, you will learn how to plot static treemap by using treemap package. In the third section, you will learn how to design interactive treemap by using d3treeR package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hand-on_Ex05.html#installing-and-launching-r-packages-3",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hand-on_Ex05.html#installing-and-launching-r-packages-3",
    "title": "Hand-on exercise 5 Visual Analytics",
    "section": "4.2 Installing and Launching R Packages",
    "text": "4.2 Installing and Launching R Packages\nBefore we get started, you are required to check if treemap and tidyverse pacakges have been installed in you R.\n\npacman::p_load(treemap, treemapify, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hand-on_Ex05.html#data-wrangling",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hand-on_Ex05.html#data-wrangling",
    "title": "Hand-on exercise 5 Visual Analytics",
    "section": "4.3 Data Wrangling",
    "text": "4.3 Data Wrangling\nIn this exercise, REALIS2018.csv data will be used. This dataset provides information of private property transaction records in 2018. The dataset is extracted from REALIS portal (https://spring.ura.gov.sg/lad/ore/login/index.cfm) of Urban Redevelopment Authority (URA).\n\n4.3.1 Importing the data set\nIn the code chunk below, read_csv() of readr is used to import realis2018.csv into R and parsed it into tibble R data.frame format.\n\nrealis2018 &lt;- read_csv(\"data/realis2018.csv\")\n\nThe output tibble data.frame is called realis2018.\n\n\n4.3.2 Data Wrangling and Manipulation\nThe data.frame realis2018 is in trasaction record form, which is highly disaggregated and not appropriate to be used to plot a treemap. In this section, we will perform the following steps to manipulate and prepare a data.frtame that is appropriate for treemap visualisation:\n\ngroup transaction records by Project Name, Planning Region, Planning Area, Property Type and Type of Sale, and\ncompute Total Unit Sold, Total Area, Median Unit Price and Median Transacted Price by applying appropriate summary statistics on No. of Units, Area (sqm), Unit Price ($ psm) and Transacted Price ($) respectively.\n\nTwo key verbs of dplyr package, namely: group_by() and summarize() will be used to perform these steps.\ngroup_by() breaks down a data.frame into specified groups of rows. When you then apply the verbs above on the resulting object they’ll be automatically applied “by group”.\nGrouping affects the verbs as follows:\n\ngrouped select() is the same as ungrouped select(), except that grouping variables are always retained.\ngrouped arrange() is the same as ungrouped; unless you set .by_group = TRUE, in which case it orders first by the grouping variables.\nmutate() and filter() are most useful in conjunction with window functions (like rank(), or min(x) == x). They are described in detail in vignette(“window-functions”).\nsample_n() and sample_frac() sample the specified number/fraction of rows in each group.\nsummarise() computes the summary for each group.\n\nIn our case, group_by() will used together with summarise() to derive the summarised data.frame.\n\n\n\n\n\n\nNote\n\n\n\nRecommendation\n\nStudents who are new to dplyr methods should consult Introduction to dplyr before moving on to the next section.\n\n\n\n\n\n4.3.3 Grouped summaries without the Pipe\nThe code chank below shows a typical two lines code approach to perform the steps.\n\nrealis2018_grouped &lt;- group_by(realis2018, `Project Name`,\n                               `Planning Region`, `Planning Area`, \n                               `Property Type`, `Type of Sale`)\nrealis2018_summarised &lt;- summarise(realis2018_grouped, \n                          `Total Unit Sold` = sum(`No. of Units`, na.rm = TRUE),\n                          `Total Area` = sum(`Area (sqm)`, na.rm = TRUE),\n                          `Median Unit Price ($ psm)` = median(`Unit Price ($ psm)`, na.rm = TRUE), \n                          `Median Transacted Price` = median(`Transacted Price ($)`, na.rm = TRUE))\n\n\n\n\n\n\n\nImportant\n\n\n\nNote\n\nAggregation functions such as sum() and meadian() obey the usual rule of missing values: if there’s any missing value in the input, the output will be a missing value. The argument na.rm = TRUE removes the missing values prior to computation.\n\n\n\nThe code chunk above is not very efficient because we have to give each intermediate data.frame a name, even though we don’t have to care about it.\n\n\n4.3.4 Grouped summaries with the pipe\nThe code chunk below shows a more efficient way to tackle the same processes by using the pipe, %&gt;%:\n\n\n\n\n\n\nNote\n\n\n\nRecommendation\nTo learn more about pipe, visit this excellent article: Pipes in R Tutorial For Beginners.\n\n\n\nrealis2018_summarised &lt;- realis2018 %&gt;% \n  group_by(`Project Name`,`Planning Region`, \n           `Planning Area`, `Property Type`, \n           `Type of Sale`) %&gt;%\n  summarise(`Total Unit Sold` = sum(`No. of Units`, na.rm = TRUE), \n            `Total Area` = sum(`Area (sqm)`, na.rm = TRUE),\n            `Median Unit Price ($ psm)` = median(`Unit Price ($ psm)`, na.rm = TRUE),\n            `Median Transacted Price` = median(`Transacted Price ($)`, na.rm = TRUE))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hand-on_Ex05.html#designing-treemap-with-treemap-package",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hand-on_Ex05.html#designing-treemap-with-treemap-package",
    "title": "Hand-on exercise 5 Visual Analytics",
    "section": "4.4 Designing Treemap with treemap Package",
    "text": "4.4 Designing Treemap with treemap Package\ntreemap package is a R package specially designed to offer great flexibility in drawing treemaps. The core function, namely: treemap() offers at least 43 arguments. In this section, we will only explore the major arguments for designing elegent and yet truthful treemaps.\n\n4.4.1 Designing a static treemap\nIn this section, treemap() of Treemap package is used to plot a treemap showing the distribution of median unit prices and total unit sold of resale condominium by geographic hierarchy in 2017.\nFirst, we will select records belongs to resale condominium property type from realis2018_selected data frame.\n\nrealis2018_selected &lt;- realis2018_summarised %&gt;%\n  filter(`Property Type` == \"Condominium\", `Type of Sale` == \"Resale\")\n\n\n\n4.4.2 Using the basic arguments\nThe code chunk below designed a treemap by using three core arguments of treemap(), namely: index, vSize and vColor.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\nThings to learn from the three arguments used:\n\nindex\n\nThe index vector must consist of at least two column names or else no hierarchy treemap will be plotted.\nIf multiple column names are provided, such as the code chunk above, the first name is the highest aggregation level, the second name the second highest aggregation level, and so on.\n\nvSize\n\nThe column must not contain negative values. This is because it’s vaues will be used to map the sizes of the rectangles of the treemaps.\n\n\nWarning:\nThe treemap above was wrongly coloured. For a correctly designed treemap, the colours of the rectagles should be in different intensity showing, in our case, median unit prices.\nFor treemap(), vColor is used in combination with the argument type to determines the colours of the rectangles. Without defining type, like the code chunk above, treemap() assumes type = index, in our case, the hierarchy of planning areas.\n\n\n4.4.3 Working with vColor and type arguments\nIn the code chunk below, type argument is define as value.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type = \"value\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\nThinking to learn from the conde chunk above.\n\nThe rectangles are coloured with different intensity of green, reflecting their respective median unit prices.\nThe legend reveals that the values are binned into ten bins, i.e. 0-5000, 5000-10000, etc. with an equal interval of 5000.\n\n\n\n4.4.4 Colours in treemap package\nThere are two arguments that determine the mapping to color palettes: mapping and palette. The only difference between “value” and “manual” is the default value for mapping. The “value” treemap considers palette to be a diverging color palette (say ColorBrewer’s “RdYlBu”), and maps it in such a way that 0 corresponds to the middle color (typically white or yellow), -max(abs(values)) to the left-end color, and max(abs(values)), to the right-end color. The “manual” treemap simply maps min(values) to the left-end color, max(values) to the right-end color, and mean(range(values)) to the middle color.\n\n\n4.4.5 The “value” type treemap\nThe code chunk below shows a value type treemap.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"value\",\n        palette=\"RdYlBu\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\nThing to learn from the code chunk above:\n\nalthough the colour palette used is RdYlBu but there are no red rectangles in the treemap above. This is because all the median unit prices are positive.\nThe reason why we see only 5000 to 45000 in the legend is because the range argument is by default c(min(values, max(values)) with some pretty rounding.\n\n\n\n4.4.6 The “manual” type treemap\nThe “manual” type does not interpret the values as the “value” type does. Instead, the value range is mapped linearly to the colour palette.\nThe code chunk below shows a manual type treemap.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"RdYlBu\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above:\n\nThe colour scheme used is very copnfusing. This is because mapping = (min(values), mean(range(values)), max(values)). It is not wise to use diverging colour palette such as RdYlBu if the values are all positive or negative\n\nTo overcome this problem, a single colour palette such as Blues should be used.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\n\n\n4.4.7 Treemap Layout\ntreemap() supports two popular treemap layouts, namely: “squarified” and “pivotSize”. The default is “pivotSize”.\nThe squarified treemap algorithm (Bruls et al., 2000) produces good aspect ratios, but ignores the sorting order of the rectangles (sortID). The ordered treemap, pivot-by-size, algorithm (Bederson et al., 2002) takes the sorting order (sortID) into account while aspect ratios are still acceptable.\n\n\n4.4.8 Working with algorithm argument\nThe code chunk below plots a squarified treemap by changing the algorithm argument.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        algorithm = \"squarified\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\n\n\n4.4.9 Using sortID\nWhen “pivotSize” algorithm is used, sortID argument can be used to dertemine the order in which the rectangles are placed from top left to bottom right.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        algorithm = \"pivotSize\",\n        sortID = \"Median Transacted Price\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hand-on_Ex05.html#designing-treemap-using-treemapify-package",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hand-on_Ex05.html#designing-treemap-using-treemapify-package",
    "title": "Hand-on exercise 5 Visual Analytics",
    "section": "4.5 Designing Treemap using treemapify Package",
    "text": "4.5 Designing Treemap using treemapify Package\ntreemapify is a R package specially developed to draw treemaps in ggplot2. In this section, you will learn how to designing treemps closely resemble treemaps designing in previous section by using treemapify. Before you getting started, you should read Introduction to “treemapify” its user guide.\n\n4.5.1 Designing a basic treemap\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`),\n       layout = \"scol\",\n       start = \"bottomleft\") + \n  geom_treemap() +\n  scale_fill_gradient(low = \"light blue\", high = \"blue\")\n\n\n\n\n\n\n\n\n\n\n4.5.2 Defining hierarchy\nGroup by Planning Region\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`),\n       start = \"topleft\") + \n  geom_treemap()\n\n\n\n\n\n\n\n\nGroup by Planning Area\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`,\n           subgroup2 = `Planning Area`)) + \n  geom_treemap()\n\n\n\n\n\n\n\n\nAdding boundary line\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`,\n           subgroup2 = `Planning Area`)) + \n  geom_treemap() +\n  geom_treemap_subgroup2_border(colour = \"gray40\",\n                                size = 2) +\n  geom_treemap_subgroup_border(colour = \"gray20\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_ex01/Take-home_ex1.html",
    "href": "Take-home_Ex/Take-home_ex01/Take-home_ex1.html",
    "title": "Programme for International Student Assessment Education Survey (Singapore) : Exploratory Data Analysis",
    "section": "",
    "text": "This project examines the educational landscape in Singapore, with a focus on disparities among students from diverse backgrounds. Despite international recognition for achieving excellence with minimal differences between children from affluent and disadvantaged families, public perception suggests the existence of disparities. The project aims to investigate disparities between elite and neighborhood schools, students from varying socioeconomic backgrounds, and families with different immigration statuses. Insights gained from this study will contribute to an informed discussion on educational equity in Singapore.\n\n\n\nThe data used for this exercise was released on December 2022 from Programme for International Student Assessment(PISA). The organization survey every three years to observe the education systems worldwide through testing 15 year old students in the subjects of mathematics, reading, and science.\nThis for this exercise, will be using appropriate Exploratory Data Analysis (EDA)methods from R ggplot2 package to reveal:\n\nDistribution of Singapore students’ performance in mathematics, reading, and science\nRelationship between these performances with school, gender, and socioeconomic status of the students."
  },
  {
    "objectID": "Take-home_Ex/Take-home_ex01/Take-home_ex1.html#project-brief",
    "href": "Take-home_Ex/Take-home_ex01/Take-home_ex1.html#project-brief",
    "title": "Programme for International Student Assessment Education Survey (Singapore) : Exploratory Data Analysis",
    "section": "",
    "text": "This project examines the educational landscape in Singapore, with a focus on disparities among students from diverse backgrounds. Despite international recognition for achieving excellence with minimal differences between children from affluent and disadvantaged families, public perception suggests the existence of disparities. The project aims to investigate disparities between elite and neighborhood schools, students from varying socioeconomic backgrounds, and families with different immigration statuses. Insights gained from this study will contribute to an informed discussion on educational equity in Singapore."
  },
  {
    "objectID": "Take-home_Ex/Take-home_ex01/Take-home_ex1.html#project-objectives",
    "href": "Take-home_Ex/Take-home_ex01/Take-home_ex1.html#project-objectives",
    "title": "Programme for International Student Assessment Education Survey (Singapore) : Exploratory Data Analysis",
    "section": "",
    "text": "The data used for this exercise was released on December 2022 from Programme for International Student Assessment(PISA). The organization survey every three years to observe the education systems worldwide through testing 15 year old students in the subjects of mathematics, reading, and science.\nThis for this exercise, will be using appropriate Exploratory Data Analysis (EDA)methods from R ggplot2 package to reveal:\n\nDistribution of Singapore students’ performance in mathematics, reading, and science\nRelationship between these performances with school, gender, and socioeconomic status of the students."
  },
  {
    "objectID": "Take-home_Ex/Take-home_ex01/Take-home_ex1.html#install-load-r-packages",
    "href": "Take-home_Ex/Take-home_ex01/Take-home_ex1.html#install-load-r-packages",
    "title": "Programme for International Student Assessment Education Survey (Singapore) : Exploratory Data Analysis",
    "section": "2.1 Install & load R packages",
    "text": "2.1 Install & load R packages\nInstalling necessary R packages that will be needed for this exercise. To load the requried packages the code chunk below use pacman::p_load() function is used to unsure that the packages are load to the current R work environment.\n\npacman::p_load(tidyverse, ggdist, ggplot2, ggthemes, gridExtra, ggrepel, patchwork, hrbrthemes, dplyr)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_ex01/Take-home_ex1.html#importing-pisa-data",
    "href": "Take-home_Ex/Take-home_ex01/Take-home_ex1.html#importing-pisa-data",
    "title": "Programme for International Student Assessment Education Survey (Singapore) : Exploratory Data Analysis",
    "section": "2.2 Importing PISA Data",
    "text": "2.2 Importing PISA Data\nFor this exercise, we are only examining the Singapore students. So the data load in the below code chunk only contains\n\nsg_stu &lt;- readRDS(\"C:/lzc0313/ISSS608-VAA/In-class_Ex/In-class_ex1/data/stu_qqq_SG.rds\")\n\nAfter examine the data using dim(),names(),summary() functions found that most of the variables from the survey is not needed for this exercise, to make it more convinient and efficient, we only extract the variables needed for this exercise as a new table for later use.\n\n# Selecting specific columns from sg_stu\nselected_columns &lt;- sg_stu[, c(\"CNTSCHID\", \"ST004D01T\", \"CNTSTUID\",\"ESCS\")]\n\n# Calculate the mean for each subject\nmean_pv_math &lt;- rowMeans(sg_stu[, c(\"PV1MATH\", \"PV2MATH\", \"PV3MATH\", \"PV4MATH\", \"PV5MATH\", \n                                    \"PV6MATH\", \"PV7MATH\", \"PV8MATH\", \"PV9MATH\", \"PV10MATH\")], na.rm = TRUE)\n\nmean_pv_read &lt;- rowMeans(sg_stu[, c(\"PV1READ\", \"PV2READ\", \"PV3READ\", \"PV4READ\", \"PV5READ\", \n                                    \"PV6READ\", \"PV7READ\", \"PV8READ\", \"PV9READ\", \"PV10READ\")], na.rm = TRUE)\n\nmean_pv_scie &lt;- rowMeans(sg_stu[, c(\"PV1SCIE\", \"PV2SCIE\", \"PV3SCIE\", \"PV4SCIE\", \"PV5SCIE\", \n                                    \"PV6SCIE\", \"PV7SCIE\", \"PV8SCIE\", \"PV9SCIE\", \"PV10SCIE\")], na.rm = TRUE)\n\n# Combine selected columns and new calculations into sg_stu_eda\nsg_stu_eda &lt;- cbind(selected_columns, mean_pv_math, mean_pv_read, mean_pv_scie)\n\nTotal of 7 Variables are used\n\nCNTSCHID, (Intl. School ID)\nST004D01T, (Students gender)\nCNTSTUID, (Intl. Student ID)\nESCS, (Index of economic, social, and culture status)\nmean_pv_math, (average of plausible value 1-10 of mathematics)\nmean_pv_read, (average of plausible value 1-10 of reading)\nmean_pv_scie, (average of plausible value 1-10 of science)\n\n\nhead(sg_stu_eda,10)\n\n   CNTSCHID ST004D01T CNTSTUID    ESCS mean_pv_math mean_pv_read mean_pv_scie\n1  70200052         1 70200001  0.1836     605.2533     667.4296     639.7873\n2  70200134         2 70200002  0.8261     689.9528     627.6078     672.0703\n3  70200112         2 70200003 -1.0357     676.7768     582.9252     660.0384\n4  70200004         2 70200004 -0.9606     401.0528     361.3969     343.6425\n5  70200152         1 70200005  0.0856     436.1151     475.6763     479.2390\n6  70200043         1 70200006  0.1268     518.1055     431.4652     476.0031\n7  70200049         2 70200007 -0.0154     707.9552     704.2219     646.9278\n8  70200107         2 70200008  1.1558     620.4136     524.4010     573.1763\n9  70200012         1 70200009  1.4654     735.7920     713.1340     691.0124\n10 70200061         2 70200010  0.5199     555.4636     504.6734     548.3434"
  },
  {
    "objectID": "Take-home_Ex/Take-home_ex01/Take-home_ex1.html#distribution-anaylsis",
    "href": "Take-home_Ex/Take-home_ex01/Take-home_ex1.html#distribution-anaylsis",
    "title": "Programme for International Student Assessment Education Survey (Singapore) : Exploratory Data Analysis",
    "section": "3.1 Distribution Anaylsis",
    "text": "3.1 Distribution Anaylsis\nBefore doing any further analysis, it is always good to look at the distribution of the data, so we can look at the distribution of the three subjects using histograms.\n\nDistribution of subjectscodes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nh1 &lt;- ggplot(data=sg_stu_eda, aes(x = mean_pv_math)) +\n  geom_histogram(bins=20, color=\"grey25\", fill=\"grey90\") + \n  ggtitle(\"Math\")\nh2 &lt;- ggplot(data=sg_stu_eda, aes(x = mean_pv_scie)) +\n  geom_histogram(bins=20, color=\"grey25\", fill=\"grey90\") +\n  ggtitle(\"Science \")\nh3 &lt;- ggplot(data=sg_stu_eda, aes(x = mean_pv_read)) +\n  geom_histogram(bins=20, color=\"grey25\", fill=\"grey90\") +\n  ggtitle(\"Reading\")\n\npatchwork &lt;- h1 + h2 + h3\npatchwork &lt;- patchwork + plot_layout(ncol = 3)\npatchwork & theme_economist()\n\n\n\n\nThe analysis of Singapore students’ performance across mathematics, science, and reading reveals a notable strength in mathematics, evidenced by a maximum score of 842.72 that surpasses the top scores in science (801.9) and reading (797.59). The data indicates a wider range of abilities in reading, as seen by the lowest minimum score of 158.5626, compared to mathematics (262.564) and science (242.0194), suggesting that students excel in numerical and scientific reasoning."
  },
  {
    "objectID": "Take-home_Ex/Take-home_ex01/Take-home_ex1.html#stacked-density-plot",
    "href": "Take-home_Ex/Take-home_ex01/Take-home_ex1.html#stacked-density-plot",
    "title": "Programme for International Student Assessment Education Survey (Singapore) : Exploratory Data Analysis",
    "section": "3.2 Stacked Density Plot",
    "text": "3.2 Stacked Density Plot\nExamining distributions individually might not provide a clear comparison; however, a stacked density plot could facilitate a more straightforward comparison across the subjects by overlaying their distributions.\n\nStacked Density plotCodes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(sg_stu_eda) +\n  geom_density(aes(x = mean_pv_math, fill = \"Mathematics\"), alpha = 0.5) +\n  geom_density(aes(x = mean_pv_read, fill = \"Reading\"), alpha = 0.5) +\n  geom_density(aes(x = mean_pv_scie, fill = \"Science\"), alpha = 0.5) +\n  ggtitle(\"Density Plot of Scores by Subject\") +\n  xlab(\"Scores\") +\n  ylab(\"Density\") +\n  scale_fill_manual(values = c(\"Mathematics\" = \"red\", \"Reading\" = \"green\", \"Science\" = \"blue\"))\n\n\n\n\nThe density plot reinforces the findings from the distribution analysis, showing that Mathematics is a standout subject for Singapore students, with a distribution indicating not only higher peak scores but also a broader range of high achievers. The overlap between Science and Reading suggests similar performance levels in these subjects, with a less pronounced rightward extension than Mathematics, implying fewer high scorers. The shape and spread of the distributions indicate variability in student performance across all subjects, with Mathematics displaying a potential for both higher peaks and greater spread, hinting at a wider variance in achievement. These visual cues complement the earlier analysis, underscoring Mathematics as a strong suit in the Singapore education system while highlighting the need for continued focus on literacy to address the lower performance and greater variability observed in Reading."
  },
  {
    "objectID": "Take-home_Ex/Take-home_ex01/Take-home_ex1.html#school-impact-on-singapore-student-subjects-score",
    "href": "Take-home_Ex/Take-home_ex01/Take-home_ex1.html#school-impact-on-singapore-student-subjects-score",
    "title": "Programme for International Student Assessment Education Survey (Singapore) : Exploratory Data Analysis",
    "section": "3.3 School Impact on Singapore Student Subjects Score",
    "text": "3.3 School Impact on Singapore Student Subjects Score\nIt is also important to examine whether students in different schools exhibit variations in their performance across various subjects, as some schools may excel in teaching certain subjects compared to others. Since the school id has a very large sample, we are using a heatmap to display the significance.\n\nHeatmapCodes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsg_stu_eda_long &lt;- sg_stu_eda %&gt;%\n  pivot_longer(\n    cols = starts_with(\"mean_pv_\"), \n    names_to = \"subject\", \n    values_to = \"score\"\n  ) %&gt;%\n  mutate(CNTSCHID = as.factor(CNTSCHID), \n         subject = factor(subject, levels = c(\"mean_pv_math\", \"mean_pv_read\", \"mean_pv_scie\"),\n                          labels = c(\"Mathematics\", \"Reading\", \"Science\")))\n\nggplot(sg_stu_eda_long, aes(x = CNTSCHID, y = subject, fill = score)) +\n  geom_tile() +\n  scale_fill_gradient(low = \"white\", high = \"blue\") +  # Adjust the color scale as needed\n  theme_minimal() +\n  labs(x = \"School ID\", y = \"Subject\", \n       title = \"Heatmap of Subject Scores Across Schools\")\n\n\n\n\nFrom the heatmap presented earlier, several noteworthy observations can be made. Firstly, it’s evident that students from certain schools consistently maintain similar scores across all three subjects. However, in other schools, there is a noticeable trend where science and mathematics scores are higher, while reading scores appear to be comparatively lower.\nFurthermore, it’s interesting to note that schools achieving high scores in mathematics also tend to perform well in science, suggesting a potential correlation between these two subjects. Additionally, there are instances, albeit rare, where certain schools excel in reading, outperforming their scores in the other two subjects. These observations indicate the presence of distinct patterns and potentially valuable insights into the performance of students across different schools and subjects."
  },
  {
    "objectID": "Take-home_Ex/Take-home_ex01/Take-home_ex1.html#gender-impact-on-singapore-student-subjects-score",
    "href": "Take-home_Ex/Take-home_ex01/Take-home_ex1.html#gender-impact-on-singapore-student-subjects-score",
    "title": "Programme for International Student Assessment Education Survey (Singapore) : Exploratory Data Analysis",
    "section": "3.4 Gender Impact on Singapore Student Subjects Score",
    "text": "3.4 Gender Impact on Singapore Student Subjects Score\nNext, we examine the gender impact on Singapore student’s different subject scores, as from what most people typically knows different gender perform differently in certain subjects, so it is important to check that if gender shows significance on the subjects score.\n\nBoxplotCodes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsg_stu_eda_long$ST004D01T &lt;- factor(sg_stu_eda_long$ST004D01T, levels = c(\"1\", \"2\"), labels = c(\"Female\", \"Male\"))\n\nggplot(sg_stu_eda_long, aes(x = ST004D01T, y = score, fill = subject)) +\n  geom_boxplot(position = position_dodge(1)) +\n  stat_summary(fun = mean, geom = \"point\", shape = 20, size = 3, color = \"darkred\", position = position_dodge(1)) +\n  stat_summary(fun = mean, geom = \"text\", aes(label = round(..y.., 1)), vjust = 1.5, color=\"darkblue\",position=position_dodge(1)) +\n  stat_summary(fun = median, geom = \"line\", size = 0.5, color = \"darkblue\", position = position_dodge(1)) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1), \n        legend.position = \"bottom\") +\n  labs(x = \"Gender\", y = \"Score\", \n       title = \"Boxplot of Subject Scores Across Gender\", \n       fill = \"Subjects\")\n\n\n\n\n\nMathematics: Males exhibit a higher average score compared to females, with the mean score for males being approximately 11 points higher. This suggests a notable performance advantage for males in mathematics within this sample.\nReading: Females outperform males by a substantial margin, with their average score being approximately 20 points higher. This considerable difference highlights a stronger performance in reading for females.\nScience: Males again have a higher average score, with the difference being approximately 6 points. While this advantage is less pronounced than in mathematics, it still indicates better performance by males in science.\n\nThe interquartile range (IQR), which represents the middle 50% of scores, are different across genders for each subject. Males tend to have larger IQR across all subject compare to female, a larger IQR indicates more variability in student performance.The presence of outliers, as indicated by points beyond the whiskers of the boxplots, suggesting that there are students with scores that are unusually low or high compared to their peers.\nFurther analysis might be needed: Adding confidence intervals around the mean could provide insight into the statistical significance of the differences in means between genders."
  },
  {
    "objectID": "Take-home_Ex/Take-home_ex01/Take-home_ex1.html#index-of-economic-social-cultural-status-impact-on-singapore-student-subjects-score",
    "href": "Take-home_Ex/Take-home_ex01/Take-home_ex1.html#index-of-economic-social-cultural-status-impact-on-singapore-student-subjects-score",
    "title": "Programme for International Student Assessment Education Survey (Singapore) : Exploratory Data Analysis",
    "section": "3.5 Index of Economic, Social, Cultural Status Impact on Singapore Student Subjects Score",
    "text": "3.5 Index of Economic, Social, Cultural Status Impact on Singapore Student Subjects Score\nFrom the dataset, it is observe that the Index of Economic, Social, Cultural Status has differnt positive and negative values, and it seem that differnt values correspond with different range of subject scores. So, there might be relationship between this two variables, let use some EDA techniques to explore.\nFirst, since the ESCS value as a wide range, we can use bin function ntile() to divide ESCS values into quantiles.\n\ndata_escs &lt;- sg_stu_eda %&gt;%\n  filter(!is.na(ESCS)) %&gt;% # Remove rows with NA in ESCS\n  mutate(ESCS_bin = ntile(ESCS, 8)) # Bin ESCS into 8 bins\n\n# Now, summarize to find the range of ESCS in each bin\nbin_ranges &lt;- data_escs %&gt;%\n  group_by(ESCS_bin) %&gt;%\n  summarise(\n    min_ESCS = min(ESCS, na.rm = TRUE),\n    max_ESCS = max(ESCS, na.rm = TRUE)\n  )\n\nbin_ranges\n\n# A tibble: 8 × 3\n  ESCS_bin min_ESCS max_ESCS\n     &lt;int&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1        1   -3.55    -0.788\n2        2   -0.786   -0.233\n3        3   -0.232    0.174\n4        4    0.174    0.482\n5        5    0.482    0.706\n6        6    0.706    0.904\n7        7    0.904    1.14 \n8        8    1.14     3.28 \n\n\n\nBoxplot Subject vs ESCS binsCodes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data_escs, aes(x = factor(ESCS_bin), y = mean_pv_math)) +\n  geom_boxplot() +\n  labs(x = 'ESCS Bin', y = 'Math Score') +\n  theme_bw()\nggplot(data_escs, aes(x = factor(ESCS_bin), y = mean_pv_scie)) +\n  geom_boxplot() +\n  labs(x = 'ESCS Bin', y = 'Science Score') +\n  theme_bw()\nggplot(data_escs, aes(x = factor(ESCS_bin), y = mean_pv_read)) +\n  geom_boxplot() +\n  labs(x = 'ESCS Bin', y = 'Reading Score') +\n  theme_bw()\n\n\n\n\nFrom the exploratory data analysis (EDA), it is evident that as the Index of Economic, Social, Cultural Status (ESCS) increases from bin 1 to bin 8, the scores in all three subjects exhibit a consistent upward trend. This observation holds true despite the presence of some outliers within each bin, which can affect the overall sample distribution. Furthermore, it is notable that the interquartile range (IQR) increases progressively as we move from lower ESCS bins to higher ones, indicating a widening spread of scores. This widening IQR suggests a greater degree of variability in scores among students as the ESCS index increases, reflecting the diversity in academic performance among students with different socio-economic backgrounds."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hand-on_Ex01.html",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hand-on_Ex01.html",
    "title": "Hands-on Exercise 1",
    "section": "",
    "text": "The code chunk below used p_load() of pacman package to check if tidyverse packages are installed in the system. if they are,then they will be launched into R.\n\npacman::p_load(tidyverse)\n\n\n\n\n\nexam_data &lt;- read_csv(\"data/Exam_data.csv\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hand-on_Ex01.html#install-and-launching-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hand-on_Ex01.html#install-and-launching-r-packages",
    "title": "Hands-on Exercise 1",
    "section": "",
    "text": "The code chunk below used p_load() of pacman package to check if tidyverse packages are installed in the system. if they are,then they will be launched into R.\n\npacman::p_load(tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hand-on_Ex01.html#importing-the-data",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hand-on_Ex01.html#importing-the-data",
    "title": "Hands-on Exercise 1",
    "section": "",
    "text": "exam_data &lt;- read_csv(\"data/Exam_data.csv\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hand-on_Ex02.html",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hand-on_Ex02.html",
    "title": "Hands-on Exercise 2",
    "section": "",
    "text": "This exercise will be introduced to several ggplot2 extension for creating statistical graphics."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hand-on_Ex02.html#install-and-load-r-libraries",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hand-on_Ex02.html#install-and-load-r-libraries",
    "title": "Hands-on Exercise 2",
    "section": "2.1 Install and Load R Libraries",
    "text": "2.1 Install and Load R Libraries\nBeside tidyverse which is introduced in hands on exercies 1, four more R packages will be used. They are:\n\nggrepel: an R package provides geoms for ggplot2 to repel overlapping text labels.\nggthemes: an R package provides some extra themes, geoms, and scales for ‘ggplot2’.\nhrbrthemes: an R package provides typography-centric themes and theme components for ggplot2.\npatchwork: an R package for preparing composite figure created using ggplot2.\n\nCode Chunk below is used to check the packages are installed and also load them into the current R environment.\n\npacman::p_load(ggrepel, patchwork, ggthemes, hrbrthemes, tidyverse)\n\nAfter checking there is no issues with installing and uploading the packages, we proceed to import the data used for this exercise."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hand-on_Ex02.html#importing-data",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hand-on_Ex02.html#importing-data",
    "title": "Hands-on Exercise 2",
    "section": "2.2 Importing Data",
    "text": "2.2 Importing Data\nFor this exercise, Exam_data.csv is introduced. It consist of year end examination grades of a cohort of primary 3 students from a local school.\nCode chunk below imports the data mentioned above into current R environment using read_csv() function of readr package. readr is one of the tidyverse package.\n\nexam_data &lt;- read_csv(\"C:/lzc0313/ISSS608-VAA/Hands-on_Ex/Hands-on_Ex01/data/Exam_data.csv\")\n\nThere are 7 attributes and 322 columns in the exam_data. Four of the attributes are categorical data type and others are in continuous data type.\n\nThe categorical attributes are: ID, CLASS, GENDER and RACE.\nThe continuous attributes are: MATHS, ENGLISH and SCIENCE."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hand-on_Ex02.html#beyond-ggplot2-annotation-ggrepel",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hand-on_Ex02.html#beyond-ggplot2-annotation-ggrepel",
    "title": "Hands-on Exercise 2",
    "section": "2.3 Beyond ggplot2 Annotation: ggrepel",
    "text": "2.3 Beyond ggplot2 Annotation: ggrepel\nOne of the challenge in plotting statistical graph is annotation, especially with large number of data points.\n\n# Your ggplot code\nggplot(data=exam_data, \n       aes(x= MATHS, \n           y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              size=0.5) +  \n  geom_label(aes(label = ID), \n             hjust = .5, \n             vjust = -.5) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) +\n  ggtitle(\"English scores versus Maths scores for Primary 3\")\n\n\n\n\n\n\n\n\nggrepel is an extension of ggplot2 which provides geoms for ggplot2 to repel overlapping text as in the above graph.\nWe simply replace geom_text() by geom_text_repel() and geom_label() by geom_label_repel.\n\n2.3.1 Working with ggrepel\n\nggplot(data=exam_data, \n       aes(x= MATHS, \n           y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              size=0.5) +  \n  geom_label_repel(aes(label = ID), \n                   fontface = \"bold\") +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) +\n  ggtitle(\"English scores versus Maths scores for Primary 3\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hand-on_Ex02.html#beyond-ggplot2-themes",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hand-on_Ex02.html#beyond-ggplot2-themes",
    "title": "Hands-on Exercise 2",
    "section": "2.4 Beyond ggplot2 Themes",
    "text": "2.4 Beyond ggplot2 Themes\nThere are 8 build-in themes in ggplot2, they are:theme_gray(), theme_bw(), theme_classic(), theme_dark(), theme_light(), theme_linedraw(), theme_minimal(), and theme_void().\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  theme_void() +\n  ggtitle(\"Distribution of Maths scores\") \n\n\n\n\n\n\n\n\nMore about ggplot2 themes (link).\n\n2.4.1 Working with ggtheme package\nggthemes provides ‘ggplot2’ themes that replicate the look of plots by Edward Tufte, Stephen Few, Fivethirtyeight, The Economist, ‘Stata’, ‘Excel’, and The Wall Street Journal, among others.\nBelow is the example of a economist theme.\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  ggtitle(\"Distribution of Maths scores\") +\n  theme_economist()\n\n\n\n\n\n\n\n\n\n\n2.4.2 hrbthemes package\nhrbrthemes package provides a base theme that focuses on typographic elements, including where various labels are placed as well as the fonts that are used.\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  ggtitle(\"Distribution of Maths scores\") +\n  theme_ipsum()\n\n\n\n\n\n\n\n\nThe second goal of hrbrthemes centers around the productivity for a production workflow. learn more\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  ggtitle(\"Distribution of Maths scores\") +\n  theme_ipsum(axis_title_size = 18,\n              base_size = 15,\n              grid = \"Y\")\n\n\n\n\n\n\n\n\n\n\n\nWhat can we learn from the above code chunk?\n\n\n\naxis_title_size argument is used to increase the font size of the axis title to 18,\nbase_size argument is used to increase the default axis label to 15, and\ngrid argument is used to remove the x-axis grid lines."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hand-on_Ex02.html#beyond-single-graph",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hand-on_Ex02.html#beyond-single-graph",
    "title": "Hands-on Exercise 2",
    "section": "2.5 Beyond Single Graph",
    "text": "2.5 Beyond Single Graph\nIt is not unusual that multiple graphs are required to tell a compelling visual story. There are several ggplot2 extensions provide functions to compose figure with multiple graphs. In this section, you will learn how to create composite plot by combining multiple graphs. First, let us create three statistical graphics by using the code chunk below.\n\np1 &lt;- ggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") + \n  coord_cartesian(xlim=c(0,100)) +\n  ggtitle(\"Distribution of Maths scores\")\n\nNext,\n\np2 &lt;- ggplot(data=exam_data, \n             aes(x = ENGLISH)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  coord_cartesian(xlim=c(0,100)) +\n  ggtitle(\"Distribution of English scores\")\n\nLastly, a scatterplot.\n\np3 &lt;- ggplot(data=exam_data, \n             aes(x= MATHS, \n                 y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              size=0.5) +  \n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) +\n  ggtitle(\"English scores versus Maths scores for Primary 3\")\n\n\n2.5.1 Creating Composite Graphics: pathwork methods\nPathwork from the ggplot2 extension is specially designed for combining separate ggplot2 graphs into a single figure.\nPatchwork package has a very simple syntax where we can create layouts super easily. Here’s the general syntax that combines:\n\nTwo-Column Layout using the Plus Sign +.\nParenthesis () to create a subplot group.\nTwo-Row Layout using the Division Sign /\n\n\n\n2.5.2 Combining two ggplot2 graphs\nbelow codes combines two histogram.\n\np1 + p2\n\n\n\n\n\n\n\n\n\n\n2.5.3 Combining three ggplot2 graphs\nWe can plot more complex composite by using appropriate operators. For example, the composite figure below is plotted by using:\n\n“|” operator to stack two ggplot2 graphs,\n“/” operator to place the plots beside each other,\n“()” operator the define the sequence of the plotting.\n\n\n(p1 / p2) | p3\n\n\n\n\n\n\n\n\nMore about this topics, refer to Plot Assembly.\n2.5.4 Creating a composite figure with tag\nTo identify the subplots in text, patchwork also provides auto-tagging capabilities shown below.\n\n((p1 / p2) | p3) + \n  plot_annotation(tag_levels = 'I')\n\n\n\n\n\n\n\n\n2.5.5 Creating figure with insert\nOther than putting the graphs side by side, we can put graph on top or below one another using inset_element() of patchwork.\n\np3 + inset_element(p2, \n                   left = 0.02, \n                   bottom = 0.7, \n                   right = 0.5, \n                   top = 1)\n\n\n\n\n\n\n\n\n\n\n2.5.6 Creating composite figure using patchwork and ggtheme\nFigure below is created by combining patchwork and theme_economist() of ggthemes package discussed earlier.\n\npatchwork &lt;- (p1 / p2) | p3\npatchwork & theme_economist()"
  },
  {
    "objectID": "In-class_Ex/in-class-6/inclass_ex6.html",
    "href": "In-class_Ex/in-class-6/inclass_ex6.html",
    "title": "In class Exercise 6 Time on the Horizon",
    "section": "",
    "text": "Before getting start, make sure that ggHoriPlot has been included in the pacman::p_load(...) statement above.\n\npacman::p_load(ggHoriPlot, ggthemes, tidyverse)\n\n\n\nFor the purpose of this hands-on exercise, Average Retail Prices Of Selected Consumer Items will be used.\nUse the code chunk below to import the AVERP.csv file into R environment.\n\naverp &lt;- read_csv(\"data/AVERP.csv\") %&gt;%   \n  mutate(`Date` = dmy(`Date`))\n\nThing to learn from the code chunk above.\n\nBy default, read_csv will import data in Date field as Character data type. dmy() of lubridate package to palse the Date field into appropriate Date data type in R.\n\n\n\n\n\n\n\nTip\n\n\n\nThing to learn from the code chunk above.\n\nBy default, read_csv will import data in Date field as Character data type. dmy() of lubridate package to palse the Date field into appropriate Date data type in R.\n\n\n\n\n\n\nNext, the code chunk below will be used to plot the horizon graph.\n\naverp %&gt;% \n  filter(Date &gt;= \"2018-01-01\") %&gt;%\n  ggplot() +\n  geom_horizon(aes(x = Date, y=Values), \n               origin = \"midpoint\", \n               horizonscale = 6)+\n  facet_grid(`Consumer Items`~.) +\n    theme_few() +\n  scale_fill_hcl(palette = 'RdBu') +   #diverging color red and blue.\n  theme(panel.spacing.y=unit(0, \"lines\"), strip.text.y = element_text(\n    size = 5, angle = 0, hjust = 0),\n    legend.position = 'none',\n    axis.text.y = element_blank(),\n    axis.text.x = element_text(size=7),\n    axis.title.y = element_blank(),\n    axis.title.x = element_blank(),\n    axis.ticks.y = element_blank(),\n    panel.border = element_blank()\n    ) +\n    scale_x_date(expand=c(0,0), date_breaks = \"3 month\", date_labels = \"%b%y\") +\n  ggtitle('Average Retail Prices of Selected Consumer Items (Jan 2018 to Dec 2022)')\n\n\n\n\n\n\n\n\nattacks &lt;- attacks %&gt;%\nmutate(wday = lubridate::wday(timestamp,\nlabel =True\nabbr= True,),\nhour = lubridate::hour(timestamp))\nthe code chunk above avoid to change date format."
  },
  {
    "objectID": "In-class_Ex/in-class-6/inclass_ex6.html#getting-started",
    "href": "In-class_Ex/in-class-6/inclass_ex6.html#getting-started",
    "title": "In class Exercise 6 Time on the Horizon",
    "section": "",
    "text": "Before getting start, make sure that ggHoriPlot has been included in the pacman::p_load(...) statement above.\n\npacman::p_load(ggHoriPlot, ggthemes, tidyverse)\n\n\n\nFor the purpose of this hands-on exercise, Average Retail Prices Of Selected Consumer Items will be used.\nUse the code chunk below to import the AVERP.csv file into R environment.\n\naverp &lt;- read_csv(\"data/AVERP.csv\") %&gt;%   \n  mutate(`Date` = dmy(`Date`))\n\nThing to learn from the code chunk above.\n\nBy default, read_csv will import data in Date field as Character data type. dmy() of lubridate package to palse the Date field into appropriate Date data type in R.\n\n\n\n\n\n\n\nTip\n\n\n\nThing to learn from the code chunk above.\n\nBy default, read_csv will import data in Date field as Character data type. dmy() of lubridate package to palse the Date field into appropriate Date data type in R.\n\n\n\n\n\n\nNext, the code chunk below will be used to plot the horizon graph.\n\naverp %&gt;% \n  filter(Date &gt;= \"2018-01-01\") %&gt;%\n  ggplot() +\n  geom_horizon(aes(x = Date, y=Values), \n               origin = \"midpoint\", \n               horizonscale = 6)+\n  facet_grid(`Consumer Items`~.) +\n    theme_few() +\n  scale_fill_hcl(palette = 'RdBu') +   #diverging color red and blue.\n  theme(panel.spacing.y=unit(0, \"lines\"), strip.text.y = element_text(\n    size = 5, angle = 0, hjust = 0),\n    legend.position = 'none',\n    axis.text.y = element_blank(),\n    axis.text.x = element_text(size=7),\n    axis.title.y = element_blank(),\n    axis.title.x = element_blank(),\n    axis.ticks.y = element_blank(),\n    panel.border = element_blank()\n    ) +\n    scale_x_date(expand=c(0,0), date_breaks = \"3 month\", date_labels = \"%b%y\") +\n  ggtitle('Average Retail Prices of Selected Consumer Items (Jan 2018 to Dec 2022)')\n\n\n\n\n\n\n\n\nattacks &lt;- attacks %&gt;%\nmutate(wday = lubridate::wday(timestamp,\nlabel =True\nabbr= True,),\nhour = lubridate::hour(timestamp))\nthe code chunk above avoid to change date format."
  },
  {
    "objectID": "In-class_Ex/in_class7/inclasss7.html",
    "href": "In-class_Ex/in_class7/inclasss7.html",
    "title": "In-class 7 Visual Analytics",
    "section": "",
    "text": "pacman::p_load(sf, terra, gstat, automap,\n               tmap, viridis, tidyverse)\nrfstations &lt;- read.csv('data/aspatial/RainfallStation.csv')\nrfdata &lt;- read_csv(\"data/aspatial/DAILYDATA_202402.csv\") %&gt;%\n  select(c(1,5)) %&gt;%\n  group_by(Station) %&gt;%\n  summarise(MONTHSUM = sum (`Daily Rainfall Total (mm)`)) %&gt;%\n  ungroup()\nrfdata&lt;- rfdata  %&gt;%\n  left_join(rfstations)\nrfdata_sf &lt;- st_as_sf(rfdata,\n                      coords = c('Longitude',\n                                 'Latitude'),\n                      crs= 4326) %&gt;%\n  st_transform(crs = 3414)\nmpsz2019 &lt;- st_read(dsn = 'data/geospatial',\n                    layer = 'MPSZ-2019')%&gt;%\n  st_transform(crs = 3414)\n\nReading layer `MPSZ-2019' from data source \n  `C:\\lzc0313\\ISSS608-VAA\\In-class_Ex\\in_class7\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84\ntmap_options(check.and.fix = TRUE)\ntmap_mode(\"view\")\ntm_shape(rfdata_sf) +\n  tm_dots(col = \"red\")\ntmap_options(check.and.fix = TRUE)\ntmap_mode('view')\ntm_shape(mpsz2019) +\n  tm_borders()+\ntm_shape(rfdata_sf) +\n  tm_dots(col = 'MONTHSUM')\n\n\n\n\ntmap_mode('plot')"
  },
  {
    "objectID": "In-class_Ex/in_class7/inclasss7.html#spatial-interpolation-gstat-method",
    "href": "In-class_Ex/in_class7/inclasss7.html#spatial-interpolation-gstat-method",
    "title": "In-class 7 Visual Analytics",
    "section": "Spatial Interpolation: gstat method",
    "text": "Spatial Interpolation: gstat method\nIn this section, you will gain hands-on experience on performing spatial interpolation by using gstat package. In order to perform spatial interpolation by using gstat, we first need to create an object of class called gstat, using a function of the same name: gstat. A gstat object contains all necessary information to conduct spatial interpolation, namely:\n\nThe model definition\nThe calibration data\n\nBased on its arguments, the gstat function “understands” what type of interpolation model we want to use:\n\nNo variogram model → IDW\nVariogram model, no covariates → Ordinary Kriging\nVariogram model, with covariates → Universal Kriging\n\nThe complete decision tree of gstat, including several additional methods which we are not going to use, is shown in the figure below.\n\nData preparation\nTo getting start, we need create a grid data object by using rast() of terra package as shown in the cod chunk below.\n\ngrid &lt;- terra::rast(mpsz2019, \n                    nrows = 690, \n                    ncols = 1075)\ngrid\n\nclass       : SpatRaster \ndimensions  : 690, 1075, 1  (nrow, ncol, nlyr)\nresolution  : 49.98037, 50.01103  (x, y)\nextent      : 2667.538, 56396.44, 15748.72, 50256.33  (xmin, xmax, ymin, ymax)\ncoord. ref. : SVY21 / Singapore TM (EPSG:3414) \n\n\nNext, a list called xy will be created by using xyFromCell() of terra package.\n\nxy &lt;- terra::xyFromCell(grid, \n                        1:ncell(grid))\nhead(xy)\n\n            x        y\n[1,] 2692.528 50231.33\n[2,] 2742.509 50231.33\n[3,] 2792.489 50231.33\n[4,] 2842.469 50231.33\n[5,] 2892.450 50231.33\n[6,] 2942.430 50231.33\n\n\n\n\n\n\n\n\nTip\n\n\n\nTip\nxyFromCell()gets coordinates of the center of raster cells for a row, column, or cell number of a SpatRaster. Or get row, column, or cell numbers from coordinates or from each other.\n\n\n\ncoop &lt;- st_as_sf(as.data.frame(xy), \n                 coords = c(\"x\", \"y\"),\n                 crs = st_crs(mpsz2019))\ncoop &lt;- st_filter(coop, mpsz2019)\nhead(coop)\n\nSimple feature collection with 6 features and 0 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 25883.42 ymin: 50231.33 xmax: 26133.32 ymax: 50231.33\nProjected CRS: SVY21 / Singapore TM\n                   geometry\n1 POINT (25883.42 50231.33)\n2  POINT (25933.4 50231.33)\n3 POINT (25983.38 50231.33)\n4 POINT (26033.36 50231.33)\n5 POINT (26083.34 50231.33)\n6 POINT (26133.32 50231.33)"
  },
  {
    "objectID": "In-class_Ex/in_class7/inclasss7.html#inverse-distance-weighted-idw",
    "href": "In-class_Ex/in_class7/inclasss7.html#inverse-distance-weighted-idw",
    "title": "In-class 7 Visual Analytics",
    "section": "Inverse Distance Weighted (IDW)",
    "text": "Inverse Distance Weighted (IDW)\n\nThe method\nIn the IDW interpolation method, the sample points are weighted during interpolation such that the influence of one point relative to another declines with distance from the unknown point you want to create.\n\nWeighting is assigned to sample points through the use of a weighting coefficient that controls how the weighting influence will drop off as the distance from new point increases. The greater the weighting coefficient, the less the effect points will have if they are far from the unknown point during the interpolation process. As the coefficient increases, the value of the unknown point approaches the value of the nearest observational point.\nIt is important to notice that the IDW interpolation method also has some disadvantages: the quality of the interpolation result can decrease, if the distribution of sample data points is uneven. Furthermore, maximum and minimum values in the interpolated surface can only occur at sample data points. This often results in small peaks and pits around the sample data points.\n\n\nWorking with gstat\nWe are going to use three parameters of the gstat function:\n\nformula: The prediction “formula” specifying the dependent and the independent variables (covariates)\ndata: The calibration data\nmodel: The variogram model\n\nKeep in mind that we need to specify parameter names, because these three parameters are not the first three in the gstat function definition.\nFor example, to interpolate using the IDW method we create the following gstat object, specifying just the formula and data:\ng = gstat(formula = annual ~ 1, data = rainfall)\n\n\n\n\n\n\nTip\n\n\n\nTip\nIn R, formula objects are used to specify relation between objects, in particular—the role of different data columns in statistical models. A formula object is created using the ~ operator, which separates names of dependent variables (to the left of the ~ symbol) and independent variables (to the right of the ~ symbol). Writing 1 to the right of the ~ symbol, as in ~ 1, means that there are no independent variables38.\n\n\n\nres &lt;- gstat(formula = MONTHSUM ~ 1, \n             locations = rfdata_sf, \n             nmax = 5,\n             set = list(idp = 0))\n\n\n\n\n\n\n\nImportant\n\n\n\nImportant\nSpatial interpolation is not a rocket science, students should try to explore the method by changing nmax argument in order to understand how the final surface map will be affected by different nmax values.\n\n\nNow that our model is defined, we can use predict() to actually interpolate, i.e., to calculate predicted values. The predict function accepts:\n\nA raster—stars object, such as dem\nA model—gstat object, such as g\n\nThe raster serves for two purposes:\n\nSpecifying the locations where we want to make predictions (in all methods), and\nSpecifying covariate values (in Universal Kriging only).\n\n\nresp &lt;- predict(res, coop)\n\n[inverse distance weighted interpolation]\n\n\n\nresp$x &lt;- st_coordinates(resp)[,1]\nresp$y &lt;- st_coordinates(resp)[,2]\nresp$pred &lt;- resp$var1.pred\n\npred &lt;- terra::rasterize(resp, grid, \n                         field = \"pred\", \n                         fun = \"mean\")\n\nNow, we will map the interpolated surface by using tmap functions as shown in the code chunk below.\n\ntmap_options(check.and.fix = TRUE)\ntmap_mode(\"plot\")\ntm_shape(pred) + \n  tm_raster(alpha = 0.6, \n            palette = \"viridis\")\n\n\n\n\n\n\n\n\n\n\nWorking with gstat\nFirstly, we will calculate and examine the empirical variogram by using variogram() of gstat package. The function requires two arguments:\n\nformula, the dependent variable and the covariates (same as in gstat, see Section 12.2.1)\ndata, a point layer with the dependent variable and covariates as attributes\n\nas shown in the code chunk below.\n\nv &lt;- variogram(MONTHSUM ~ 1, \n               data = rfdata_sf)\nplot(v)\n\n\n\n\n\n\n\n\nWith reference to the comparison above, am empirical variogram model will be fitted by using fit.variogram() of gstat package as shown in the code chunk below.\n\nfv &lt;- fit.variogram(object = v,\n                    model = vgm(\n                      psill = 0.5, \n                      model = \"Sph\",\n                      range = 5000, \n                      nugget = 0.1))\nfv\n\n  model     psill    range\n1   Nug 0.1129190    0.000\n2   Sph 0.5292397 5213.396\n\n\nThe plot above reveals that the empirical model fits rather well. In view of this, we will go ahead to perform spatial interpolation by using the newly derived model as shown in the code chunk below.\n\nk &lt;- gstat(formula = MONTHSUM ~ 1, \n           data = rfdata_sf, \n           model = fv)\nk\n\ndata:\nvar1 : formula = MONTHSUM`~`1 ; data dim = 43 x 2\nvariograms:\n        model     psill    range\nvar1[1]   Nug 0.1129190    0.000\nvar1[2]   Sph 0.5292397 5213.396\n\n\n\nresp &lt;- predict(k, coop)\n\n[using ordinary kriging]\n\n\n\nresp$x &lt;- st_coordinates(resp)[,1]\nresp$y &lt;- st_coordinates(resp)[,2]\nresp$pred &lt;- resp$var1.pred\nresp$pred &lt;- resp$pred\nresp\n\nSimple feature collection with 314019 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 2692.528 ymin: 15773.73 xmax: 56371.45 ymax: 50231.33\nProjected CRS: SVY21 / Singapore TM\nFirst 10 features:\n   var1.pred  var1.var                  geometry        x        y     pred\n1   131.0667 0.6608399 POINT (25883.42 50231.33) 25883.42 50231.33 131.0667\n2   130.9986 0.6610337  POINT (25933.4 50231.33) 25933.40 50231.33 130.9986\n3   130.9330 0.6612129 POINT (25983.38 50231.33) 25983.38 50231.33 130.9330\n4   130.8698 0.6613782 POINT (26033.36 50231.33) 26033.36 50231.33 130.8698\n5   130.8092 0.6615303 POINT (26083.34 50231.33) 26083.34 50231.33 130.8092\n6   130.7514 0.6616697 POINT (26133.32 50231.33) 26133.32 50231.33 130.7514\n7   130.6965 0.6617971  POINT (26183.3 50231.33) 26183.30 50231.33 130.6965\n8   130.6446 0.6619131 POINT (26233.28 50231.33) 26233.28 50231.33 130.6446\n9   130.5958 0.6620184 POINT (26283.26 50231.33) 26283.26 50231.33 130.5958\n10  132.5484 0.6542154 POINT (25033.76 50181.32) 25033.76 50181.32 132.5484\n\n\nIn order to create a raster surface data object, rasterize() of terra is used as shown in the code chunk below.\n\nkpred &lt;- terra::rasterize(resp, grid, \n                         field = \"pred\")\nkpred\n\nclass       : SpatRaster \ndimensions  : 690, 1075, 1  (nrow, ncol, nlyr)\nresolution  : 49.98037, 50.01103  (x, y)\nextent      : 2667.538, 56396.44, 15748.72, 50256.33  (xmin, xmax, ymin, ymax)\ncoord. ref. : SVY21 / Singapore TM (EPSG:3414) \nsource(s)   : memory\nname        :      last \nmin value   :  72.77826 \nmax value   : 195.53284 \n\n\n\n\nMapping the interpolated rainfall raster\nFinally, tmap functions are used to map the interpolated rainfall raster (i.e. kpred) by using the code chunk below.\n\ntmap_options(check.and.fix = TRUE)\ntmap_mode(\"plot\")\ntm_shape(kpred) + \n  tm_raster(alpha = 0.6, \n            palette = \"viridis\",\n            title = \"Total monthly rainfall (mm)\") +\n  tm_layout(main.title = \"Distribution of monthly rainfall, Feb 2024\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2)\n\n\n\n\n\n\n\n\n\n\nAutomatic variogram modelling\nBeside using gstat to perform variogram modelling manually, autofirVariogram() of automap package can be used to perform varigram modelling as shown in the code chunk below.\n\nv_auto &lt;- autofitVariogram(MONTHSUM ~ 1, \n                           rfdata_sf)\nplot(v_auto)\n\n\n\n\n\n\n\n\n\nk &lt;- gstat(formula = MONTHSUM ~ 1, \n           model = v_auto$var_model,\n           data = rfdata_sf)\nk\n\ndata:\nvar1 : formula = MONTHSUM`~`1 ; data dim = 43 x 2\nvariograms:\n        model    psill   range kappa\nvar1[1]   Nug     0.00       0   0.0\nvar1[2]   Ste 24100.71 1647955   0.3\n\n\n\nresp &lt;- predict(k, coop)\n\n[using ordinary kriging]\n\n\n\nresp$x &lt;- st_coordinates(resp)[,1]\nresp$y &lt;- st_coordinates(resp)[,2]\nresp$pred &lt;- resp$var1.pred\nresp$pred &lt;- resp$pred\n\nkpred &lt;- terra::rasterize(resp, grid, \n                         field = \"pred\")\n\n\ntmap_options(check.and.fix = TRUE)\ntmap_mode(\"plot\")\ntm_shape(kpred) + \n  tm_raster(alpha = 0.6, \n            palette = \"viridis\",\n            title = \"Total monthly rainfall (mm)\") +\n  tm_layout(main.title = \"Distribution of monthly rainfall, Feb 2024\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_ex04/Take-home_ex4.html",
    "href": "Take-home_Ex/Take-home_ex04/Take-home_ex4.html",
    "title": "Take-home exercise 4 Visual Analytics",
    "section": "",
    "text": "For this Take Home Exercise, there are several task needs to be done to create the Shiny application.\n\nTo evaluate and determine the necessary R packages needed for your Shiny application are supported in R CRAN,\nTo prepare and test the specific R codes can be run and returned the correct output as expected,\nTo determine the parameters and outputs that will be exposed on the Shiny applications, and\nTo select the appropriate Shiny UI components for exposing the parameters determine above.\n\nThis submission includes the prototype report for the group project, which will includes:\n\nthe data preparation process,\nthe selection of data visualization techniques used,\nand the data visualization design and interactivity principles and best practices implemented.\n\nBased on the discussion with team member i will be focusing on the Exploratory Data Analysis & Confirmatory Data Analysis, and the UI design for our Shiny app."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands_on_ex7.html",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands_on_ex7.html",
    "title": "Hand-on exercise 7 Visual Analytics",
    "section": "",
    "text": "Choropleth mapping involves the symbolisation of enumeration units, such as countries, provinces, states, counties or census units, using area patterns or graduated colors. For example, a social scientist may need to use a choropleth map to portray the spatial distribution of aged population of Singapore by Master Plan 2014 Subzone Boundary.\nIn this chapter, you will learn how to plot functional and truthful choropleth maps by using an R package called tmap package.\n\n\n\n\n\n\nTip\n\n\n\nTip\nIt is advisable for you to read the functional description of each function before using them.\n\n\n\n\n\nIn this hands-on exercise, the key R package use is tmap package in R. Beside tmap package, four other R packages will be used. They are:\n\nreadr for importing delimited text file,\ntidyr for tidying data,\ndplyr for wrangling data and\nsf for handling geospatial data.\n\nAmong the four packages, readr, tidyr and dplyr are part of tidyverse package.\nThe code chunk below will be used to install and load these packages in RStudio.\n\npacman::p_load(sf, tmap, tidyverse)\n\n\n\n\n\n\nTwo data set will be used to create the choropleth map. They are:\n\nMaster Plan 2014 Subzone Boundary (Web) (i.e. MP14_SUBZONE_WEB_PL) in ESRI shapefile format. It can be downloaded at data.gov.sg This is a geospatial data. It consists of the geographical boundary of Singapore at the planning subzone level. The data is based on URA Master Plan 2014.\nSingapore Residents by Planning Area / Subzone, Age Group, Sex and Type of Dwelling, June 2011-2020 in csv format (i.e. respopagesextod2011to2020.csv). This is an aspatial data fie. It can be downloaded at Department of Statistics, Singapore Although it does not contain any coordinates values, but it’s PA and SZ fields can be used as unique identifiers to geocode to MP14_SUBZONE_WEB_PL shapefile.\n\n\n\n\nThe code chunk below uses the st_read() function of sf package to import MP14_SUBZONE_WEB_PL shapefile into R as a simple feature data frame called mpsz.\n\nmpsz &lt;- st_read(dsn = \"data/geospatial\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\lzc0313\\ISSS608-VAA\\Hands-on_Ex\\Hands-on_Ex07\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nYou can examine the content of mpsz by using the code chunk below.\n\nmpsz\n\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 10 features:\n   OBJECTID SUBZONE_NO       SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1         1          1    MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2         2          1    PEARL'S HILL    OTSZ01      Y          OUTRAM\n3         3          3       BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4         4          8  HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5         5          3         REDHILL    BMSZ03      N     BUKIT MERAH\n6         6          7  ALEXANDRA HILL    BMSZ07      N     BUKIT MERAH\n7         7          9   BUKIT HO SWEE    BMSZ09      N     BUKIT MERAH\n8         8          2     CLARKE QUAY    SRSZ02      Y SINGAPORE RIVER\n9         9         13 PASIR PANJANG 1    QTSZ13      N      QUEENSTOWN\n10       10          7       QUEENSWAY    QTSZ07      N      QUEENSTOWN\n   PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1          MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2          OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3          SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4          BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5          BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n6          BM CENTRAL REGION       CR 9D286521EF5E3B59 2014-12-05 25358.82\n7          BM CENTRAL REGION       CR 7839A8577144EFE2 2014-12-05 27680.06\n8          SR CENTRAL REGION       CR 48661DC0FBA09F7A 2014-12-05 29253.21\n9          QT CENTRAL REGION       CR 1F721290C421BFAB 2014-12-05 22077.34\n10         QT CENTRAL REGION       CR 3580D2AFFBEE914C 2014-12-05 24168.31\n     Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1  29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2  29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3  29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4  29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5  30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n6  29991.38   4428.913  1030378.8 MULTIPOLYGON (((25899.7 297...\n7  30230.86   3275.312   551732.0 MULTIPOLYGON (((27746.95 30...\n8  30222.86   2208.619   290184.7 MULTIPOLYGON (((29351.26 29...\n9  29893.78   6571.323  1084792.3 MULTIPOLYGON (((20996.49 30...\n10 30104.18   3454.239   631644.3 MULTIPOLYGON (((24472.11 29...\n\n\nNotice that only the first ten records will be displayed. Do you know why?\n\n\n\nNext, we will import respopagsex2011to2020.csv file into RStudio and save the file into an R dataframe called popagsex.\nThe task will be performed by using read_csv() function of readr package as shown in the code chunk below.\n\npopdata &lt;- read_csv(\"data/aspatial/respopagesextod2011to2020.csv\")\n\n\n\n\nBefore a thematic map can be prepared, you are required to prepare a data table with year 2020 values. The data table should include the variables PA, SZ, YOUNG, ECONOMY ACTIVE, AGED, TOTAL, DEPENDENCY.\n\nYOUNG: age group 0 to 4 until age groyup 20 to 24,\nECONOMY ACTIVE: age group 25-29 until age group 60-64,\nAGED: age group 65 and above,\nTOTAL: all age group, and\nDEPENDENCY: the ratio between young and aged against economy active group\n\n\n\nThe following data wrangling and transformation functions will be used:\n\npivot_wider() of tidyr package, and\nmutate(), filter(), group_by() and select() of dplyr package\n\n\npopdata2020 &lt;- popdata %&gt;%\n  filter(Time == 2020) %&gt;%\n  group_by(PA, SZ, AG) %&gt;%\n  summarise(`POP` = sum(`Pop`)) %&gt;%\n  ungroup() %&gt;%\n  pivot_wider(names_from=AG, \n              values_from=POP) %&gt;%\n  mutate(YOUNG = rowSums(.[3:6])\n         +rowSums(.[12])) %&gt;%\nmutate(`ECONOMY ACTIVE` = rowSums(.[7:11])+\nrowSums(.[13:15]))%&gt;%\nmutate(`AGED`=rowSums(.[16:21])) %&gt;%\nmutate(`TOTAL`=rowSums(.[3:21])) %&gt;%  \nmutate(`DEPENDENCY` = (`YOUNG` + `AGED`)\n/`ECONOMY ACTIVE`) %&gt;%\n  select(`PA`, `SZ`, `YOUNG`, \n       `ECONOMY ACTIVE`, `AGED`, \n       `TOTAL`, `DEPENDENCY`)\n\n\n\n\nBefore we can perform the georelational join, one extra step is required to convert the values in PA and SZ fields to uppercase. This is because the values of PA and SZ fields are made up of upper- and lowercase. On the other, hand the SUBZONE_N and PLN_AREA_N are in uppercase.\n\npopdata2020 &lt;- popdata2020 %&gt;%\n  mutate_at(.vars = vars(PA, SZ), \n          .funs = funs(toupper)) %&gt;%\n  filter(`ECONOMY ACTIVE` &gt; 0)\n\nNext, left_join() of dplyr is used to join the geographical data and attribute table using planning subzone name e.g. SUBZONE_N and SZ as the common identifier.\n\nmpsz_pop2020 &lt;- left_join(mpsz, popdata2020,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))\n\nThing to learn from the code chunk above:\n\nleft_join() of dplyr package is used with mpsz simple feature data frame as the left data table is to ensure that the output will be a simple features data frame.\n\nwrite_rds(mpsz_pop2020, \"data/rds/mpszpop2020.rds\")\n\n\n\n\n\n\n\nTwo approaches can be used to prepare thematic map using tmap, they are:\n\nPlotting a thematic map quickly by using qtm().\nPlotting highly customisable thematic map by using tmap elements.\n\n\n\nThe easiest and quickest to draw a choropleth map using tmap is using qtm(). It is concise and provides a good default visualisation in many cases.\nThe code chunk below will draw a cartographic standard choropleth map as shown below.\n\ntmap_mode(\"plot\")\nqtm(mpsz_pop2020, \n    fill = \"DEPENDENCY\")\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above:\n\ntmap_mode() with “plot” option is used to produce a static map. For interactive mode, “view” option should be used.\nfill argument is used to map the attribute (i.e. DEPENDENCY)\n\n\n\n\nDespite its usefulness of drawing a choropleth map quickly and easily, the disadvantge of qtm() is that it makes aesthetics of individual layers harder to control. To draw a high quality cartographic choropleth map as shown in the figure below, tmap’s drawing elements should be used.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Dependency ratio\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\n\nIn the following sub-section, we will share with you tmap functions that used to plot these elements.\n\n\nThe basic building block of tmap is tm_shape() followed by one or more layer elemments such as tm_fill() and tm_polygons().\nIn the code chunk below, tm_shape() is used to define the input data (i.e mpsz_pop2020) and tm_polygons() is used to draw the planning subzone polygons\n\ntm_shape(mpsz_pop2020) +\n  tm_polygons()\n\n\n\n\n\n\n\n\n\n\n\nTo draw a choropleth map showing the geographical distribution of a selected variable by planning subzone, we just need to assign the target variable such as Dependency to tm_polygons().\n\ntm_shape(mpsz_pop2020)+\n  tm_polygons(\"DEPENDENCY\")\n\n\n\n\n\n\n\n\nThings to learn from tm_polygons():\n\nThe default interval binning used to draw the choropleth map is called “pretty”. A detailed discussion of the data classification methods supported by tmap will be provided in sub-section 4.3.\nThe default colour scheme used is YlOrRd of ColorBrewer. You will learn more about the color scheme in sub-section 4.4.\nBy default, Missing value will be shaded in grey.\n\n\n\n\nActually, tm_polygons() is a wraper of tm_fill() and tm_border(). tm_fill() shades the polygons by using the default colour scheme and tm_borders() adds the borders of the shapefile onto the choropleth map.\nThe code chunk below draws a choropleth map by using tm_fill() alone.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\")\n\n\n\n\n\n\n\n\nNotice that the planning subzones are shared according to the respective dependecy values\nTo add the boundary of the planning subzones, tm_borders will be used as shown in the code chunk below.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\") +\n  tm_borders(lwd = 0.1,  alpha = 1)\n\n\n\n\n\n\n\n\nNotice that light-gray border lines have been added on the choropleth map.\nThe alpha argument is used to define transparency number between 0 (totally transparent) and 1 (not transparent). By default, the alpha value of the col is used (normally 1).\nBeside alpha argument, there are three other arguments for tm_borders(), they are:\n\ncol = border colour,\nlwd = border line width. The default is 1, and\nlty = border line type. The default is “solid”.\n\n\n\n\n\nMost choropleth maps employ some methods of data classification. The point of classification is to take a large number of observations and group them into data ranges or classes.\ntmap provides a total ten data classification methods, namely: fixed, sd, equal, pretty (default), quantile, kmeans, hclust, bclust, fisher, and jenks.\nTo define a data classification method, the style argument of tm_fill() or tm_polygons() will be used.\n\n\nThe code chunk below shows a quantile data classification that used 5 classes.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\nIn the code chunk below, equal data classification method is used.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\nNotice that the distribution of quantile data classification method are more evenly distributed then equal data classification method.\n\nWarning: Maps Lie!\n\n\nDIY: Using what you had learned, prepare choropleth maps by using different classification methods supported by tmap and compare their differences.\n\n\nDIY: Preparing choropleth maps by using similar classification method but with different numbers of classes (i.e. 2, 6, 10, 20). Compare the output maps, what observation can you draw?\n\n\n\n\nFor all the built-in styles, the category breaks are computed internally. In order to override these defaults, the breakpoints can be set explicitly by means of the breaks argument to the tm_fill(). It is important to note that, in tmap the breaks include a minimum and maximum. As a result, in order to end up with n categories, n+1 elements must be specified in the breaks option (the values must be in increasing order).\nBefore we get started, it is always a good practice to get some descriptive statistics on the variable before setting the break points. Code chunk below will be used to compute and display the descriptive statistics of DEPENDENCY field.\n\nsummary(mpsz_pop2020$DEPENDENCY)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.1111  0.7147  0.7866  0.8585  0.8763 19.0000      92 \n\n\nWith reference to the results above, we set break point at 0.60, 0.70, 0.80, and 0.90. In addition, we also need to include a minimum and maximum, which we set at 0 and 100. Our breaks vector is thus c(0, 0.60, 0.70, 0.80, 0.90, 1.00)\nNow, we will plot the choropleth map by using the code chunk below.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          breaks = c(0, 0.60, 0.70, 0.80, 0.90, 1.00)) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n\ntmap supports colour ramps either defined by the user or a set of predefined colour ramps from the RColorBrewer package.\n\n\nTo change the colour, we assign the preferred colour to palette argument of tm_fill() as shown in the code chunk below.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"Blues\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\nNotice that the choropleth map is shaded in green.\nTo reverse the colour shading, add a “-” prefix.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\nNotice that the colour scheme has been reversed.\n\n\n\n\nMap layout refers to the combination of all map elements into a cohensive map. Map elements include among others the objects to be mapped, the title, the scale bar, the compass, margins and aspects ratios. Colour settings and data classification methods covered in the previous section relate to the palette and break-points are used to affect how the map looks.\n\n\nIn tmap, several legend options are provided to change the placement, format and appearance of the legend.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"jenks\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone \\n(Jenks classification)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            legend.outside = FALSE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\ntmap allows a wide variety of layout settings to be changed. They can be called by using tmap_style().\nThe code chunk below shows the classic style is used.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"classic\")\n\n\n\n\n\n\n\n\n\n\n\nBeside map style, tmap also also provides arguments to draw other map furniture such as compass, scale bar and grid lines.\nIn the code chunk below, tm_compass(), tm_scale_bar() and tm_grid() are used to add compass, scale bar and grid lines onto the choropleth map.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"No. of persons\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio \\nby planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\n\nTo reset the default style, refer to the code chunk below.\n\ntmap_style(\"white\")\n\n\n\n\n\nSmall multiple maps, also referred to as facet maps, are composed of many maps arrange side-by-side, and sometimes stacked vertically. Small multiple maps enable the visualisation of how spatial relationships change with respect to another variable, such as time.\nIn tmap, small multiple maps can be plotted in three ways:\n\nby assigning multiple values to at least one of the asthetic arguments,\nby defining a group-by variable in tm_facets(), and\nby creating multiple stand-alone maps with tmap_arrange().\n\n\n\nIn this example, small multiple choropleth maps are created by defining ncols in tm_fill()\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(c(\"YOUNG\", \"AGED\"),\n          style = \"equal\", \n          palette = \"Blues\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\")) +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"white\")\n\n\n\n\n\n\n\n\nIn this example, small multiple choropleth maps are created by assigning multiple values to at least one of the aesthetic arguments\n\ntm_shape(mpsz_pop2020)+ \n  tm_polygons(c(\"DEPENDENCY\",\"AGED\"),\n          style = c(\"equal\", \"quantile\"), \n          palette = list(\"Blues\",\"Greens\")) +\n  tm_layout(legend.position = c(\"right\", \"bottom\"))\n\n\n\n\n\n\n\n\n\n\n\nIn this example, multiple small choropleth maps are created by using tm_facets().\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"Blues\",\n          thres.poly = 0) + \n  tm_facets(by=\"REGION_N\", \n            free.coords=TRUE, \n            drop.shapes=FALSE) +\n  tm_layout(legend.show = FALSE,\n            title.position = c(\"center\", \"center\"), \n            title.size = 20) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\nIn this example, multiple small choropleth maps are created by creating multiple stand-alone maps with tmap_arrange().\n\nyoungmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"YOUNG\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\nagedmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"AGED\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\ntmap_arrange(youngmap, agedmap, asp=1, ncol=2)\n\n\n\n\n\n\n\n\n\n\n\n\nInstead of creating small multiple choropleth map, you can also use selection funtion to map spatial objects meeting the selection criterion.\n\ntm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ])+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(legend.outside = TRUE,\n            legend.height = 0.45, \n            legend.width = 5.0,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntmap: Thematic Maps in R\ntmap\ntmap: get started!\ntmap: changes in version 2.0\ntmap: creating thematic maps in a flexible way (useR!2015)\nExploring and presenting maps with tmap (useR!2017)\n\n\n\n\n\nsf: Simple Features for R\nSimple Features for R: StandardizedSupport for Spatial Vector Data\nReading, Writing and Converting Simple Features\n\n\n\n\n\ndplyr\nTidy data\ntidyr: Easily Tidy Data with ‘spread()’ and ‘gather()’ Functions"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands_on_ex7.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands_on_ex7.html#overview",
    "title": "Hand-on exercise 7 Visual Analytics",
    "section": "",
    "text": "Choropleth mapping involves the symbolisation of enumeration units, such as countries, provinces, states, counties or census units, using area patterns or graduated colors. For example, a social scientist may need to use a choropleth map to portray the spatial distribution of aged population of Singapore by Master Plan 2014 Subzone Boundary.\nIn this chapter, you will learn how to plot functional and truthful choropleth maps by using an R package called tmap package.\n\n\n\n\n\n\nTip\n\n\n\nTip\nIt is advisable for you to read the functional description of each function before using them."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands_on_ex7.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands_on_ex7.html#getting-started",
    "title": "Hand-on exercise 7 Visual Analytics",
    "section": "",
    "text": "In this hands-on exercise, the key R package use is tmap package in R. Beside tmap package, four other R packages will be used. They are:\n\nreadr for importing delimited text file,\ntidyr for tidying data,\ndplyr for wrangling data and\nsf for handling geospatial data.\n\nAmong the four packages, readr, tidyr and dplyr are part of tidyverse package.\nThe code chunk below will be used to install and load these packages in RStudio.\n\npacman::p_load(sf, tmap, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands_on_ex7.html#importing-data-into-r",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands_on_ex7.html#importing-data-into-r",
    "title": "Hand-on exercise 7 Visual Analytics",
    "section": "",
    "text": "Two data set will be used to create the choropleth map. They are:\n\nMaster Plan 2014 Subzone Boundary (Web) (i.e. MP14_SUBZONE_WEB_PL) in ESRI shapefile format. It can be downloaded at data.gov.sg This is a geospatial data. It consists of the geographical boundary of Singapore at the planning subzone level. The data is based on URA Master Plan 2014.\nSingapore Residents by Planning Area / Subzone, Age Group, Sex and Type of Dwelling, June 2011-2020 in csv format (i.e. respopagesextod2011to2020.csv). This is an aspatial data fie. It can be downloaded at Department of Statistics, Singapore Although it does not contain any coordinates values, but it’s PA and SZ fields can be used as unique identifiers to geocode to MP14_SUBZONE_WEB_PL shapefile.\n\n\n\n\nThe code chunk below uses the st_read() function of sf package to import MP14_SUBZONE_WEB_PL shapefile into R as a simple feature data frame called mpsz.\n\nmpsz &lt;- st_read(dsn = \"data/geospatial\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\lzc0313\\ISSS608-VAA\\Hands-on_Ex\\Hands-on_Ex07\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nYou can examine the content of mpsz by using the code chunk below.\n\nmpsz\n\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 10 features:\n   OBJECTID SUBZONE_NO       SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1         1          1    MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2         2          1    PEARL'S HILL    OTSZ01      Y          OUTRAM\n3         3          3       BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4         4          8  HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5         5          3         REDHILL    BMSZ03      N     BUKIT MERAH\n6         6          7  ALEXANDRA HILL    BMSZ07      N     BUKIT MERAH\n7         7          9   BUKIT HO SWEE    BMSZ09      N     BUKIT MERAH\n8         8          2     CLARKE QUAY    SRSZ02      Y SINGAPORE RIVER\n9         9         13 PASIR PANJANG 1    QTSZ13      N      QUEENSTOWN\n10       10          7       QUEENSWAY    QTSZ07      N      QUEENSTOWN\n   PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1          MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2          OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3          SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4          BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5          BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n6          BM CENTRAL REGION       CR 9D286521EF5E3B59 2014-12-05 25358.82\n7          BM CENTRAL REGION       CR 7839A8577144EFE2 2014-12-05 27680.06\n8          SR CENTRAL REGION       CR 48661DC0FBA09F7A 2014-12-05 29253.21\n9          QT CENTRAL REGION       CR 1F721290C421BFAB 2014-12-05 22077.34\n10         QT CENTRAL REGION       CR 3580D2AFFBEE914C 2014-12-05 24168.31\n     Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1  29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2  29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3  29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4  29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5  30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n6  29991.38   4428.913  1030378.8 MULTIPOLYGON (((25899.7 297...\n7  30230.86   3275.312   551732.0 MULTIPOLYGON (((27746.95 30...\n8  30222.86   2208.619   290184.7 MULTIPOLYGON (((29351.26 29...\n9  29893.78   6571.323  1084792.3 MULTIPOLYGON (((20996.49 30...\n10 30104.18   3454.239   631644.3 MULTIPOLYGON (((24472.11 29...\n\n\nNotice that only the first ten records will be displayed. Do you know why?\n\n\n\nNext, we will import respopagsex2011to2020.csv file into RStudio and save the file into an R dataframe called popagsex.\nThe task will be performed by using read_csv() function of readr package as shown in the code chunk below.\n\npopdata &lt;- read_csv(\"data/aspatial/respopagesextod2011to2020.csv\")\n\n\n\n\nBefore a thematic map can be prepared, you are required to prepare a data table with year 2020 values. The data table should include the variables PA, SZ, YOUNG, ECONOMY ACTIVE, AGED, TOTAL, DEPENDENCY.\n\nYOUNG: age group 0 to 4 until age groyup 20 to 24,\nECONOMY ACTIVE: age group 25-29 until age group 60-64,\nAGED: age group 65 and above,\nTOTAL: all age group, and\nDEPENDENCY: the ratio between young and aged against economy active group\n\n\n\nThe following data wrangling and transformation functions will be used:\n\npivot_wider() of tidyr package, and\nmutate(), filter(), group_by() and select() of dplyr package\n\n\npopdata2020 &lt;- popdata %&gt;%\n  filter(Time == 2020) %&gt;%\n  group_by(PA, SZ, AG) %&gt;%\n  summarise(`POP` = sum(`Pop`)) %&gt;%\n  ungroup() %&gt;%\n  pivot_wider(names_from=AG, \n              values_from=POP) %&gt;%\n  mutate(YOUNG = rowSums(.[3:6])\n         +rowSums(.[12])) %&gt;%\nmutate(`ECONOMY ACTIVE` = rowSums(.[7:11])+\nrowSums(.[13:15]))%&gt;%\nmutate(`AGED`=rowSums(.[16:21])) %&gt;%\nmutate(`TOTAL`=rowSums(.[3:21])) %&gt;%  \nmutate(`DEPENDENCY` = (`YOUNG` + `AGED`)\n/`ECONOMY ACTIVE`) %&gt;%\n  select(`PA`, `SZ`, `YOUNG`, \n       `ECONOMY ACTIVE`, `AGED`, \n       `TOTAL`, `DEPENDENCY`)\n\n\n\n\nBefore we can perform the georelational join, one extra step is required to convert the values in PA and SZ fields to uppercase. This is because the values of PA and SZ fields are made up of upper- and lowercase. On the other, hand the SUBZONE_N and PLN_AREA_N are in uppercase.\n\npopdata2020 &lt;- popdata2020 %&gt;%\n  mutate_at(.vars = vars(PA, SZ), \n          .funs = funs(toupper)) %&gt;%\n  filter(`ECONOMY ACTIVE` &gt; 0)\n\nNext, left_join() of dplyr is used to join the geographical data and attribute table using planning subzone name e.g. SUBZONE_N and SZ as the common identifier.\n\nmpsz_pop2020 &lt;- left_join(mpsz, popdata2020,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))\n\nThing to learn from the code chunk above:\n\nleft_join() of dplyr package is used with mpsz simple feature data frame as the left data table is to ensure that the output will be a simple features data frame.\n\nwrite_rds(mpsz_pop2020, \"data/rds/mpszpop2020.rds\")"
  },
  {
    "objectID": "In-class_Ex/in_class7/data/geospatial/MPSZ-2019.html",
    "href": "In-class_Ex/in_class7/data/geospatial/MPSZ-2019.html",
    "title": "ISSS608-Zhongchao",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ISSS608-Zhongchao’s Visual Analytics",
    "section": "",
    "text": "Welcome to ISSS608 Visual Analytics and Applications.\nIn this website, you will find my coursework prepared for this course……….."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands_on_ex7.html#choropleth-mapping-geospatial-data-using-tmap",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands_on_ex7.html#choropleth-mapping-geospatial-data-using-tmap",
    "title": "Hand-on exercise 7 Visual Analytics",
    "section": "",
    "text": "Two approaches can be used to prepare thematic map using tmap, they are:\n\nPlotting a thematic map quickly by using qtm().\nPlotting highly customisable thematic map by using tmap elements.\n\n\n\nThe easiest and quickest to draw a choropleth map using tmap is using qtm(). It is concise and provides a good default visualisation in many cases.\nThe code chunk below will draw a cartographic standard choropleth map as shown below.\n\ntmap_mode(\"plot\")\nqtm(mpsz_pop2020, \n    fill = \"DEPENDENCY\")\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above:\n\ntmap_mode() with “plot” option is used to produce a static map. For interactive mode, “view” option should be used.\nfill argument is used to map the attribute (i.e. DEPENDENCY)\n\n\n\n\nDespite its usefulness of drawing a choropleth map quickly and easily, the disadvantge of qtm() is that it makes aesthetics of individual layers harder to control. To draw a high quality cartographic choropleth map as shown in the figure below, tmap’s drawing elements should be used.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Dependency ratio\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\n\nIn the following sub-section, we will share with you tmap functions that used to plot these elements.\n\n\nThe basic building block of tmap is tm_shape() followed by one or more layer elemments such as tm_fill() and tm_polygons().\nIn the code chunk below, tm_shape() is used to define the input data (i.e mpsz_pop2020) and tm_polygons() is used to draw the planning subzone polygons\n\ntm_shape(mpsz_pop2020) +\n  tm_polygons()\n\n\n\n\n\n\n\n\n\n\n\nTo draw a choropleth map showing the geographical distribution of a selected variable by planning subzone, we just need to assign the target variable such as Dependency to tm_polygons().\n\ntm_shape(mpsz_pop2020)+\n  tm_polygons(\"DEPENDENCY\")\n\n\n\n\n\n\n\n\nThings to learn from tm_polygons():\n\nThe default interval binning used to draw the choropleth map is called “pretty”. A detailed discussion of the data classification methods supported by tmap will be provided in sub-section 4.3.\nThe default colour scheme used is YlOrRd of ColorBrewer. You will learn more about the color scheme in sub-section 4.4.\nBy default, Missing value will be shaded in grey.\n\n\n\n\nActually, tm_polygons() is a wraper of tm_fill() and tm_border(). tm_fill() shades the polygons by using the default colour scheme and tm_borders() adds the borders of the shapefile onto the choropleth map.\nThe code chunk below draws a choropleth map by using tm_fill() alone.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\")\n\n\n\n\n\n\n\n\nNotice that the planning subzones are shared according to the respective dependecy values\nTo add the boundary of the planning subzones, tm_borders will be used as shown in the code chunk below.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\") +\n  tm_borders(lwd = 0.1,  alpha = 1)\n\n\n\n\n\n\n\n\nNotice that light-gray border lines have been added on the choropleth map.\nThe alpha argument is used to define transparency number between 0 (totally transparent) and 1 (not transparent). By default, the alpha value of the col is used (normally 1).\nBeside alpha argument, there are three other arguments for tm_borders(), they are:\n\ncol = border colour,\nlwd = border line width. The default is 1, and\nlty = border line type. The default is “solid”.\n\n\n\n\n\nMost choropleth maps employ some methods of data classification. The point of classification is to take a large number of observations and group them into data ranges or classes.\ntmap provides a total ten data classification methods, namely: fixed, sd, equal, pretty (default), quantile, kmeans, hclust, bclust, fisher, and jenks.\nTo define a data classification method, the style argument of tm_fill() or tm_polygons() will be used.\n\n\nThe code chunk below shows a quantile data classification that used 5 classes.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\nIn the code chunk below, equal data classification method is used.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\nNotice that the distribution of quantile data classification method are more evenly distributed then equal data classification method.\n\nWarning: Maps Lie!\n\n\nDIY: Using what you had learned, prepare choropleth maps by using different classification methods supported by tmap and compare their differences.\n\n\nDIY: Preparing choropleth maps by using similar classification method but with different numbers of classes (i.e. 2, 6, 10, 20). Compare the output maps, what observation can you draw?\n\n\n\n\nFor all the built-in styles, the category breaks are computed internally. In order to override these defaults, the breakpoints can be set explicitly by means of the breaks argument to the tm_fill(). It is important to note that, in tmap the breaks include a minimum and maximum. As a result, in order to end up with n categories, n+1 elements must be specified in the breaks option (the values must be in increasing order).\nBefore we get started, it is always a good practice to get some descriptive statistics on the variable before setting the break points. Code chunk below will be used to compute and display the descriptive statistics of DEPENDENCY field.\n\nsummary(mpsz_pop2020$DEPENDENCY)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.1111  0.7147  0.7866  0.8585  0.8763 19.0000      92 \n\n\nWith reference to the results above, we set break point at 0.60, 0.70, 0.80, and 0.90. In addition, we also need to include a minimum and maximum, which we set at 0 and 100. Our breaks vector is thus c(0, 0.60, 0.70, 0.80, 0.90, 1.00)\nNow, we will plot the choropleth map by using the code chunk below.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          breaks = c(0, 0.60, 0.70, 0.80, 0.90, 1.00)) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n\ntmap supports colour ramps either defined by the user or a set of predefined colour ramps from the RColorBrewer package.\n\n\nTo change the colour, we assign the preferred colour to palette argument of tm_fill() as shown in the code chunk below.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"Blues\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\nNotice that the choropleth map is shaded in green.\nTo reverse the colour shading, add a “-” prefix.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\nNotice that the colour scheme has been reversed.\n\n\n\n\nMap layout refers to the combination of all map elements into a cohensive map. Map elements include among others the objects to be mapped, the title, the scale bar, the compass, margins and aspects ratios. Colour settings and data classification methods covered in the previous section relate to the palette and break-points are used to affect how the map looks.\n\n\nIn tmap, several legend options are provided to change the placement, format and appearance of the legend.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"jenks\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone \\n(Jenks classification)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            legend.outside = FALSE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\ntmap allows a wide variety of layout settings to be changed. They can be called by using tmap_style().\nThe code chunk below shows the classic style is used.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"classic\")\n\n\n\n\n\n\n\n\n\n\n\nBeside map style, tmap also also provides arguments to draw other map furniture such as compass, scale bar and grid lines.\nIn the code chunk below, tm_compass(), tm_scale_bar() and tm_grid() are used to add compass, scale bar and grid lines onto the choropleth map.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"No. of persons\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio \\nby planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\n\nTo reset the default style, refer to the code chunk below.\n\ntmap_style(\"white\")\n\n\n\n\n\nSmall multiple maps, also referred to as facet maps, are composed of many maps arrange side-by-side, and sometimes stacked vertically. Small multiple maps enable the visualisation of how spatial relationships change with respect to another variable, such as time.\nIn tmap, small multiple maps can be plotted in three ways:\n\nby assigning multiple values to at least one of the asthetic arguments,\nby defining a group-by variable in tm_facets(), and\nby creating multiple stand-alone maps with tmap_arrange().\n\n\n\nIn this example, small multiple choropleth maps are created by defining ncols in tm_fill()\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(c(\"YOUNG\", \"AGED\"),\n          style = \"equal\", \n          palette = \"Blues\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\")) +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"white\")\n\n\n\n\n\n\n\n\nIn this example, small multiple choropleth maps are created by assigning multiple values to at least one of the aesthetic arguments\n\ntm_shape(mpsz_pop2020)+ \n  tm_polygons(c(\"DEPENDENCY\",\"AGED\"),\n          style = c(\"equal\", \"quantile\"), \n          palette = list(\"Blues\",\"Greens\")) +\n  tm_layout(legend.position = c(\"right\", \"bottom\"))\n\n\n\n\n\n\n\n\n\n\n\nIn this example, multiple small choropleth maps are created by using tm_facets().\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"Blues\",\n          thres.poly = 0) + \n  tm_facets(by=\"REGION_N\", \n            free.coords=TRUE, \n            drop.shapes=FALSE) +\n  tm_layout(legend.show = FALSE,\n            title.position = c(\"center\", \"center\"), \n            title.size = 20) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\nIn this example, multiple small choropleth maps are created by creating multiple stand-alone maps with tmap_arrange().\n\nyoungmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"YOUNG\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\nagedmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"AGED\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\ntmap_arrange(youngmap, agedmap, asp=1, ncol=2)\n\n\n\n\n\n\n\n\n\n\n\n\nInstead of creating small multiple choropleth map, you can also use selection funtion to map spatial objects meeting the selection criterion.\n\ntm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ])+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(legend.outside = TRUE,\n            legend.height = 0.45, \n            legend.width = 5.0,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands_on_ex7.html#reference",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands_on_ex7.html#reference",
    "title": "Hand-on exercise 7 Visual Analytics",
    "section": "",
    "text": "tmap: Thematic Maps in R\ntmap\ntmap: get started!\ntmap: changes in version 2.0\ntmap: creating thematic maps in a flexible way (useR!2015)\nExploring and presenting maps with tmap (useR!2017)\n\n\n\n\n\nsf: Simple Features for R\nSimple Features for R: StandardizedSupport for Spatial Vector Data\nReading, Writing and Converting Simple Features\n\n\n\n\n\ndplyr\nTidy data\ntidyr: Easily Tidy Data with ‘spread()’ and ‘gather()’ Functions"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands_on_ex7.html#learning-outcome",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands_on_ex7.html#learning-outcome",
    "title": "Hand-on exercise 7 Visual Analytics",
    "section": "2.1.1 Learning outcome",
    "text": "2.1.1 Learning outcome\nBy the end of this hands-on exercise, you will acquire the following skills by using appropriate R packages:\n\nTo import an aspatial data file into R.\nTo convert it into simple point feature data frame and at the same time, to assign an appropriate projection reference to the newly create simple point feature data frame.\nTo plot interactive proportional symbol maps."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands_on_ex7.html#the-data-1",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands_on_ex7.html#the-data-1",
    "title": "Hand-on exercise 7 Visual Analytics",
    "section": "2.1 The data",
    "text": "2.1 The data\nThe data set use for this hands-on exercise is called SGPools_svy21. The data is in csv file format.\nFigure below shows the first 15 records of SGPools_svy21.csv. It consists of seven columns. The XCOORD and YCOORD columns are the x-coordinates and y-coordinates of SingPools outlets and branches. They are in Singapore SVY21 Projected Coordinates System."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands_on_ex7.html#data-import-and-preparation",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands_on_ex7.html#data-import-and-preparation",
    "title": "Hand-on exercise 7 Visual Analytics",
    "section": "2.2 Data Import and Preparation",
    "text": "2.2 Data Import and Preparation\nThe code chunk below uses read_csv() function of readr package to import SGPools_svy21.csv into R as a tibble data frame called sgpools.\n\nsgpools &lt;- read_csv(\"data/aspatial/SGPools_svy21.csv\")\n\nAfter importing the data file into R, it is important for us to examine if the data file has been imported correctly.\nThe code chunk below shows list() is used to do the job.\n\nlist(sgpools) \n\n[[1]]\n# A tibble: 306 × 7\n   NAME           ADDRESS POSTCODE XCOORD YCOORD `OUTLET TYPE` `Gp1Gp2 Winnings`\n   &lt;chr&gt;          &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;                     &lt;dbl&gt;\n 1 Livewire (Mar… 2 Bayf…    18972 30842. 29599. Branch                        5\n 2 Livewire (Res… 26 Sen…    98138 26704. 26526. Branch                       11\n 3 SportsBuzz (K… Lotus …   738078 20118. 44888. Branch                        0\n 4 SportsBuzz (P… 1 Sele…   188306 29777. 31382. Branch                       44\n 5 Prime Serango… Blk 54…   552542 32239. 39519. Branch                        0\n 6 Singapore Poo… 1A Woo…   731001 21012. 46987. Branch                        3\n 7 Singapore Poo… Blk 64…   370064 33990. 34356. Branch                       17\n 8 Singapore Poo… Blk 88…   370088 33847. 33976. Branch                       16\n 9 Singapore Poo… Blk 30…   540308 33910. 41275. Branch                       21\n10 Singapore Poo… Blk 20…   560202 29246. 38943. Branch                       25\n# ℹ 296 more rows\n\n\nNotice that the sgpools data in tibble data frame and not the common R data frame."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands_on_ex7.html#creating-a-sf-data-frame-from-an-aspatial-data-frame",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands_on_ex7.html#creating-a-sf-data-frame-from-an-aspatial-data-frame",
    "title": "Hand-on exercise 7 Visual Analytics",
    "section": "2.2.3 Creating a sf data frame from an aspatial data frame",
    "text": "2.2.3 Creating a sf data frame from an aspatial data frame\nThe code chunk below converts sgpools data frame into a simple feature data frame by using st_as_sf() of sf packages\n\nsgpools_sf &lt;- st_as_sf(sgpools, \n                       coords = c(\"XCOORD\", \"YCOORD\"),\n                       crs= 3414)\n\nThings to learn from the arguments above:\n\nThe coords argument requires you to provide the column name of the x-coordinates first then followed by the column name of the y-coordinates.\nThe crs argument required you to provide the coordinates system in epsg format. EPSG: 3414 is Singapore SVY21 Projected Coordinate System. You can search for other country’s epsg code by refering to epsg.io.\n\nFigure below shows the data table of sgpools_sf. Notice that a new column called geometry has been added into the data frame.\n\nYou can display the basic information of the newly created sgpools_sf by using the code chunk below.\n\nlist(sgpools_sf)\n\n[[1]]\nSimple feature collection with 306 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 7844.194 ymin: 26525.7 xmax: 45176.57 ymax: 47987.13\nProjected CRS: SVY21 / Singapore TM\n# A tibble: 306 × 6\n   NAME                         ADDRESS POSTCODE `OUTLET TYPE` `Gp1Gp2 Winnings`\n * &lt;chr&gt;                        &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;                     &lt;dbl&gt;\n 1 Livewire (Marina Bay Sands)  2 Bayf…    18972 Branch                        5\n 2 Livewire (Resorts World Sen… 26 Sen…    98138 Branch                       11\n 3 SportsBuzz (Kranji)          Lotus …   738078 Branch                        0\n 4 SportsBuzz (PoMo)            1 Sele…   188306 Branch                       44\n 5 Prime Serangoon North        Blk 54…   552542 Branch                        0\n 6 Singapore Pools Woodlands C… 1A Woo…   731001 Branch                        3\n 7 Singapore Pools 64 Circuit … Blk 64…   370064 Branch                       17\n 8 Singapore Pools 88 Circuit … Blk 88…   370088 Branch                       16\n 9 Singapore Pools Anchorvale … Blk 30…   540308 Branch                       21\n10 Singapore Pools Ang Mo Kio … Blk 20…   560202 Branch                       25\n# ℹ 296 more rows\n# ℹ 1 more variable: geometry &lt;POINT [m]&gt;\n\n\nThe output shows that sgppols_sf is in point feature class. It’s epsg ID is 3414. The bbox provides information of the extend of the geospatial data."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands_on_ex7.html#it-all-started-with-an-interactive-point-symbol-map",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands_on_ex7.html#it-all-started-with-an-interactive-point-symbol-map",
    "title": "Hand-on exercise 7 Visual Analytics",
    "section": "2.3.1 It all started with an interactive point symbol map",
    "text": "2.3.1 It all started with an interactive point symbol map\nThe code chunks below are used to create an interactive point symbol map.\n\ntm_shape(sgpools_sf)+\ntm_bubbles(col = \"red\",\n           size = 1,\n           border.col = \"black\",\n           border.lwd = 1)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands_on_ex7.html#lets-make-it-proportional",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands_on_ex7.html#lets-make-it-proportional",
    "title": "Hand-on exercise 7 Visual Analytics",
    "section": "2.3.2 Lets make it proportional",
    "text": "2.3.2 Lets make it proportional\nTo draw a proportional symbol map, we need to assign a numerical variable to the size visual attribute. The code chunks below show that the variable Gp1Gp2Winnings is assigned to size visual attribute.\n\ntm_shape(sgpools_sf)+\ntm_bubbles(col = \"red\",\n           size = \"Gp1Gp2 Winnings\",\n           border.col = \"black\",\n           border.lwd = 1)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands_on_ex7.html#lets-give-it-a-different-colour",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands_on_ex7.html#lets-give-it-a-different-colour",
    "title": "Hand-on exercise 7 Visual Analytics",
    "section": "2.3.3 Lets give it a different colour",
    "text": "2.3.3 Lets give it a different colour\nThe proportional symbol map can be further improved by using the colour visual attribute. In the code chunks below, OUTLET_TYPE variable is used as the colour attribute variable.\n\ntm_shape(sgpools_sf)+\ntm_bubbles(col = \"OUTLET TYPE\", \n          size = \"Gp1Gp2 Winnings\",\n          border.col = \"black\",\n          border.lwd = 1)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands_on_ex7.html#i-have-a-twin-brothers",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands_on_ex7.html#i-have-a-twin-brothers",
    "title": "Hand-on exercise 7 Visual Analytics",
    "section": "2.3.4 I have a twin brothers :)",
    "text": "2.3.4 I have a twin brothers :)\nAn impressive and little-know feature of tmap’s view mode is that it also works with faceted plots. The argument sync in tm_facets() can be used in this case to produce multiple maps with synchronised zoom and pan settings.\n\ntm_shape(sgpools_sf) +\n  tm_bubbles(col = \"OUTLET TYPE\", \n          size = \"Gp1Gp2 Winnings\",\n          border.col = \"black\",\n          border.lwd = 1) +\n  tm_facets(by= \"OUTLET TYPE\",\n            nrow = 1,\n            sync = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\nBefore you end the session, it is wiser to switch tmap’s Viewer back to plot mode by using the code chunk below.\n\ntmap_mode(\"plot\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands_on_ex7.html#all-about-tmap-package-1",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands_on_ex7.html#all-about-tmap-package-1",
    "title": "Hand-on exercise 7 Visual Analytics",
    "section": "2.4.1 All about tmap package",
    "text": "2.4.1 All about tmap package\n\ntmap: Thematic Maps in R\ntmap\ntmap: get started!\ntmap: changes in version 2.0\ntmap: creating thematic maps in a flexible way (useR!2015)\nExploring and presenting maps with tmap (useR!2017)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands_on_ex7.html#geospatial-data-wrangling-2",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands_on_ex7.html#geospatial-data-wrangling-2",
    "title": "Hand-on exercise 7 Visual Analytics",
    "section": "2.4.2 Geospatial data wrangling",
    "text": "2.4.2 Geospatial data wrangling\n\nsf: Simple Features for R\nSimple Features for R: StandardizedSupport for Spatial Vector Data\nReading, Writing and Converting Simple Features"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands_on_ex7.html#data-wrangling-2",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands_on_ex7.html#data-wrangling-2",
    "title": "Hand-on exercise 7 Visual Analytics",
    "section": "2.4.3 Data wrangling",
    "text": "2.4.3 Data wrangling\n\ndplyr\nTidy data\ntidyr: Easily Tidy Data with ‘spread()’ and ‘gather()’ Functions"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands_on_ex7.html#overview-2",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands_on_ex7.html#overview-2",
    "title": "Hand-on exercise 7 Visual Analytics",
    "section": "3.1 Overview",
    "text": "3.1 Overview\n\n3.1.1 Objectives\nIn this in-class exercise, you will gain hands-on experience on using appropriate R methods to plot analytical maps.\n\n\n3.1.2 Learning outcome\nBy the end of this in-class exercise, you will be able to use appropriate functions of tmap and tidyverse to perform the following tasks:\n\nImporting geospatial data in rds format into R environment.\nCreating cartographic quality choropleth maps by using appropriate tmap functions.\nCreating rate map\nCreating percentile map\nCreating boxmap"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands_on_ex7.html#getting-started-2",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands_on_ex7.html#getting-started-2",
    "title": "Hand-on exercise 7 Visual Analytics",
    "section": "3.2 Getting Started",
    "text": "3.2 Getting Started\n\n3.2.1 Installing and loading packages\n\npacman::p_load(tmap, tidyverse, sf)\n\n\n\n3.2.2 Importing data\nFor the purpose of this hands-on exercise, a prepared data set called NGA_wp.rds will be used. The data set is a polygon feature data.frame providing information on water point of Nigeria at the LGA level. You can find the data set in the rds sub-direct of the hands-on data folder.\n\nNGA_wp &lt;- read_rds(\"data/rds/NGA_wp.rds\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands_on_ex7.html#basic-choropleth-mapping",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands_on_ex7.html#basic-choropleth-mapping",
    "title": "Hand-on exercise 7 Visual Analytics",
    "section": "3.3 Basic Choropleth Mapping",
    "text": "3.3 Basic Choropleth Mapping\n\n3.3.1 Visualising distribution of non-functional water point\n\n\n\n\n\n\nNote\n\n\n\n Plot a choropleth map showing the distribution of non-function water point by LGA\n\n\n\np1 &lt;- tm_shape(NGA_wp) +\n  tm_fill(\"wp_functional\",\n          n = 10,\n          style = \"equal\",\n          palette = \"Blues\") +\n  tm_borders(lwd = 0.1,\n             alpha = 1) +\n  tm_layout(main.title = \"Distribution of functional water point by LGAs\",\n            legend.outside = FALSE)\n\n\np2 &lt;- tm_shape(NGA_wp) +\n  tm_fill(\"total_wp\",\n          n = 10,\n          style = \"equal\",\n          palette = \"Blues\") +\n  tm_borders(lwd = 0.1,\n             alpha = 1) +\n  tm_layout(main.title = \"Distribution of total  water point by LGAs\",\n            legend.outside = FALSE)\n\n\ntmap_arrange(p2, p1, nrow = 1)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands_on_ex7.html#choropleth-map-for-rates",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands_on_ex7.html#choropleth-map-for-rates",
    "title": "Hand-on exercise 7 Visual Analytics",
    "section": "3.4 Choropleth Map for Rates",
    "text": "3.4 Choropleth Map for Rates\nIn much of our readings we have now seen the importance to map rates rather than counts of things, and that is for the simple reason that water points are not equally distributed in space. That means that if we do not account for how many water points are somewhere, we end up mapping total water point size rather than our topic of interest.\n\n3.4.1 Deriving Proportion of Functional Water Points and Non-Functional Water Points\nWe will tabulate the proportion of functional water points and the proportion of non-functional water points in each LGA. In the following code chunk, mutate() from dplyr package is used to derive two fields, namely pct_functional and pct_nonfunctional.\n\nNGA_wp &lt;- NGA_wp %&gt;%\n  mutate(pct_functional = wp_functional/total_wp) %&gt;%\n  mutate(pct_nonfunctional = wp_nonfunctional/total_wp)\n\n\n\n3.4.2 Plotting map of rate\n\ntm_shape(NGA_wp) +\n  tm_fill(\"pct_functional\",\n          n = 10,\n          style = \"equal\",\n          palette = \"Blues\",\n          legend.hist = TRUE) +\n  tm_borders(lwd = 0.1,\n             alpha = 1) +\n  tm_layout(main.title = \"Rate map of functional water point by LGAs\",\n            legend.outside = TRUE)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands_on_ex7.html#extreme-value-maps",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands_on_ex7.html#extreme-value-maps",
    "title": "Hand-on exercise 7 Visual Analytics",
    "section": "3.5 Extreme Value Maps",
    "text": "3.5 Extreme Value Maps\nExtreme value maps are variations of common choropleth maps where the classification is designed to highlight extreme values at the lower and upper end of the scale, with the goal of identifying outliers. These maps were developed in the spirit of spatializing EDA, i.e., adding spatial features to commonly used approaches in non-spatial EDA (Anselin 1994).\n\n3.5.1 Percentile Map\nThe percentile map is a special type of quantile map with six specific categories: 0-1%,1-10%, 10-50%,50-90%,90-99%, and 99-100%. The corresponding breakpoints can be derived by means of the base R quantile command, passing an explicit vector of cumulative probabilities as c(0,.01,.1,.5,.9,.99,1). Note that the begin and endpoint need to be included.3.5.1.1 Data Preparation\nStep 1: Exclude records with NA by using the code chunk below.\n\nNGA_wp &lt;- NGA_wp %&gt;%\n  drop_na()\n\nStep 2: Creating customised classification and extracting values\n\npercent &lt;- c(0,.01,.1,.5,.9,.99,1)\nvar &lt;- NGA_wp[\"pct_functional\"] %&gt;%\n  st_set_geometry(NULL)\nquantile(var[,1], percent)\n\n       0%        1%       10%       50%       90%       99%      100% \n0.0000000 0.0000000 0.2169811 0.4791667 0.8611111 1.0000000 1.0000000 \n\n\n\n3.5.1.2 Why writing functions?\nWriting a function has three big advantages over using copy-and-paste:\n\nYou can give a function an evocative name that makes your code easier to understand.\nAs requirements change, you only need to update code in one place, instead of many.\nYou eliminate the chance of making incidental mistakes when you copy and paste (i.e. updating a variable name in one place, but not in another).\n\nSource: Chapter 19: Functions of R for Data Science.\n\n\n3.5.1.3 Creating the get.var function\nFirstly, we will write an R function as shown below to extract a variable (i.e. wp_nonfunctional) as a vector out of an sf data.frame.\n\narguments:\n\nvname: variable name (as character, in quotes)\ndf: name of sf data frame\n\nreturns:\n\nv: vector with values (without a column name)\n\n\n\nget.var &lt;- function(vname,df) {\n  v &lt;- df[vname] %&gt;% \n    st_set_geometry(NULL)\n  v &lt;- unname(v[,1])\n  return(v)\n}\n\n\n\n3.5.1.4 A percentile mapping function\nNext, we will write a percentile mapping function by using the code chunk below.\n\npercentmap &lt;- function(vnam, df, legtitle=NA, mtitle=\"Percentile Map\"){\n  percent &lt;- c(0,.01,.1,.5,.9,.99,1)\n  var &lt;- get.var(vnam, df)\n  bperc &lt;- quantile(var, percent)\n  tm_shape(df) +\n  tm_polygons() +\n  tm_shape(df) +\n     tm_fill(vnam,\n             title=legtitle,\n             breaks=bperc,\n             palette=\"Blues\",\n          labels=c(\"&lt; 1%\", \"1% - 10%\", \"10% - 50%\", \"50% - 90%\", \"90% - 99%\", \"&gt; 99%\"))  +\n  tm_borders() +\n  tm_layout(main.title = mtitle, \n            title.position = c(\"right\",\"bottom\"))\n}\n\n\n\n3.5.1.5 Test drive the percentile mapping function\nTo run the function, type the code chunk as shown below.\n\npercentmap(\"total_wp\", NGA_wp)\n\n\n\n\n\n\n\n\nNote that this is just a bare bones implementation. Additional arguments such as the title, legend positioning just to name a few of them, could be passed to customise various features of the map.\n\n\n\n3.5.2 Box map\nIn essence, a box map is an augmented quartile map, with an additional lower and upper category. When there are lower outliers, then the starting point for the breaks is the minimum value, and the second break is the lower fence. In contrast, when there are no lower outliers, then the starting point for the breaks will be the lower fence, and the second break is the minimum value (there will be no observations that fall in the interval between the lower fence and the minimum value).\n\nggplot(data = NGA_wp,\n       aes(x = \"\",\n           y = wp_nonfunctional)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\nDisplaying summary statistics on a choropleth map by using the basic principles of boxplot.\nTo create a box map, a custom breaks specification will be used. However, there is a complication. The break points for the box map vary depending on whether lower or upper outliers are present.\n\n\n3.5.2.1 Creating the boxbreaks function\nThe code chunk below is an R function that creating break points for a box map.\n\narguments:\n\nv: vector with observations\nmult: multiplier for IQR (default 1.5)\n\nreturns:\n\nbb: vector with 7 break points compute quartile and fences\n\n\n\nboxbreaks &lt;- function(v,mult=1.5) {\n  qv &lt;- unname(quantile(v))\n  iqr &lt;- qv[4] - qv[2]\n  upfence &lt;- qv[4] + mult * iqr\n  lofence &lt;- qv[2] - mult * iqr\n  # initialize break points vector\n  bb &lt;- vector(mode=\"numeric\",length=7)\n  # logic for lower and upper fences\n  if (lofence &lt; qv[1]) {  # no lower outliers\n    bb[1] &lt;- lofence\n    bb[2] &lt;- floor(qv[1])\n  } else {\n    bb[2] &lt;- lofence\n    bb[1] &lt;- qv[1]\n  }\n  if (upfence &gt; qv[5]) { # no upper outliers\n    bb[7] &lt;- upfence\n    bb[6] &lt;- ceiling(qv[5])\n  } else {\n    bb[6] &lt;- upfence\n    bb[7] &lt;- qv[5]\n  }\n  bb[3:5] &lt;- qv[2:4]\n  return(bb)\n}\n\n\n\n3.5.2.2 Creating the get.var function\nThe code chunk below is an R function to extract a variable as a vector out of an sf data frame.\n\narguments:\n\nvname: variable name (as character, in quotes)\ndf: name of sf data frame\n\nreturns:\n\nv: vector with values (without a column name)\n\n\n\nget.var &lt;- function(vname,df) {\n  v &lt;- df[vname] %&gt;% st_set_geometry(NULL)\n  v &lt;- unname(v[,1])\n  return(v)\n}\n\n\n\n3.5.2.3 Test drive the newly created function\nLet’s test the newly created function\n\nvar &lt;- get.var(\"wp_nonfunctional\", NGA_wp) \nboxbreaks(var)\n\n[1] -56.5   0.0  14.0  34.0  61.0 131.5 278.0\n\n\n\n\n3.5.2.4 Boxmap function\nThe code chunk below is an R function to create a box map. - arguments: - vnam: variable name (as character, in quotes) - df: simple features polygon layer - legtitle: legend title - mtitle: map title - mult: multiplier for IQR - returns: - a tmap-element (plots a map)\n\nboxmap &lt;- function(vnam, df, \n                   legtitle=NA,\n                   mtitle=\"Box Map\",\n                   mult=1.5){\n  var &lt;- get.var(vnam,df)\n  bb &lt;- boxbreaks(var)\n  tm_shape(df) +\n    tm_polygons() +\n  tm_shape(df) +\n     tm_fill(vnam,title=legtitle,\n             breaks=bb,\n             palette=\"Blues\",\n          labels = c(\"lower outlier\", \n                     \"&lt; 25%\", \n                     \"25% - 50%\", \n                     \"50% - 75%\",\n                     \"&gt; 75%\", \n                     \"upper outlier\"))  +\n  tm_borders() +\n  tm_layout(main.title = mtitle, \n            title.position = c(\"left\",\n                               \"top\"))\n}\n\n\ntmap_mode(\"plot\")\nboxmap(\"wp_nonfunctional\", NGA_wp)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands_on_ex8.html",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands_on_ex8.html",
    "title": "Hand-on exercise 8 Visual Analytics",
    "section": "",
    "text": "In this hands-on exercise, you will learn how to model, analyse and visualise network data using R.\nBy the end of this hands-on exercise, you will be able to:\n\ncreate graph object data frames, manipulate them using appropriate functions of dplyr, lubridate, and tidygraph,\nbuild network graph visualisation using appropriate functions of ggraph,\ncompute network geometrics using tidygraph,\nbuild advanced graph visualisation by incorporating the network geometrics, and\nbuild interactive network visualisation using visNetwork package.\n\n\n\n\n\n\nIn this hands-on exercise, four network data modelling and visualisation packages will be installed and launched. They are igraph, tidygraph, ggraph and visNetwork. Beside these four packages, tidyverse and lubridate, an R package specially designed to handle and wrangling time data will be installed and launched too.\nThe code chunk:\n\npacman::p_load(igraph, tidygraph, ggraph, \n               visNetwork, lubridate, clock,\n               tidyverse, graphlayouts)\n\n\n\n\n\nThe data sets used in this hands-on exercise is from an oil exploration and extraction company. There are two data sets. One contains the nodes data and the other contains the edges (also know as link) data.\n\n\n\nGAStech-email_edges.csv which consists of two weeks of 9063 emails correspondances between 55 employees.\n\n\n\n\n\n\nGAStech_email_nodes.csv which consist of the names, department and title of the 55 employees.\n\n\n\n\n\nIn this step, you will import GAStech_email_node.csv and GAStech_email_edges-v2.csv into RStudio environment by using read_csv() of readr package.\n\nGAStech_nodes &lt;- read_csv(\"data/GAStech_email_node.csv\")\nGAStech_edges &lt;- read_csv(\"data/GAStech_email_edge-v2.csv\")\n\n\n\n\nNext, we will examine the structure of the data frame using glimpse() of dplyr.\n\nglimpse(GAStech_edges)\n\nRows: 9,063\nColumns: 8\n$ source      &lt;dbl&gt; 43, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 26, 26, 26…\n$ target      &lt;dbl&gt; 41, 40, 51, 52, 53, 45, 44, 46, 48, 49, 47, 54, 27, 28, 29…\n$ SentDate    &lt;chr&gt; \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\"…\n$ SentTime    &lt;time&gt; 08:39:00, 08:39:00, 08:58:00, 08:58:00, 08:58:00, 08:58:0…\n$ Subject     &lt;chr&gt; \"GT-SeismicProcessorPro Bug Report\", \"GT-SeismicProcessorP…\n$ MainSubject &lt;chr&gt; \"Work related\", \"Work related\", \"Work related\", \"Work rela…\n$ sourceLabel &lt;chr&gt; \"Sven.Flecha\", \"Sven.Flecha\", \"Kanon.Herrero\", \"Kanon.Herr…\n$ targetLabel &lt;chr&gt; \"Isak.Baza\", \"Lucas.Alcazar\", \"Felix.Resumir\", \"Hideki.Coc…\n\n\n\n\n\nThe code chunk below will be used to perform the changes.\n\nGAStech_edges &lt;- GAStech_edges %&gt;%\n  mutate(SendDate = dmy(SentDate)) %&gt;%\n  mutate(Weekday = wday(SentDate,\n                        label = TRUE,\n                        abbr = FALSE))\n\n\n\n\n\n\n\nTip\n\n\n\nThings to learn from the code chunk above\n\nboth dmy() and wday() are functions of lubridate package. lubridate is an R package that makes it easier to work with dates and times.\ndmy() transforms the SentDate to Date data type.\nwday() returns the day of the week as a decimal number or an ordered factor if label is TRUE. The argument abbr is FALSE keep the daya spells in full, i.e. Monday. The function will create a new column in the data.frame i.e. Weekday and the output of wday() will save in this newly created field.\nthe values in the Weekday field are in ordinal scale.\n\n\n\n\n\n\nA close examination of GAStech_edges data.frame reveals that it consists of individual e-mail flow records. This is not very useful for visualisation.\nIn view of this, we will aggregate the individual by date, senders, receivers, main subject and day of the week.\nThe code chunk:\n\nGAStech_edges_aggregated &lt;- GAStech_edges %&gt;%\n  filter(MainSubject == \"Work related\") %&gt;%\n  group_by(source, target, Weekday) %&gt;%\n    summarise(Weight = n()) %&gt;%\n  filter(source!=target) %&gt;%\n  filter(Weight &gt; 1) %&gt;%\n  ungroup()\n\nglimpse(GAStech_edges_aggregated)\n\nRows: 1,372\nColumns: 4\n$ source  &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ target  &lt;dbl&gt; 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 6,…\n$ Weekday &lt;ord&gt; Sunday, Monday, Tuesday, Wednesday, Friday, Sunday, Monday, Tu…\n$ Weight  &lt;int&gt; 5, 2, 3, 4, 6, 5, 2, 3, 4, 6, 5, 2, 3, 4, 6, 5, 2, 3, 4, 6, 5,…\n\n\n\n\n\n\nIn this section, you will learn how to create a graph data model by using tidygraph package. It provides a tidy API for graph/network manipulation. While network data itself is not tidy, it can be envisioned as two tidy tables, one for node data and one for edge data. tidygraph provides a way to switch between the two tables and provides dplyr verbs for manipulating them. Furthermore it provides access to a lot of graph algorithms with return values that facilitate their use in a tidy workflow.\nBefore getting started, you are advised to read these two articles:\n\nIntroducing tidygraph\ntidygraph 1.1 - A tidy hope\n\n\n\nTwo functions of tidygraph package can be used to create network objects, they are:\n\ntbl_graph() creates a tbl_graph network object from nodes and edges data.\nas_tbl_graph() converts network data and objects to a tbl_graph network. Below are network data and objects supported by as_tbl_graph()\n\na node data.frame and an edge data.frame,\ndata.frame, list, matrix from base,\nigraph from igraph,\nnetwork from network,\ndendrogram and hclust from stats,\nNode from data.tree,\nphylo and evonet from ape, and\ngraphNEL, graphAM, graphBAM from graph (in Bioconductor).\n\n\n\n\n\n\nactivate() verb from tidygraph serves as a switch between tibbles for nodes and edges. All dplyr verbs applied to tbl_graph object are applied to the active tibble.\n\n\n\nIn the above the .N() function is used to gain access to the node data while manipulating the edge data. Similarly .E() will give you the edge data and .G() will give you the tbl_graph object itself.\n\n\n\n\nIn this section, you will use tbl_graph() of tinygraph package to build an tidygraph’s network graph data.frame.\nBefore typing the codes, you are recommended to review to reference guide of tbl_graph()\n\nGAStech_graph &lt;- tbl_graph(nodes = GAStech_nodes,\n                           edges = GAStech_edges_aggregated, \n                           directed = TRUE)\n\n\n\n\n\nGAStech_graph\n\n# A tbl_graph: 54 nodes and 1372 edges\n#\n# A directed multigraph with 1 component\n#\n# Node Data: 54 × 4 (active)\n      id label               Department     Title                               \n   &lt;dbl&gt; &lt;chr&gt;               &lt;chr&gt;          &lt;chr&gt;                               \n 1     1 Mat.Bramar          Administration Assistant to CEO                    \n 2     2 Anda.Ribera         Administration Assistant to CFO                    \n 3     3 Rachel.Pantanal     Administration Assistant to CIO                    \n 4     4 Linda.Lagos         Administration Assistant to COO                    \n 5     5 Ruscella.Mies.Haber Administration Assistant to Engineering Group Mana…\n 6     6 Carla.Forluniau     Administration Assistant to IT Group Manager       \n 7     7 Cornelia.Lais       Administration Assistant to Security Group Manager \n 8    44 Kanon.Herrero       Security       Badging Office                      \n 9    45 Varja.Lagos         Security       Badging Office                      \n10    46 Stenig.Fusil        Security       Building Control                    \n# ℹ 44 more rows\n#\n# Edge Data: 1,372 × 4\n   from    to Weekday Weight\n  &lt;int&gt; &lt;int&gt; &lt;ord&gt;    &lt;int&gt;\n1     1     2 Sunday       5\n2     1     2 Monday       2\n3     1     2 Tuesday      3\n# ℹ 1,369 more rows\n\n\n\n\n\n\nThe output above reveals that GAStech_graph is a tbl_graph object with 54 nodes and 4541 edges.\nThe command also prints the first six rows of “Node Data” and the first three of “Edge Data”.\nIt states that the Node Data is active. The notion of an active tibble within a tbl_graph object makes it possible to manipulate the data in one tibble at a time.\n\n\n\n\nThe nodes tibble data frame is activated by default, but you can change which tibble data frame is active with the activate() function. Thus, if we wanted to rearrange the rows in the edges tibble to list those with the highest “weight” first, we could use activate() and then arrange().\nFor example,\n\nGAStech_graph %&gt;%\n  activate(edges) %&gt;%\n  arrange(desc(Weight))\n\n# A tbl_graph: 54 nodes and 1372 edges\n#\n# A directed multigraph with 1 component\n#\n# Edge Data: 1,372 × 4 (active)\n    from    to Weekday   Weight\n   &lt;int&gt; &lt;int&gt; &lt;ord&gt;      &lt;int&gt;\n 1    40    41 Saturday      13\n 2    41    43 Monday        11\n 3    35    31 Tuesday       10\n 4    40    41 Monday        10\n 5    40    43 Monday        10\n 6    36    32 Sunday         9\n 7    40    43 Saturday       9\n 8    41    40 Monday         9\n 9    19    15 Wednesday      8\n10    35    38 Tuesday        8\n# ℹ 1,362 more rows\n#\n# Node Data: 54 × 4\n     id label           Department     Title           \n  &lt;dbl&gt; &lt;chr&gt;           &lt;chr&gt;          &lt;chr&gt;           \n1     1 Mat.Bramar      Administration Assistant to CEO\n2     2 Anda.Ribera     Administration Assistant to CFO\n3     3 Rachel.Pantanal Administration Assistant to CIO\n# ℹ 51 more rows\n\n\nVisit the reference guide of activate() to find out more about the function.\n\n\n\n\nggraph is an extension of ggplot2, making it easier to carry over basic ggplot skills to the design of network graphs.\nAs in all network graph, there are three main aspects to a ggraph’s network graph, they are:\n\nnodes,\nedges and\nlayouts.\n\nFor a comprehensive discussion of each of this aspect of graph, please refer to their respective vignettes provided.\n\n\nThe code chunk below uses ggraph(), geom-edge_link() and geom_node_point() to plot a network graph by using GAStech_graph. Before your get started, it is advisable to read their respective reference guide at least once.\n\nggraph(GAStech_graph) +\n  geom_edge_link() +\n  geom_node_point()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThings to learn from the code chunk above:\n\nThe basic plotting function is ggraph(), which takes the data to be used for the graph and the type of layout desired. Both of the arguments for ggraph() are built around igraph. Therefore, ggraph() can use either an igraph object or a tbl_graph object.\n\n\n\n\n\n\nIn this section, you will use theme_graph() to remove the x and y axes. Before your get started, it is advisable to read it’s reference guide at least once.\n\ng &lt;- ggraph(GAStech_graph) + \n  geom_edge_link(aes()) +\n  geom_node_point(aes())\n\ng + theme_graph()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThings to learn from the code chunk above:\n\nggraph introduces a special ggplot theme that provides better defaults for network graphs than the normal ggplot defaults. theme_graph(), besides removing axes, grids, and border, changes the font to Arial Narrow (this can be overridden).\nThe ggraph theme can be set for a series of plots with the set_graph_style() command run before the graphs are plotted or by using theme_graph() in the individual plots.\n\n\n\n\n\n\nFurthermore, theme_graph() makes it easy to change the coloring of the plot.\n\ng &lt;- ggraph(GAStech_graph) + \n  geom_edge_link(aes(colour = 'grey50')) +\n  geom_node_point(aes(colour = 'grey40'))\n\ng + theme_graph(background = 'grey10',\n                text_colour = 'white')\n\n\n\n\n\n\n\n\n\n\n\nggraph support many layout for standard used, they are: star, circle, nicely (default), dh, gem, graphopt, grid, mds, spahere, randomly, fr, kk, drl and lgl. Figures below and on the right show layouts supported by ggraph().\n\n\n\n\n\n\n\n\n\nThe code chunks below will be used to plot the network graph using Fruchterman and Reingold layout.\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"fr\") +\n  geom_edge_link(aes()) +\n  geom_node_point(aes())\n\ng + theme_graph()\n\n\n\n\n\n\n\n\nThing to learn from the code chunk above:\n\nlayout argument is used to define the layout to be used.\n\n\n\n\nIn this section, you will colour each node by referring to their respective departments.\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes()) +\n  geom_node_point(aes(colour = Department, \n                      size = 3))\n\ng + theme_graph()\n\n\n\n\n\n\n\n\nThings to learn from the code chunks above:\n\ngeom_node_point is equivalent in functionality to geo_point of ggplot2. It allows for simple plotting of nodes in different shapes, colours and sizes. In the codes chnuks above colour and size are used.\n\n\n\n\nIn the code chunk below, the thickness of the edges will be mapped with the Weight variable.\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") +\n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 3)\n\ng + theme_graph()\n\n\n\n\n\n\n\n\nThings to learn from the code chunks above:\n\ngeom_edge_link draws edges in the simplest way - as straight lines between the start and end nodes. But, it can do more that that. In the example above, argument width is used to map the width of the line in proportional to the Weight attribute and argument alpha is used to introduce opacity on the line.\n\n\n\n\n\nAnother very useful feature of ggraph is faceting. In visualising network data, this technique can be used to reduce edge over-plotting in a very meaning way by spreading nodes and edges out based on their attributes. In this section, you will learn how to use faceting technique to visualise network data.\nThere are three functions in ggraph to implement faceting, they are:\n\nfacet_nodes() whereby edges are only draw in a panel if both terminal nodes are present here,\nfacet_edges() whereby nodes are always drawn in al panels even if the node data contains an attribute named the same as the one used for the edge facetting, and\nfacet_graph() faceting on two variables simultaneously.\n\n\n\nIn the code chunk below, facet_edges() is used. Before getting started, it is advisable for you to read it’s reference guide at least once.\n\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n\ng + facet_edges(~Weekday)\n\n\n\n\n\n\n\n\n\n\n\nThe code chunk below uses theme() to change the position of the legend.\n\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2) +\n  theme(legend.position = 'bottom')\n  \ng + facet_edges(~Weekday)\n\n\n\n\n\n\n\n\n\n\n\nThe code chunk below adds frame to each graph.\n\nset_graph_style() \n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n  \ng + facet_edges(~Weekday) +\n  th_foreground(foreground = \"grey80\",  \n                border = TRUE) +\n  theme(legend.position = 'bottom')\n\n\n\n\n\n\n\n\n\n\n\nIn the code chunkc below, facet_nodes() is used. Before getting started, it is advisable for you to read it’s reference guide at least once.\n\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n  \ng + facet_nodes(~Department)+\n  th_foreground(foreground = \"grey80\",  \n                border = TRUE) +\n  theme(legend.position = 'bottom')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCentrality measures are a collection of statistical indices use to describe the relative important of the actors are to a network. There are four well-known centrality measures, namely: degree, betweenness, closeness and eigenvector. It is beyond the scope of this hands-on exercise to cover the principles and mathematics of these measure here. Students are encouraged to refer to Chapter 7: Actor Prominence of A User’s Guide to Network Analysis in R to gain better understanding of theses network measures.\n\ng &lt;- GAStech_graph %&gt;%\n  mutate(betweenness_centrality = centrality_betweenness()) %&gt;%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department,\n            size=betweenness_centrality))\ng + theme_graph()\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above:\n\nmutate() of dplyr is used to perform the computation.\nthe algorithm used, on the other hand, is the centrality_betweenness() of tidygraph.\n\n\n\n\nIt is important to note that from ggraph v2.0 onward tidygraph algorithms such as centrality measures can be accessed directly in ggraph calls. This means that it is no longer necessary to precompute and store derived node and edge centrality measures on the graph in order to use them in a plot.\n\ng &lt;- GAStech_graph %&gt;%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department, \n                      size = centrality_betweenness()))\ng + theme_graph()\n\n\n\n\n\n\n\n\n\n\n\ntidygraph package inherits many of the community detection algorithms imbedded into igraph and makes them available to us, including Edge-betweenness (group_edge_betweenness), Leading eigenvector (group_leading_eigen), Fast-greedy (group_fast_greedy), Louvain (group_louvain), Walktrap (group_walktrap), Label propagation (group_label_prop), InfoMAP (group_infomap), Spinglass (group_spinglass), and Optimal (group_optimal). Some community algorithms are designed to take into account direction or weight, while others ignore it. Use this link to find out more about community detection functions provided by tidygraph,\nIn the code chunk below group_edge_betweenness() is used.\n\ng &lt;- GAStech_graph %&gt;%\n  mutate(community = as.factor(group_edge_betweenness(weights = Weight, directed = TRUE))) %&gt;%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = community))  \n\ng + theme_graph()\n\n\n\n\n\n\n\n\n\n\n\n\n\nvisNetwork() is a R package for network visualization, using vis.js javascript library.\nvisNetwork() function uses a nodes list and edges list to create an interactive graph.\n\nThe nodes list must include an “id” column, and the edge list must have “from” and “to” columns.\nThe function also plots the labels for the nodes, using the names of the actors from the “label” column in the node list.\n\nThe resulting graph is fun to play around with.\n\nYou can move the nodes and the graph will use an algorithm to keep the nodes properly spaced.\nYou can also zoom in and out on the plot and move it around to re-center it.\n\n\n\n\nBefore we can plot the interactive network graph, we need to prepare the data model by using the code chunk below.\n\nGAStech_edges_aggregated &lt;- GAStech_edges %&gt;%\n  left_join(GAStech_nodes, by = c(\"sourceLabel\" = \"label\")) %&gt;%\n  rename(from = id) %&gt;%\n  left_join(GAStech_nodes, by = c(\"targetLabel\" = \"label\")) %&gt;%\n  rename(to = id) %&gt;%\n  filter(MainSubject == \"Work related\") %&gt;%\n  group_by(from, to) %&gt;%\n    summarise(weight = n()) %&gt;%\n  filter(from!=to) %&gt;%\n  filter(weight &gt; 1) %&gt;%\n  ungroup()\n\n\n\n\nThe code chunk below will be used to plot an interactive network graph by using the data prepared.\n\nvisNetwork(GAStech_nodes, \n           GAStech_edges_aggregated)\n\n\n\n\n\n\n\n\nIn the code chunk below, Fruchterman and Reingold layout is used.\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") \n\n\n\n\n\nVisit Igraph to find out more about visIgraphLayout’s argument.\n\n\n\nvisNetwork() looks for a field called “group” in the nodes object and colour the nodes according to the values of the group field.\nThe code chunk below rename Department field to group.\n\nGAStech_nodes &lt;- GAStech_nodes %&gt;%\n  rename(group = Department) \n\nWhen we rerun the code chunk below, visNetwork shades the nodes by assigning unique colour to each category in the group field.\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\n\n\nIn the code run below visEdges() is used to symbolise the edges.\n- The argument arrows is used to define where to place the arrow.\n- The smooth argument is used to plot the edges using a smooth curve.\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") %&gt;%\n  visEdges(arrows = \"to\", \n           smooth = list(enabled = TRUE, \n                         type = \"curvedCW\")) %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 123)\n\n\n\n\n\nVisit Option to find out more about visEdges’s argument.\n\n\n\nIn the code chunk below, visOptions() is used to incorporate interactivity features in the data visualisation.\n\nThe argument highlightNearest highlights nearest when clicking a node.\nThe argument nodesIdSelection adds an id node selection creating an HTML select element.\n\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") %&gt;%\n  visOptions(highlightNearest = TRUE,\n             nodesIdSelection = TRUE) %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 123)\n\n\n\n\n\nVisit Option to find out more about visOption’s argument."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands_on_ex8.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands_on_ex8.html#overview",
    "title": "Hand-on exercise 8 Visual Analytics",
    "section": "",
    "text": "In this hands-on exercise, you will learn how to model, analyse and visualise network data using R.\nBy the end of this hands-on exercise, you will be able to:\n\ncreate graph object data frames, manipulate them using appropriate functions of dplyr, lubridate, and tidygraph,\nbuild network graph visualisation using appropriate functions of ggraph,\ncompute network geometrics using tidygraph,\nbuild advanced graph visualisation by incorporating the network geometrics, and\nbuild interactive network visualisation using visNetwork package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands_on_ex8.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands_on_ex8.html#getting-started",
    "title": "Hand-on exercise 8 Visual Analytics",
    "section": "",
    "text": "In this hands-on exercise, four network data modelling and visualisation packages will be installed and launched. They are igraph, tidygraph, ggraph and visNetwork. Beside these four packages, tidyverse and lubridate, an R package specially designed to handle and wrangling time data will be installed and launched too.\nThe code chunk:\n\npacman::p_load(igraph, tidygraph, ggraph, \n               visNetwork, lubridate, clock,\n               tidyverse, graphlayouts)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands_on_ex8.html#the-data",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands_on_ex8.html#the-data",
    "title": "Hand-on exercise 8 Visual Analytics",
    "section": "",
    "text": "The data sets used in this hands-on exercise is from an oil exploration and extraction company. There are two data sets. One contains the nodes data and the other contains the edges (also know as link) data.\n\n\n\nGAStech-email_edges.csv which consists of two weeks of 9063 emails correspondances between 55 employees.\n\n\n\n\n\n\nGAStech_email_nodes.csv which consist of the names, department and title of the 55 employees.\n\n\n\n\n\nIn this step, you will import GAStech_email_node.csv and GAStech_email_edges-v2.csv into RStudio environment by using read_csv() of readr package.\n\nGAStech_nodes &lt;- read_csv(\"data/GAStech_email_node.csv\")\nGAStech_edges &lt;- read_csv(\"data/GAStech_email_edge-v2.csv\")\n\n\n\n\nNext, we will examine the structure of the data frame using glimpse() of dplyr.\n\nglimpse(GAStech_edges)\n\nRows: 9,063\nColumns: 8\n$ source      &lt;dbl&gt; 43, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 26, 26, 26…\n$ target      &lt;dbl&gt; 41, 40, 51, 52, 53, 45, 44, 46, 48, 49, 47, 54, 27, 28, 29…\n$ SentDate    &lt;chr&gt; \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\"…\n$ SentTime    &lt;time&gt; 08:39:00, 08:39:00, 08:58:00, 08:58:00, 08:58:00, 08:58:0…\n$ Subject     &lt;chr&gt; \"GT-SeismicProcessorPro Bug Report\", \"GT-SeismicProcessorP…\n$ MainSubject &lt;chr&gt; \"Work related\", \"Work related\", \"Work related\", \"Work rela…\n$ sourceLabel &lt;chr&gt; \"Sven.Flecha\", \"Sven.Flecha\", \"Kanon.Herrero\", \"Kanon.Herr…\n$ targetLabel &lt;chr&gt; \"Isak.Baza\", \"Lucas.Alcazar\", \"Felix.Resumir\", \"Hideki.Coc…\n\n\n\n\n\nThe code chunk below will be used to perform the changes.\n\nGAStech_edges &lt;- GAStech_edges %&gt;%\n  mutate(SendDate = dmy(SentDate)) %&gt;%\n  mutate(Weekday = wday(SentDate,\n                        label = TRUE,\n                        abbr = FALSE))\n\n\n\n\n\n\n\nTip\n\n\n\nThings to learn from the code chunk above\n\nboth dmy() and wday() are functions of lubridate package. lubridate is an R package that makes it easier to work with dates and times.\ndmy() transforms the SentDate to Date data type.\nwday() returns the day of the week as a decimal number or an ordered factor if label is TRUE. The argument abbr is FALSE keep the daya spells in full, i.e. Monday. The function will create a new column in the data.frame i.e. Weekday and the output of wday() will save in this newly created field.\nthe values in the Weekday field are in ordinal scale.\n\n\n\n\n\n\nA close examination of GAStech_edges data.frame reveals that it consists of individual e-mail flow records. This is not very useful for visualisation.\nIn view of this, we will aggregate the individual by date, senders, receivers, main subject and day of the week.\nThe code chunk:\n\nGAStech_edges_aggregated &lt;- GAStech_edges %&gt;%\n  filter(MainSubject == \"Work related\") %&gt;%\n  group_by(source, target, Weekday) %&gt;%\n    summarise(Weight = n()) %&gt;%\n  filter(source!=target) %&gt;%\n  filter(Weight &gt; 1) %&gt;%\n  ungroup()\n\nglimpse(GAStech_edges_aggregated)\n\nRows: 1,372\nColumns: 4\n$ source  &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ target  &lt;dbl&gt; 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 6,…\n$ Weekday &lt;ord&gt; Sunday, Monday, Tuesday, Wednesday, Friday, Sunday, Monday, Tu…\n$ Weight  &lt;int&gt; 5, 2, 3, 4, 6, 5, 2, 3, 4, 6, 5, 2, 3, 4, 6, 5, 2, 3, 4, 6, 5,…"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands_on_ex8.html#creating-network-objects-using-tidygraph",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands_on_ex8.html#creating-network-objects-using-tidygraph",
    "title": "Hand-on exercise 8 Visual Analytics",
    "section": "",
    "text": "In this section, you will learn how to create a graph data model by using tidygraph package. It provides a tidy API for graph/network manipulation. While network data itself is not tidy, it can be envisioned as two tidy tables, one for node data and one for edge data. tidygraph provides a way to switch between the two tables and provides dplyr verbs for manipulating them. Furthermore it provides access to a lot of graph algorithms with return values that facilitate their use in a tidy workflow.\nBefore getting started, you are advised to read these two articles:\n\nIntroducing tidygraph\ntidygraph 1.1 - A tidy hope\n\n\n\nTwo functions of tidygraph package can be used to create network objects, they are:\n\ntbl_graph() creates a tbl_graph network object from nodes and edges data.\nas_tbl_graph() converts network data and objects to a tbl_graph network. Below are network data and objects supported by as_tbl_graph()\n\na node data.frame and an edge data.frame,\ndata.frame, list, matrix from base,\nigraph from igraph,\nnetwork from network,\ndendrogram and hclust from stats,\nNode from data.tree,\nphylo and evonet from ape, and\ngraphNEL, graphAM, graphBAM from graph (in Bioconductor).\n\n\n\n\n\n\nactivate() verb from tidygraph serves as a switch between tibbles for nodes and edges. All dplyr verbs applied to tbl_graph object are applied to the active tibble.\n\n\n\nIn the above the .N() function is used to gain access to the node data while manipulating the edge data. Similarly .E() will give you the edge data and .G() will give you the tbl_graph object itself.\n\n\n\n\nIn this section, you will use tbl_graph() of tinygraph package to build an tidygraph’s network graph data.frame.\nBefore typing the codes, you are recommended to review to reference guide of tbl_graph()\n\nGAStech_graph &lt;- tbl_graph(nodes = GAStech_nodes,\n                           edges = GAStech_edges_aggregated, \n                           directed = TRUE)\n\n\n\n\n\nGAStech_graph\n\n# A tbl_graph: 54 nodes and 1372 edges\n#\n# A directed multigraph with 1 component\n#\n# Node Data: 54 × 4 (active)\n      id label               Department     Title                               \n   &lt;dbl&gt; &lt;chr&gt;               &lt;chr&gt;          &lt;chr&gt;                               \n 1     1 Mat.Bramar          Administration Assistant to CEO                    \n 2     2 Anda.Ribera         Administration Assistant to CFO                    \n 3     3 Rachel.Pantanal     Administration Assistant to CIO                    \n 4     4 Linda.Lagos         Administration Assistant to COO                    \n 5     5 Ruscella.Mies.Haber Administration Assistant to Engineering Group Mana…\n 6     6 Carla.Forluniau     Administration Assistant to IT Group Manager       \n 7     7 Cornelia.Lais       Administration Assistant to Security Group Manager \n 8    44 Kanon.Herrero       Security       Badging Office                      \n 9    45 Varja.Lagos         Security       Badging Office                      \n10    46 Stenig.Fusil        Security       Building Control                    \n# ℹ 44 more rows\n#\n# Edge Data: 1,372 × 4\n   from    to Weekday Weight\n  &lt;int&gt; &lt;int&gt; &lt;ord&gt;    &lt;int&gt;\n1     1     2 Sunday       5\n2     1     2 Monday       2\n3     1     2 Tuesday      3\n# ℹ 1,369 more rows\n\n\n\n\n\n\nThe output above reveals that GAStech_graph is a tbl_graph object with 54 nodes and 4541 edges.\nThe command also prints the first six rows of “Node Data” and the first three of “Edge Data”.\nIt states that the Node Data is active. The notion of an active tibble within a tbl_graph object makes it possible to manipulate the data in one tibble at a time.\n\n\n\n\nThe nodes tibble data frame is activated by default, but you can change which tibble data frame is active with the activate() function. Thus, if we wanted to rearrange the rows in the edges tibble to list those with the highest “weight” first, we could use activate() and then arrange().\nFor example,\n\nGAStech_graph %&gt;%\n  activate(edges) %&gt;%\n  arrange(desc(Weight))\n\n# A tbl_graph: 54 nodes and 1372 edges\n#\n# A directed multigraph with 1 component\n#\n# Edge Data: 1,372 × 4 (active)\n    from    to Weekday   Weight\n   &lt;int&gt; &lt;int&gt; &lt;ord&gt;      &lt;int&gt;\n 1    40    41 Saturday      13\n 2    41    43 Monday        11\n 3    35    31 Tuesday       10\n 4    40    41 Monday        10\n 5    40    43 Monday        10\n 6    36    32 Sunday         9\n 7    40    43 Saturday       9\n 8    41    40 Monday         9\n 9    19    15 Wednesday      8\n10    35    38 Tuesday        8\n# ℹ 1,362 more rows\n#\n# Node Data: 54 × 4\n     id label           Department     Title           \n  &lt;dbl&gt; &lt;chr&gt;           &lt;chr&gt;          &lt;chr&gt;           \n1     1 Mat.Bramar      Administration Assistant to CEO\n2     2 Anda.Ribera     Administration Assistant to CFO\n3     3 Rachel.Pantanal Administration Assistant to CIO\n# ℹ 51 more rows\n\n\nVisit the reference guide of activate() to find out more about the function."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands_on_ex8.html#plotting-static-network-graphs-with-ggraph-package",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands_on_ex8.html#plotting-static-network-graphs-with-ggraph-package",
    "title": "Hand-on exercise 8 Visual Analytics",
    "section": "",
    "text": "ggraph is an extension of ggplot2, making it easier to carry over basic ggplot skills to the design of network graphs.\nAs in all network graph, there are three main aspects to a ggraph’s network graph, they are:\n\nnodes,\nedges and\nlayouts.\n\nFor a comprehensive discussion of each of this aspect of graph, please refer to their respective vignettes provided.\n\n\nThe code chunk below uses ggraph(), geom-edge_link() and geom_node_point() to plot a network graph by using GAStech_graph. Before your get started, it is advisable to read their respective reference guide at least once.\n\nggraph(GAStech_graph) +\n  geom_edge_link() +\n  geom_node_point()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThings to learn from the code chunk above:\n\nThe basic plotting function is ggraph(), which takes the data to be used for the graph and the type of layout desired. Both of the arguments for ggraph() are built around igraph. Therefore, ggraph() can use either an igraph object or a tbl_graph object.\n\n\n\n\n\n\nIn this section, you will use theme_graph() to remove the x and y axes. Before your get started, it is advisable to read it’s reference guide at least once.\n\ng &lt;- ggraph(GAStech_graph) + \n  geom_edge_link(aes()) +\n  geom_node_point(aes())\n\ng + theme_graph()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThings to learn from the code chunk above:\n\nggraph introduces a special ggplot theme that provides better defaults for network graphs than the normal ggplot defaults. theme_graph(), besides removing axes, grids, and border, changes the font to Arial Narrow (this can be overridden).\nThe ggraph theme can be set for a series of plots with the set_graph_style() command run before the graphs are plotted or by using theme_graph() in the individual plots.\n\n\n\n\n\n\nFurthermore, theme_graph() makes it easy to change the coloring of the plot.\n\ng &lt;- ggraph(GAStech_graph) + \n  geom_edge_link(aes(colour = 'grey50')) +\n  geom_node_point(aes(colour = 'grey40'))\n\ng + theme_graph(background = 'grey10',\n                text_colour = 'white')\n\n\n\n\n\n\n\n\n\n\n\nggraph support many layout for standard used, they are: star, circle, nicely (default), dh, gem, graphopt, grid, mds, spahere, randomly, fr, kk, drl and lgl. Figures below and on the right show layouts supported by ggraph().\n\n\n\n\n\n\n\n\n\nThe code chunks below will be used to plot the network graph using Fruchterman and Reingold layout.\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"fr\") +\n  geom_edge_link(aes()) +\n  geom_node_point(aes())\n\ng + theme_graph()\n\n\n\n\n\n\n\n\nThing to learn from the code chunk above:\n\nlayout argument is used to define the layout to be used.\n\n\n\n\nIn this section, you will colour each node by referring to their respective departments.\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes()) +\n  geom_node_point(aes(colour = Department, \n                      size = 3))\n\ng + theme_graph()\n\n\n\n\n\n\n\n\nThings to learn from the code chunks above:\n\ngeom_node_point is equivalent in functionality to geo_point of ggplot2. It allows for simple plotting of nodes in different shapes, colours and sizes. In the codes chnuks above colour and size are used.\n\n\n\n\nIn the code chunk below, the thickness of the edges will be mapped with the Weight variable.\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") +\n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 3)\n\ng + theme_graph()\n\n\n\n\n\n\n\n\nThings to learn from the code chunks above:\n\ngeom_edge_link draws edges in the simplest way - as straight lines between the start and end nodes. But, it can do more that that. In the example above, argument width is used to map the width of the line in proportional to the Weight attribute and argument alpha is used to introduce opacity on the line."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands_on_ex8.html#creating-facet-graphs",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands_on_ex8.html#creating-facet-graphs",
    "title": "Hand-on exercise 8 Visual Analytics",
    "section": "",
    "text": "Another very useful feature of ggraph is faceting. In visualising network data, this technique can be used to reduce edge over-plotting in a very meaning way by spreading nodes and edges out based on their attributes. In this section, you will learn how to use faceting technique to visualise network data.\nThere are three functions in ggraph to implement faceting, they are:\n\nfacet_nodes() whereby edges are only draw in a panel if both terminal nodes are present here,\nfacet_edges() whereby nodes are always drawn in al panels even if the node data contains an attribute named the same as the one used for the edge facetting, and\nfacet_graph() faceting on two variables simultaneously.\n\n\n\nIn the code chunk below, facet_edges() is used. Before getting started, it is advisable for you to read it’s reference guide at least once.\n\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n\ng + facet_edges(~Weekday)\n\n\n\n\n\n\n\n\n\n\n\nThe code chunk below uses theme() to change the position of the legend.\n\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2) +\n  theme(legend.position = 'bottom')\n  \ng + facet_edges(~Weekday)\n\n\n\n\n\n\n\n\n\n\n\nThe code chunk below adds frame to each graph.\n\nset_graph_style() \n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n  \ng + facet_edges(~Weekday) +\n  th_foreground(foreground = \"grey80\",  \n                border = TRUE) +\n  theme(legend.position = 'bottom')\n\n\n\n\n\n\n\n\n\n\n\nIn the code chunkc below, facet_nodes() is used. Before getting started, it is advisable for you to read it’s reference guide at least once.\n\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n  \ng + facet_nodes(~Department)+\n  th_foreground(foreground = \"grey80\",  \n                border = TRUE) +\n  theme(legend.position = 'bottom')"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands_on_ex8.html#network-metrics-analysis",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands_on_ex8.html#network-metrics-analysis",
    "title": "Hand-on exercise 8 Visual Analytics",
    "section": "",
    "text": "Centrality measures are a collection of statistical indices use to describe the relative important of the actors are to a network. There are four well-known centrality measures, namely: degree, betweenness, closeness and eigenvector. It is beyond the scope of this hands-on exercise to cover the principles and mathematics of these measure here. Students are encouraged to refer to Chapter 7: Actor Prominence of A User’s Guide to Network Analysis in R to gain better understanding of theses network measures.\n\ng &lt;- GAStech_graph %&gt;%\n  mutate(betweenness_centrality = centrality_betweenness()) %&gt;%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department,\n            size=betweenness_centrality))\ng + theme_graph()\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above:\n\nmutate() of dplyr is used to perform the computation.\nthe algorithm used, on the other hand, is the centrality_betweenness() of tidygraph.\n\n\n\n\nIt is important to note that from ggraph v2.0 onward tidygraph algorithms such as centrality measures can be accessed directly in ggraph calls. This means that it is no longer necessary to precompute and store derived node and edge centrality measures on the graph in order to use them in a plot.\n\ng &lt;- GAStech_graph %&gt;%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department, \n                      size = centrality_betweenness()))\ng + theme_graph()\n\n\n\n\n\n\n\n\n\n\n\ntidygraph package inherits many of the community detection algorithms imbedded into igraph and makes them available to us, including Edge-betweenness (group_edge_betweenness), Leading eigenvector (group_leading_eigen), Fast-greedy (group_fast_greedy), Louvain (group_louvain), Walktrap (group_walktrap), Label propagation (group_label_prop), InfoMAP (group_infomap), Spinglass (group_spinglass), and Optimal (group_optimal). Some community algorithms are designed to take into account direction or weight, while others ignore it. Use this link to find out more about community detection functions provided by tidygraph,\nIn the code chunk below group_edge_betweenness() is used.\n\ng &lt;- GAStech_graph %&gt;%\n  mutate(community = as.factor(group_edge_betweenness(weights = Weight, directed = TRUE))) %&gt;%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = community))  \n\ng + theme_graph()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands_on_ex8.html#building-interactive-network-graph-with-visnetwork",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands_on_ex8.html#building-interactive-network-graph-with-visnetwork",
    "title": "Hand-on exercise 8 Visual Analytics",
    "section": "",
    "text": "visNetwork() is a R package for network visualization, using vis.js javascript library.\nvisNetwork() function uses a nodes list and edges list to create an interactive graph.\n\nThe nodes list must include an “id” column, and the edge list must have “from” and “to” columns.\nThe function also plots the labels for the nodes, using the names of the actors from the “label” column in the node list.\n\nThe resulting graph is fun to play around with.\n\nYou can move the nodes and the graph will use an algorithm to keep the nodes properly spaced.\nYou can also zoom in and out on the plot and move it around to re-center it.\n\n\n\n\nBefore we can plot the interactive network graph, we need to prepare the data model by using the code chunk below.\n\nGAStech_edges_aggregated &lt;- GAStech_edges %&gt;%\n  left_join(GAStech_nodes, by = c(\"sourceLabel\" = \"label\")) %&gt;%\n  rename(from = id) %&gt;%\n  left_join(GAStech_nodes, by = c(\"targetLabel\" = \"label\")) %&gt;%\n  rename(to = id) %&gt;%\n  filter(MainSubject == \"Work related\") %&gt;%\n  group_by(from, to) %&gt;%\n    summarise(weight = n()) %&gt;%\n  filter(from!=to) %&gt;%\n  filter(weight &gt; 1) %&gt;%\n  ungroup()\n\n\n\n\nThe code chunk below will be used to plot an interactive network graph by using the data prepared.\n\nvisNetwork(GAStech_nodes, \n           GAStech_edges_aggregated)\n\n\n\n\n\n\n\n\nIn the code chunk below, Fruchterman and Reingold layout is used.\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") \n\n\n\n\n\nVisit Igraph to find out more about visIgraphLayout’s argument.\n\n\n\nvisNetwork() looks for a field called “group” in the nodes object and colour the nodes according to the values of the group field.\nThe code chunk below rename Department field to group.\n\nGAStech_nodes &lt;- GAStech_nodes %&gt;%\n  rename(group = Department) \n\nWhen we rerun the code chunk below, visNetwork shades the nodes by assigning unique colour to each category in the group field.\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\n\n\nIn the code run below visEdges() is used to symbolise the edges.\n- The argument arrows is used to define where to place the arrow.\n- The smooth argument is used to plot the edges using a smooth curve.\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") %&gt;%\n  visEdges(arrows = \"to\", \n           smooth = list(enabled = TRUE, \n                         type = \"curvedCW\")) %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 123)\n\n\n\n\n\nVisit Option to find out more about visEdges’s argument.\n\n\n\nIn the code chunk below, visOptions() is used to incorporate interactivity features in the data visualisation.\n\nThe argument highlightNearest highlights nearest when clicking a node.\nThe argument nodesIdSelection adds an id node selection creating an HTML select element.\n\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") %&gt;%\n  visOptions(highlightNearest = TRUE,\n             nodesIdSelection = TRUE) %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 123)\n\n\n\n\n\nVisit Option to find out more about visOption’s argument."
  },
  {
    "objectID": "Take-home_Ex/Take-home_ex04/Take-home_ex4.html#check-structure-with-glimpse",
    "href": "Take-home_Ex/Take-home_ex04/Take-home_ex4.html#check-structure-with-glimpse",
    "title": "Take-home exercise 4 Visual Analytics",
    "section": "4.1 Check structure with glimpse()",
    "text": "4.1 Check structure with glimpse()\n\nglimpse(raw_weather_data)\n\nRows: 204,464\nColumns: 15\n$ Station                       &lt;chr&gt; \"Paya Lebar\", \"Paya Lebar\", \"Paya Lebar\"…\n$ Year                          &lt;int&gt; 2014, 2014, 2014, 2014, 2014, 2014, 2014…\n$ Month                         &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ Day                           &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1…\n$ Daily.Rainfall.Total..mm.     &lt;chr&gt; \"0\", \"0\", \"2.2\", \"0.6\", \"10.5\", \"31.2\", …\n$ Highest.30.min.Rainfall..mm.  &lt;chr&gt; \"�\", \"�\", \"�\", \"�\", \"�\", \"�\", \"�\", \"�\", …\n$ Highest.60.min.Rainfall..mm.  &lt;chr&gt; \"�\", \"�\", \"�\", \"�\", \"�\", \"�\", \"�\", \"�\", …\n$ Highest.120.min.Rainfall..mm. &lt;chr&gt; \"�\", \"�\", \"�\", \"�\", \"�\", \"�\", \"�\", \"�\", …\n$ Mean.Temperature...C.         &lt;chr&gt; \"�\", \"�\", \"�\", \"�\", \"�\", \"�\", \"�\", \"�\", …\n$ Maximum.Temperature...C.      &lt;chr&gt; \"29.5\", \"31.7\", \"31.1\", \"32.3\", \"27\", \"2…\n$ Minimum.Temperature...C.      &lt;chr&gt; \"24.8\", \"25\", \"25.1\", \"23.7\", \"23.8\", \"2…\n$ Mean.Wind.Speed..km.h.        &lt;chr&gt; \"15.8\", \"16.5\", \"14.9\", \"8.9\", \"11.9\", \"…\n$ Max.Wind.Speed..km.h.         &lt;chr&gt; \"35.3\", \"37.1\", \"33.5\", \"35.3\", \"33.5\", …\n$ Latitude                      &lt;dbl&gt; 1.3524, 1.3524, 1.3524, 1.3524, 1.3524, …\n$ Longitude                     &lt;dbl&gt; 103.9007, 103.9007, 103.9007, 103.9007, …\n\n\nThere are 204464 rows, and 15 columns in the dataset. We see that there are missing values shown as ‘?’ in the dataset. In the next few steps, we will drop specific columns and rows based on the project focus."
  },
  {
    "objectID": "Take-home_Ex/Take-home_ex04/Take-home_ex4.html#drop-unused-columns",
    "href": "Take-home_Ex/Take-home_ex04/Take-home_ex4.html#drop-unused-columns",
    "title": "Take-home exercise 4 Visual Analytics",
    "section": "4.2 Drop unused columns",
    "text": "4.2 Drop unused columns\nWe will not be using all 15 columns for this project. The following columns will be dropped:\n\nHighest 30 Min Rainfall (mm)\nHighest 60 Min Rainfall (mm)\nHighest 1200 Min Rainfall (mm)\nMean Wind Speed (km/h)\nMax Wind Speed (km/h)\n\n\nraw_weather_data &lt;- raw_weather_data %&gt;%\n  select(-c(`Highest.30.min.Rainfall..mm.`, \n            `Highest.60.min.Rainfall..mm.`, \n            `Highest.120.min.Rainfall..mm.`,\n            `Mean.Wind.Speed..km.h.`,\n            `Max.Wind.Speed..km.h.`))"
  },
  {
    "objectID": "Take-home_Ex/Take-home_ex04/Take-home_ex4.html#remove-rows-for-specific-stations",
    "href": "Take-home_Ex/Take-home_ex04/Take-home_ex4.html#remove-rows-for-specific-stations",
    "title": "Take-home exercise 4 Visual Analytics",
    "section": "4.3 Remove rows for specific Stations",
    "text": "4.3 Remove rows for specific Stations\nThe Meteorological Service Singapore also provides a file, Station Records that has some information on the availability of data for each station. After examining the station records file, we found that 41 stations had missing information for some variables. We will hence drop rows for these stations.\n\n# Drop rows of 41 stations\n# Define the station names to remove\nstations_to_remove &lt;- c(\"Macritchie Reservoir\", \"Lower Peirce Reservoir\", \"Pasir Ris (West)\", \"Kampong Bahru\", \"Jurong Pier\", \"Ulu Pandan\", \"Serangoon\", \"Jurong (East)\", \"Mandai\", \"Upper Thomson\", \"Buangkok\", \"Boon Lay (West)\", \"Bukit Panjang\", \"Kranji Reservoir\", \"Tanjong Pagar\", \"Admiralty West\", \"Queenstown\", \"Tanjong Katong\", \"Chai Chee\", \"Upper Peirce Reservoir\", \"Kent Ridge\", \"Somerset (Road)\", \"Punggol\", \"Tuas West\", \"Simei\", \"Toa Payoh\", \"Tuas\", \"Bukit Timah\", \"Yishun\", \"Buona Vista\", \"Pasir Ris (Central)\", \"Jurong (North)\", \"Choa Chu Kang (West)\", \"Serangoon North\", \"Lim Chu Kang\", \"Marine Parade\", \"Choa Chu Kang (Central)\", \"Dhoby Ghaut\", \"Nicoll Highway\", \"Botanic Garden\", \"Whampoa\")\n\n# Remove rows with the specified station names\nraw_weather_data &lt;- raw_weather_data[!raw_weather_data$Station %in% stations_to_remove, ]\n\n# Print the number of stations left\nprint(sprintf(\" %d stations removed. %d stations left.\", length(stations_to_remove), n_distinct(raw_weather_data$Station)))\n\n[1] \" 41 stations removed. 22 stations left.\""
  },
  {
    "objectID": "Take-home_Ex/Take-home_ex04/Take-home_ex4.html#check-for-duplicates",
    "href": "Take-home_Ex/Take-home_ex04/Take-home_ex4.html#check-for-duplicates",
    "title": "Take-home exercise 4 Visual Analytics",
    "section": "4.4 Check for duplicates",
    "text": "4.4 Check for duplicates\n\n# Identify duplicates\nduplicates &lt;- raw_weather_data[duplicated(raw_weather_data[c(\"Station\", \"Year\", \"Month\", \"Day\")]) | duplicated(raw_weather_data[c(\"Station\", \"Year\", \"Month\", \"Day\")], fromLast = TRUE), ]\n\n# Check if 'duplicates' dataframe is empty\nif (nrow(duplicates) == 0) {\n  print(\"The combination of Station Name, Year, Month, and Day is unique.\")\n} else {\n  print(\"There are duplicates in the combination of Station Name, Year, Month, and Day. Showing duplicated rows:\")\n  print(duplicates)\n}\n\n[1] \"There are duplicates in the combination of Station Name, Year, Month, and Day. Showing duplicated rows:\"\n               Station Year Month Day Daily.Rainfall.Total..mm.\n13381   Semakau Island   NA    NA  NA                         -\n13382   Semakau Island   NA    NA  NA                         -\n13383   Semakau Island   NA    NA  NA                         -\n13384   Semakau Island   NA    NA  NA                         -\n13385   Semakau Island   NA    NA  NA                         -\n13386   Semakau Island   NA    NA  NA                         -\n13387   Semakau Island   NA    NA  NA                         -\n13388   Semakau Island   NA    NA  NA                         -\n13389   Semakau Island   NA    NA  NA                         -\n13390   Semakau Island   NA    NA  NA                         -\n13391   Semakau Island   NA    NA  NA                         -\n13392   Semakau Island   NA    NA  NA                         -\n13393   Semakau Island   NA    NA  NA                         -\n13394   Semakau Island   NA    NA  NA                         -\n13395   Semakau Island   NA    NA  NA                         -\n13396   Semakau Island   NA    NA  NA                         -\n13397   Semakau Island   NA    NA  NA                         -\n13398   Semakau Island   NA    NA  NA                         -\n13399   Semakau Island   NA    NA  NA                         -\n13400   Semakau Island   NA    NA  NA                         -\n13401   Semakau Island   NA    NA  NA                         -\n13402   Semakau Island   NA    NA  NA                         -\n13403   Semakau Island   NA    NA  NA                         -\n13404   Semakau Island   NA    NA  NA                         -\n13405   Semakau Island   NA    NA  NA                         -\n13406   Semakau Island   NA    NA  NA                         -\n13407   Semakau Island   NA    NA  NA                         -\n13408   Semakau Island   NA    NA  NA                         -\n13409   Semakau Island   NA    NA  NA                         -\n13410   Semakau Island   NA    NA  NA                         -\n13411   Semakau Island   NA    NA  NA                         -\n13412   Semakau Island   NA    NA  NA                         -\n13413   Semakau Island   NA    NA  NA                         -\n13414   Semakau Island   NA    NA  NA                         -\n13415   Semakau Island   NA    NA  NA                         -\n13416   Semakau Island   NA    NA  NA                         -\n13417   Semakau Island   NA    NA  NA                         -\n13418   Semakau Island   NA    NA  NA                         -\n13419   Semakau Island   NA    NA  NA                         -\n13420   Semakau Island   NA    NA  NA                         -\n13421   Semakau Island   NA    NA  NA                         -\n13422   Semakau Island   NA    NA  NA                         -\n13423   Semakau Island   NA    NA  NA                         -\n13424   Semakau Island   NA    NA  NA                         -\n13425   Semakau Island   NA    NA  NA                         -\n13426   Semakau Island   NA    NA  NA                         -\n13427   Semakau Island   NA    NA  NA                         -\n13428   Semakau Island   NA    NA  NA                         -\n13429   Semakau Island   NA    NA  NA                         -\n13430   Semakau Island   NA    NA  NA                         -\n13431   Semakau Island   NA    NA  NA                         -\n13432   Semakau Island   NA    NA  NA                         -\n13433   Semakau Island   NA    NA  NA                         -\n13434   Semakau Island   NA    NA  NA                         -\n13435   Semakau Island   NA    NA  NA                         -\n13436   Semakau Island   NA    NA  NA                         -\n13437   Semakau Island   NA    NA  NA                         -\n13438   Semakau Island   NA    NA  NA                         -\n13439   Semakau Island   NA    NA  NA                         -\n13440   Semakau Island   NA    NA  NA                         -\n13441   Semakau Island   NA    NA  NA                         -\n14084   Semakau Island   NA    NA  NA                         -\n14085   Semakau Island   NA    NA  NA                         -\n14086   Semakau Island   NA    NA  NA                         -\n14087   Semakau Island   NA    NA  NA                         -\n14088   Semakau Island   NA    NA  NA                         -\n183829 Boon Lay (East)   NA    NA  NA                         -\n183830 Boon Lay (East)   NA    NA  NA                         -\n183831 Boon Lay (East)   NA    NA  NA                         -\n183832 Boon Lay (East)   NA    NA  NA                         -\n183833 Boon Lay (East)   NA    NA  NA                         -\n183834 Boon Lay (East)   NA    NA  NA                         -\n183835 Boon Lay (East)   NA    NA  NA                         -\n183836 Boon Lay (East)   NA    NA  NA                         -\n183837 Boon Lay (East)   NA    NA  NA                         -\n183838 Boon Lay (East)   NA    NA  NA                         -\n183839 Boon Lay (East)   NA    NA  NA                         -\n183840 Boon Lay (East)   NA    NA  NA                         -\n183841 Boon Lay (East)   NA    NA  NA                         -\n183842 Boon Lay (East)   NA    NA  NA                         -\n183843 Boon Lay (East)   NA    NA  NA                         -\n183844 Boon Lay (East)   NA    NA  NA                         -\n183845 Boon Lay (East)   NA    NA  NA                         -\n183846 Boon Lay (East)   NA    NA  NA                         -\n183847 Boon Lay (East)   NA    NA  NA                         -\n183848 Boon Lay (East)   NA    NA  NA                         -\n183849 Boon Lay (East)   NA    NA  NA                         -\n183850 Boon Lay (East)   NA    NA  NA                         -\n183851 Boon Lay (East)   NA    NA  NA                         -\n183852 Boon Lay (East)   NA    NA  NA                         -\n183853 Boon Lay (East)   NA    NA  NA                         -\n183854 Boon Lay (East)   NA    NA  NA                         -\n183855 Boon Lay (East)   NA    NA  NA                         -\n183856 Boon Lay (East)   NA    NA  NA                         -\n183857 Boon Lay (East)   NA    NA  NA                         -\n183858 Boon Lay (East)   NA    NA  NA                         -\n191071 Boon Lay (East)   NA    NA  NA                         -\n191072 Boon Lay (East)   NA    NA  NA                         -\n191073 Boon Lay (East)   NA    NA  NA                         -\n191074 Boon Lay (East)   NA    NA  NA                         -\n191075 Boon Lay (East)   NA    NA  NA                         -\n191076 Boon Lay (East)   NA    NA  NA                         -\n191077 Boon Lay (East)   NA    NA  NA                         -\n191078 Boon Lay (East)   NA    NA  NA                         -\n191079 Boon Lay (East)   NA    NA  NA                         -\n191080 Boon Lay (East)   NA    NA  NA                         -\n191081 Boon Lay (East)   NA    NA  NA                         -\n191082 Boon Lay (East)   NA    NA  NA                         -\n191083 Boon Lay (East)   NA    NA  NA                         -\n191084 Boon Lay (East)   NA    NA  NA                         -\n191085 Boon Lay (East)   NA    NA  NA                         -\n191086 Boon Lay (East)   NA    NA  NA                         -\n191087 Boon Lay (East)   NA    NA  NA                         -\n191088 Boon Lay (East)   NA    NA  NA                         -\n191089 Boon Lay (East)   NA    NA  NA                         -\n191090 Boon Lay (East)   NA    NA  NA                         -\n191091 Boon Lay (East)   NA    NA  NA                         -\n191092 Boon Lay (East)   NA    NA  NA                         -\n191093 Boon Lay (East)   NA    NA  NA                         -\n191094 Boon Lay (East)   NA    NA  NA                         -\n191095 Boon Lay (East)   NA    NA  NA                         -\n191096 Boon Lay (East)   NA    NA  NA                         -\n191097 Boon Lay (East)   NA    NA  NA                         -\n191098 Boon Lay (East)   NA    NA  NA                         -\n191099 Boon Lay (East)   NA    NA  NA                         -\n191100 Boon Lay (East)   NA    NA  NA                         -\n191101 Boon Lay (East)   NA    NA  NA                         -\n       Mean.Temperature...C. Maximum.Temperature...C. Minimum.Temperature...C.\n13381                      -                        -                        -\n13382                      -                        -                        -\n13383                      -                        -                        -\n13384                      -                        -                        -\n13385                      -                        -                        -\n13386                      -                        -                        -\n13387                      -                        -                        -\n13388                      -                        -                        -\n13389                      -                        -                        -\n13390                      -                        -                        -\n13391                      -                        -                        -\n13392                      -                        -                        -\n13393                      -                        -                        -\n13394                      -                        -                        -\n13395                      -                        -                        -\n13396                      -                        -                        -\n13397                      -                        -                        -\n13398                      -                        -                        -\n13399                      -                        -                        -\n13400                      -                        -                        -\n13401                      -                        -                        -\n13402                      -                        -                        -\n13403                      -                        -                        -\n13404                      -                        -                        -\n13405                      -                        -                        -\n13406                      -                        -                        -\n13407                      -                        -                        -\n13408                      -                        -                        -\n13409                      -                        -                        -\n13410                      -                        -                        -\n13411                      -                        -                        -\n13412                      -                        -                        -\n13413                      -                        -                        -\n13414                      -                        -                        -\n13415                      -                        -                        -\n13416                      -                        -                        -\n13417                      -                        -                        -\n13418                      -                        -                        -\n13419                      -                        -                        -\n13420                      -                        -                        -\n13421                      -                        -                        -\n13422                      -                        -                        -\n13423                      -                        -                        -\n13424                      -                        -                        -\n13425                      -                        -                        -\n13426                      -                        -                        -\n13427                      -                        -                        -\n13428                      -                        -                        -\n13429                      -                        -                        -\n13430                      -                        -                        -\n13431                      -                        -                        -\n13432                      -                        -                        -\n13433                      -                        -                        -\n13434                      -                        -                        -\n13435                      -                        -                        -\n13436                      -                        -                        -\n13437                      -                        -                        -\n13438                      -                        -                        -\n13439                      -                        -                        -\n13440                      -                        -                        -\n13441                      -                        -                        -\n14084                      -                        -                        -\n14085                      -                        -                        -\n14086                      -                        -                        -\n14087                      -                        -                        -\n14088                      -                        -                        -\n183829                     -                        -                        -\n183830                     -                        -                        -\n183831                     -                        -                        -\n183832                     -                        -                        -\n183833                     -                        -                        -\n183834                     -                        -                        -\n183835                     -                        -                        -\n183836                     -                        -                        -\n183837                     -                        -                        -\n183838                     -                        -                        -\n183839                     -                        -                        -\n183840                     -                        -                        -\n183841                     -                        -                        -\n183842                     -                        -                        -\n183843                     -                        -                        -\n183844                     -                        -                        -\n183845                     -                        -                        -\n183846                     -                        -                        -\n183847                     -                        -                        -\n183848                     -                        -                        -\n183849                     -                        -                        -\n183850                     -                        -                        -\n183851                     -                        -                        -\n183852                     -                        -                        -\n183853                     -                        -                        -\n183854                     -                        -                        -\n183855                     -                        -                        -\n183856                     -                        -                        -\n183857                     -                        -                        -\n183858                     -                        -                        -\n191071                     -                        -                        -\n191072                     -                        -                        -\n191073                     -                        -                        -\n191074                     -                        -                        -\n191075                     -                        -                        -\n191076                     -                        -                        -\n191077                     -                        -                        -\n191078                     -                        -                        -\n191079                     -                        -                        -\n191080                     -                        -                        -\n191081                     -                        -                        -\n191082                     -                        -                        -\n191083                     -                        -                        -\n191084                     -                        -                        -\n191085                     -                        -                        -\n191086                     -                        -                        -\n191087                     -                        -                        -\n191088                     -                        -                        -\n191089                     -                        -                        -\n191090                     -                        -                        -\n191091                     -                        -                        -\n191092                     -                        -                        -\n191093                     -                        -                        -\n191094                     -                        -                        -\n191095                     -                        -                        -\n191096                     -                        -                        -\n191097                     -                        -                        -\n191098                     -                        -                        -\n191099                     -                        -                        -\n191100                     -                        -                        -\n191101                     -                        -                        -\n       Latitude Longitude\n13381    1.1890  103.7680\n13382    1.1890  103.7680\n13383    1.1890  103.7680\n13384    1.1890  103.7680\n13385    1.1890  103.7680\n13386    1.1890  103.7680\n13387    1.1890  103.7680\n13388    1.1890  103.7680\n13389    1.1890  103.7680\n13390    1.1890  103.7680\n13391    1.1890  103.7680\n13392    1.1890  103.7680\n13393    1.1890  103.7680\n13394    1.1890  103.7680\n13395    1.1890  103.7680\n13396    1.1890  103.7680\n13397    1.1890  103.7680\n13398    1.1890  103.7680\n13399    1.1890  103.7680\n13400    1.1890  103.7680\n13401    1.1890  103.7680\n13402    1.1890  103.7680\n13403    1.1890  103.7680\n13404    1.1890  103.7680\n13405    1.1890  103.7680\n13406    1.1890  103.7680\n13407    1.1890  103.7680\n13408    1.1890  103.7680\n13409    1.1890  103.7680\n13410    1.1890  103.7680\n13411    1.1890  103.7680\n13412    1.1890  103.7680\n13413    1.1890  103.7680\n13414    1.1890  103.7680\n13415    1.1890  103.7680\n13416    1.1890  103.7680\n13417    1.1890  103.7680\n13418    1.1890  103.7680\n13419    1.1890  103.7680\n13420    1.1890  103.7680\n13421    1.1890  103.7680\n13422    1.1890  103.7680\n13423    1.1890  103.7680\n13424    1.1890  103.7680\n13425    1.1890  103.7680\n13426    1.1890  103.7680\n13427    1.1890  103.7680\n13428    1.1890  103.7680\n13429    1.1890  103.7680\n13430    1.1890  103.7680\n13431    1.1890  103.7680\n13432    1.1890  103.7680\n13433    1.1890  103.7680\n13434    1.1890  103.7680\n13435    1.1890  103.7680\n13436    1.1890  103.7680\n13437    1.1890  103.7680\n13438    1.1890  103.7680\n13439    1.1890  103.7680\n13440    1.1890  103.7680\n13441    1.1890  103.7680\n14084    1.1890  103.7680\n14085    1.1890  103.7680\n14086    1.1890  103.7680\n14087    1.1890  103.7680\n14088    1.1890  103.7680\n183829   1.3302  103.7205\n183830   1.3302  103.7205\n183831   1.3302  103.7205\n183832   1.3302  103.7205\n183833   1.3302  103.7205\n183834   1.3302  103.7205\n183835   1.3302  103.7205\n183836   1.3302  103.7205\n183837   1.3302  103.7205\n183838   1.3302  103.7205\n183839   1.3302  103.7205\n183840   1.3302  103.7205\n183841   1.3302  103.7205\n183842   1.3302  103.7205\n183843   1.3302  103.7205\n183844   1.3302  103.7205\n183845   1.3302  103.7205\n183846   1.3302  103.7205\n183847   1.3302  103.7205\n183848   1.3302  103.7205\n183849   1.3302  103.7205\n183850   1.3302  103.7205\n183851   1.3302  103.7205\n183852   1.3302  103.7205\n183853   1.3302  103.7205\n183854   1.3302  103.7205\n183855   1.3302  103.7205\n183856   1.3302  103.7205\n183857   1.3302  103.7205\n183858   1.3302  103.7205\n191071   1.3302  103.7205\n191072   1.3302  103.7205\n191073   1.3302  103.7205\n191074   1.3302  103.7205\n191075   1.3302  103.7205\n191076   1.3302  103.7205\n191077   1.3302  103.7205\n191078   1.3302  103.7205\n191079   1.3302  103.7205\n191080   1.3302  103.7205\n191081   1.3302  103.7205\n191082   1.3302  103.7205\n191083   1.3302  103.7205\n191084   1.3302  103.7205\n191085   1.3302  103.7205\n191086   1.3302  103.7205\n191087   1.3302  103.7205\n191088   1.3302  103.7205\n191089   1.3302  103.7205\n191090   1.3302  103.7205\n191091   1.3302  103.7205\n191092   1.3302  103.7205\n191093   1.3302  103.7205\n191094   1.3302  103.7205\n191095   1.3302  103.7205\n191096   1.3302  103.7205\n191097   1.3302  103.7205\n191098   1.3302  103.7205\n191099   1.3302  103.7205\n191100   1.3302  103.7205\n191101   1.3302  103.7205"
  },
  {
    "objectID": "Take-home_Ex/Take-home_ex04/Take-home_ex4.html#check-and-handle-missing-values",
    "href": "Take-home_Ex/Take-home_ex04/Take-home_ex4.html#check-and-handle-missing-values",
    "title": "Take-home exercise 4 Visual Analytics",
    "section": "4.5 Check and handle missing values",
    "text": "4.5 Check and handle missing values\n\n4.5.1 First check for missing values\nMissing values in this dataset can be represented by:\n\n\\u0097\nNA\n-\n\nWe first replace these values with actual NA values:\n\nraw_weather_data &lt;- raw_weather_data %&gt;%\n  mutate(across(where(is.character), ~na_if(.x, \"\\u0097\"))) %&gt;%\n  mutate(across(where(is.character), ~na_if(.x, \"NA\"))) %&gt;%\n  mutate(across(where(is.character), ~na_if(.x, \"-\")))\n\nNext, we visualize the missing values in the dataset:\n\n# For a simple missing data plot\ngg_miss_var(raw_weather_data)\n\n\n\n\n\n\n\n\nWe can see there is quite a number of missing data in the Mean Temperature, Minimum Temperature, Maximum Temperature and Daily Rainfall Total. We will take steps to handle the missing data.\n\n\n4.5.2 Remove Stations with significant missing data\nWe have identified two checks to make:\n\nCheck which stations have no recorded data for entire months.\nCheck which stations have more than 7 consecutive days of missing data\n\nFor both these checks, we will remove the entire station from the dataset as it would not be practical to impute such large amounts of missing values.\n\n4.5.3 Identify and remove Stations with no recorded data for entire months\nSome stations have no recorded data for entire months, as summarised in the table below:\n\n# Create complete combination of Station, Year, and Month\nall_combinations &lt;- expand.grid(\n  Station = unique(raw_weather_data$Station),\n  Year = 2021:2023,\n  Month = 1:12\n)\n\n# Left join this with the original weather data to identify missing entries\nmissing_months &lt;- all_combinations %&gt;%\n  left_join(raw_weather_data, by = c(\"Station\", \"Year\", \"Month\")) %&gt;%\n  # Use is.na() to check for rows that didn't have a match in the original data\n  filter(is.na(Day)) %&gt;%\n  # Select only the relevant columns for the final output\n  select(Station, Year, Month)\n\n# Create a summary table that lists out the missing months\nmissing_months_summary &lt;- missing_months %&gt;%\n  group_by(Station, Year) %&gt;%\n  summarise(MissingMonths = toString(sort(unique(Month))), .groups = 'drop')\n\nkable(missing_months_summary)\n\n\n\n\nStation\nYear\nMissingMonths\n\n\n\n\nBoon Lay (East)\n2021\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12\n\n\nBoon Lay (East)\n2022\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12\n\n\nBoon Lay (East)\n2023\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12\n\n\nKhatib\n2022\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12\n\n\nKhatib\n2023\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12\n\n\n\n\n\nWe hence drop these stations from our dataset:\n\nraw_weather_data &lt;- anti_join(raw_weather_data, missing_months, by = \"Station\")\n\nprint(sprintf(\"The folowing %d stations were dropped: %s\", n_distinct(missing_months$Station), paste(unique(missing_months$Station), collapse = \", \")))\n\n[1] \"The folowing 2 stations were dropped: Boon Lay (East), Khatib\"\n\n\n\nprint(sprintf(\"There are %d stations left: \", n_distinct(raw_weather_data$Station)))\n\n[1] \"There are 20 stations left: \"\n\n\n\nkable(unique(raw_weather_data$Station),\n      row.names = TRUE,\n      col.names = \"Station\",\n      caption = \"List of Remaining Stations\")\n\n\nList of Remaining Stations\n\n\n\nStation\n\n\n\n\n1\nPaya Lebar\n\n\n2\nSemakau Island\n\n\n3\nAdmiralty\n\n\n4\nPulau Ubin\n\n\n5\nEast Coast Parkway\n\n\n6\nMarina Barrage\n\n\n7\nAng Mo Kio\n\n\n8\nNewton\n\n\n9\nJurong Island\n\n\n10\nTuas South\n\n\n11\nPasir Panjang\n\n\n12\nChoa Chu Kang (South)\n\n\n13\nTengah\n\n\n14\nChangi\n\n\n15\nSeletar\n\n\n16\nTai Seng\n\n\n17\nJurong (West)\n\n\n18\nClementi\n\n\n19\nSentosa Island\n\n\n20\nSembawang\n\n\n\n\n\n\n\n4.5.4 Identify and remove Stations with excessive missing values\nIf there are any missing values, we can try to impute these missing values. However, if there are 7 or more consecutive values missing, we will remove these stations first.\n\n# Define a helper function to count the number of 7 or more consecutive NAs\ncount_seven_consecutive_NAs &lt;- function(x) {\n  na_runs &lt;- rle(is.na(x))\n  total_consecutive_NAs &lt;- sum(na_runs$lengths[na_runs$values & na_runs$lengths &gt;= 7])\n  return(total_consecutive_NAs)\n}\n\n# Apply the helper function to each relevant column within grouped data\nweather_summary &lt;- raw_weather_data %&gt;%\n  group_by(Station, Year, Month) %&gt;%\n  summarise(across(-Day, ~ count_seven_consecutive_NAs(.x), .names = \"count_consec_NAs_{.col}\"), .groups = \"drop\")\n\n# Filter to keep only rows where there is at least one column with 7 or more consecutive missing values\nweather_summary_with_consecutive_NAs &lt;- weather_summary %&gt;%\n  filter(if_any(starts_with(\"count_consec_NAs_\"), ~ . &gt; 0))\n\n# View the result\nprint(sprintf(\"There are %d stations with 7 or more consecutive missing values.\", n_distinct(weather_summary_with_consecutive_NAs$Station)))\n\n[1] \"There are 13 stations with 7 or more consecutive missing values.\"\n\n\n\n# kable(weather_summary_with_consecutive_NAs)\ndatatable(weather_summary_with_consecutive_NAs, \n            class= \"compact\",\n            rownames = FALSE,\n            width=\"100%\", \n            options = list(pageLength = 10, scrollX=T),\n          caption = 'Details of stations with &gt;=7 missing values')\n\n\n\n\n\nWe hence drop these stations from our dataset:\n\nraw_weather_data &lt;- anti_join(raw_weather_data, weather_summary_with_consecutive_NAs, by = \"Station\")\n\nprint(sprintf(\"The folowing %d stations were dropped: %s\", n_distinct(weather_summary_with_consecutive_NAs$Station), paste(unique(weather_summary_with_consecutive_NAs$Station), collapse = \", \")))\n\n[1] \"The folowing 13 stations were dropped: Admiralty, Ang Mo Kio, Clementi, Jurong Island, Marina Barrage, Paya Lebar, Pulau Ubin, Seletar, Semakau Island, Sembawang, Sentosa Island, Tengah, Tuas South\"\n\n\n\nprint(sprintf(\"There are %d stations left: \", n_distinct(raw_weather_data$Station)))\n\n[1] \"There are 7 stations left: \"\n\n\n\nkable(unique(raw_weather_data$Station),\n      row.names = TRUE,\n      col.names = \"Station\",\n      caption = \"List of Remaining Stations\")\n\n\nList of Remaining Stations\n\n\n\nStation\n\n\n\n\n1\nEast Coast Parkway\n\n\n2\nNewton\n\n\n3\nPasir Panjang\n\n\n4\nChoa Chu Kang (South)\n\n\n5\nChangi\n\n\n6\nTai Seng\n\n\n7\nJurong (West)\n\n\n\n\n\n\n\n\n4.5.5 Second check for missing values\nFrom the check below we see there are still missing values in our data. We will impute these values in the next step.\n\n# For a simple missing data plot\ngg_miss_var(raw_weather_data)\n\n\n\n\n\n\n\n\nWe can see that there is still a few missing values from the selected stations.\n\n\n4.6 Impute missing values\nTo handle the missing values for the remaining Stations, we will impute missing values using simple moving average from imputeTS package.\n\nraw_weather_data &lt;- raw_weather_data %&gt;%\n  mutate(Date = as.Date(paste(Year, Month, Day, sep = \"-\"))) %&gt;%\n  relocate(Date, .after = 1)\n\n\n# Define the weather variables to loop through\nweather_variables &lt;- c(\"Daily.Rainfall.Total..mm.\", \"Mean.Temperature...C.\", \"Maximum.Temperature...C.\", \"Minimum.Temperature...C.\")\n\n# Ensure raw_weather_data is correctly copied to a new data frame for imputation\nweather_data_imputed &lt;- raw_weather_data\n\n# Loop through each weather variable to impute missing values\nfor(variable in weather_variables) {\n  # Convert variable to numeric, ensuring that the conversion warnings are handled if necessary\n  weather_data_imputed[[variable]] &lt;- as.numeric(as.character(weather_data_imputed[[variable]]))\n  \n  # Impute missing values using a moving average\n  weather_data_imputed &lt;- weather_data_imputed %&gt;%\n    group_by(Station) %&gt;%\n    arrange(Station, Date) %&gt;%\n    mutate(\"{variable}\" := round(na_ma(.data[[variable]], k = 7, weighting = \"simple\"), 1)) %&gt;%\n    ungroup()\n}\n\n\n\n4.7 Add specific columns to data [NEW]\nThese columns are added as they may be used in plots later.\n\nweather_data_imputed &lt;- weather_data_imputed %&gt;% \n  mutate(Date_mine = make_date(2023, month(Date), day(Date)),\n         Month_Name = factor(months(Date), levels = month.name),\n         Week = isoweek(Date),\n         Weekday = wday(Date)\n  )"
  },
  {
    "objectID": "Take-home_Ex/Take-home_ex04/Take-home_ex4.html#summary-of-cleaned-data",
    "href": "Take-home_Ex/Take-home_ex04/Take-home_ex4.html#summary-of-cleaned-data",
    "title": "Take-home exercise 4 Visual Analytics",
    "section": "4.8 Summary of cleaned data",
    "text": "4.8 Summary of cleaned data\n\nDetails of stations and time period of data\n\ntime_period_start &lt;- min(weather_data_imputed$Date)\ntime_period_end &lt;- max(weather_data_imputed$Date)\ncat(\"\\nThe time period of the dataset is from\", format(time_period_start, \"%Y-%m-%d\"),\"to\", format(time_period_end, \"%Y-%m-%d\"), \"\\n\")\n\n\nThe time period of the dataset is from 2014-01-01 to 2024-01-31 \n\n\nbut i only want to keep until 2023, exclude record in 2024\n\nweather_data_imputed &lt;- subset(weather_data_imputed, Date &lt;= as.Date(\"2023-12-31\"))\n\ntime_period_start &lt;- min(weather_data_imputed$Date)\ntime_period_end &lt;- max(weather_data_imputed$Date)\ncat(\"\\nThe time period of the dataset is from\", format(time_period_start, \"%Y-%m-%d\"),\"to\", format(time_period_end, \"%Y-%m-%d\"), \"\\n\")\n\n\nThe time period of the dataset is from 2014-01-01 to 2023-12-31 \n\n\nAnd also to ease further analysis, convert year, month, day to factor data type:\n\nweather_data_imputed &lt;- weather_data_imputed %&gt;%\n  mutate_at(vars(Year,Month,Day),as.factor)\n\nglimpse(weather_data_imputed)\n\nRows: 25,258\nColumns: 15\n$ Station                   &lt;chr&gt; \"Changi\", \"Changi\", \"Changi\", \"Changi\", \"Cha…\n$ Date                      &lt;date&gt; 2014-01-01, 2014-01-02, 2014-01-03, 2014-01…\n$ Year                      &lt;fct&gt; 2014, 2014, 2014, 2014, 2014, 2014, 2014, 20…\n$ Month                     &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Day                       &lt;fct&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1…\n$ Daily.Rainfall.Total..mm. &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 18.4, 31.2, 0.0, 0.0, 2.…\n$ Mean.Temperature...C.     &lt;dbl&gt; 26.7, 27.4, 27.1, 27.1, 24.8, 25.3, 26.7, 27…\n$ Maximum.Temperature...C.  &lt;dbl&gt; 29.0, 30.9, 30.4, 31.1, 26.4, 27.1, 30.7, 31…\n$ Minimum.Temperature...C.  &lt;dbl&gt; 24.9, 25.0, 24.9, 24.9, 23.3, 23.9, 24.3, 24…\n$ Latitude                  &lt;dbl&gt; 1.3678, 1.3678, 1.3678, 1.3678, 1.3678, 1.36…\n$ Longitude                 &lt;dbl&gt; 103.9826, 103.9826, 103.9826, 103.9826, 103.…\n$ Date_mine                 &lt;date&gt; 2023-01-01, 2023-01-02, 2023-01-03, 2023-01…\n$ Month_Name                &lt;fct&gt; January, January, January, January, January,…\n$ Week                      &lt;dbl&gt; 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3,…\n$ Weekday                   &lt;dbl&gt; 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4,…\n\n\n\nkable(unique(weather_data_imputed$Station),\n      row.names = TRUE,\n      col.names = \"Station\",\n      caption = \"List of Stations\")\n\n\nList of Stations\n\n\n\nStation\n\n\n\n\n1\nChangi\n\n\n2\nChoa Chu Kang (South)\n\n\n3\nEast Coast Parkway\n\n\n4\nJurong (West)\n\n\n5\nNewton\n\n\n6\nPasir Panjang\n\n\n7\nTai Seng\n\n\n\n\n\n\n\nView dataset as interactive table\n\ndatatable(weather_data_imputed, \n            class= \"compact\",\n            rownames = FALSE,\n            width=\"100%\", \n            options = list(pageLength = 10, scrollX=T),\n          caption = 'Cleaned and imputed weather dataset')\n\n\n\n\n\n\ncolnames(weather_data_imputed)[6] &lt;- \"Daily_Rainfall_Total_mm\"\ncolnames(weather_data_imputed)[7] &lt;- \"Mean_Temperature\"\ncolnames(weather_data_imputed)[8] &lt;- \"Max_Temperature\"\ncolnames(weather_data_imputed)[9] &lt;- \"Min_Temperature\""
  },
  {
    "objectID": "In-class_Ex/in-class8/inclass8.html",
    "href": "In-class_Ex/in-class8/inclass8.html",
    "title": "In class exercise network",
    "section": "",
    "text": "Load packages:\n\npacman::p_load(igraph,tidygraph, ggraph, visNetwork,lubridate,clock,tidyverse,graphlayouts)\n\n\n\nImporting network data from files\n\nGAStech_nodes &lt;- read_csv(\"data/GAStech_email_node.csv\")\nGAStech_edges &lt;- read_csv(\"data/GAStech_email_edge-v2.csv\")\n\ncreate 2 more columns\n\nGAStech_edges &lt;- GAStech_edges %&gt;%\n  mutate(SendDate = dmy(SentDate)) %&gt;%\n  mutate(Weekday = wday(SentDate,\n                        label = TRUE,\n                        abbr = FALSE))\n\n\nWrangling attributes\nA close examination of GAStech_edges data.frame reveals that it consists of individual e-mail flow records. This is not very useful for visualisation.\nIn view of this, we will aggregate the individual by date, senders, receivers, main subject and day of the week.\n\nGAStech_edges_aggregated &lt;- GAStech_edges %&gt;%\n  filter(MainSubject == \"Work related\") %&gt;%\n  group_by(source, target, Weekday) %&gt;%\n    summarise(Weight = n()) %&gt;%\n  filter(source!=target) %&gt;%\n  filter(Weight &gt; 1) %&gt;%\n  ungroup()\n\n\n\nUsing tbl_graph() to build tidygraph data model.\nIn this section, you will use tbl_graph() of tinygraph package to build an tidygraph’s network graph data.frame.\nBefore typing the codes, you are recommended to review to reference guide of tbl_graph()\n\nGAStech_graph &lt;- tbl_graph(nodes = GAStech_nodes, edges = GAStech_edges_aggregated, directed = TRUE)\n\n\nGAStech_graph %&gt;%\n  activate(edges) %&gt;%\n  arrange(desc(Weight))\n\n# A tbl_graph: 54 nodes and 1372 edges\n#\n# A directed multigraph with 1 component\n#\n# Edge Data: 1,372 × 4 (active)\n    from    to Weekday   Weight\n   &lt;int&gt; &lt;int&gt; &lt;ord&gt;      &lt;int&gt;\n 1    40    41 Saturday      13\n 2    41    43 Monday        11\n 3    35    31 Tuesday       10\n 4    40    41 Monday        10\n 5    40    43 Monday        10\n 6    36    32 Sunday         9\n 7    40    43 Saturday       9\n 8    41    40 Monday         9\n 9    19    15 Wednesday      8\n10    35    38 Tuesday        8\n# ℹ 1,362 more rows\n#\n# Node Data: 54 × 4\n     id label           Department     Title           \n  &lt;dbl&gt; &lt;chr&gt;           &lt;chr&gt;          &lt;chr&gt;           \n1     1 Mat.Bramar      Administration Assistant to CEO\n2     2 Anda.Ribera     Administration Assistant to CFO\n3     3 Rachel.Pantanal Administration Assistant to CIO\n# ℹ 51 more rows\n\n\n\n\nPlotting a basic network graph\nThe code chunk below uses ggraph(), geom-edge_link() and geom_node_point() to plot a network graph by using GAStech_graph. Before your get started, it is advisable to read their respective reference guide at least once.\n\nggraph(GAStech_graph) +\n  geom_edge_link() +\n  geom_node_point()\n\n\n\n\n\n\n\n\n\n\nFruchterman and Reingold layout\nThe code chunks below will be used to plot the network graph using Fruchterman and Reingold layout.\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"fr\") +\n  geom_edge_link(aes()) +\n  geom_node_point(aes())\n\ng + theme_graph()\n\n\n\n\n\n\n\n\n\n\nModifying network nodes\nIn this section, you will colour each node by referring to their respective departments.\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes()) +\n  geom_node_point(aes(colour = Department, \n                      size = 3))\n\ng + theme_graph()\n\n\n\n\n\n\n\n\n\n\nModifying edges\nIn the code chunk below, the thickness of the edges will be mapped with the Weight variable.\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") +\n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 3)\n\ng + theme_graph()\n\n\n\n\n\n\n\n\n\n\nWorking with facet_edges()\nIn the code chunk below, facet_edges() is used. Before getting started, it is advisable for you to read it’s reference guide at least once.\n\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n\ng + facet_edges(~Weekday)\n\n\n\n\n\n\n\n\n\n\nA framed facet graph\nThe code chunk below adds frame to each graph.\n\nset_graph_style() \n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n  \ng + facet_edges(~Weekday) +\n  th_foreground(foreground = \"grey80\",  \n                border = TRUE) +\n  theme(legend.position = 'bottom')"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09/Hands_on_ex9.html",
    "href": "Hands-on_Ex/Hands-on_Ex09/Hands_on_ex9.html",
    "title": "Hand-on exercise 9 Visual Analytics",
    "section": "",
    "text": "By the end of this hands-on exercise, you will be able to:\n\ncreate bullet chart by using ggplot2,\ncreate sparklines by using ggplot2 ,\nbuild industry standard dashboard by using R Shiny.\n\n\n\n\nFor the purpose of this hands-on exercise, the following R packages will be used.\n\npacman::p_load(lubridate, ggthemes, reactable,\nreactablefmtr, gt, gtExtras, tidyverse,RODBC,svglite,dataui)\n\n\ntidyverse provides a collection of functions for performing data science task such as importing, tidying, wrangling data and visualising data. It is not a single package but a collection of modern R packages including but not limited to readr, tidyr, dplyr, ggplot, tibble, stringr, forcats and purrr.\nlubridate provides functions to work with dates and times more efficiently.\nggthemes is an extension of ggplot2. It provides additional themes beyond the basic themes of ggplot2.\ngtExtras provides some additional helper functions to assist in creating beautiful tables with gt, an R package specially designed for anyone to make wonderful-looking tables using the R programming language.\nreactable provides functions to create interactive data tables for R, based on the React Table library and made with reactR.\nreactablefmtr provides various features to streamline and enhance the styling of interactive reactable tables with easy-to-use and highly-customizable functions and themes.\n\n\n\n\n\n\nFor the purpose of this study, a personal database in Microsoft Access mdb format called Coffee Chain will be used.\n\n\n\nIn the code chunk below, odbcConnectAccess() of RODBC package is used used to import a database query table into R.\n\nlibrary(RODBC)\ncon &lt;- odbcConnectAccess2007('data/Coffee Chain.mdb')\ncoffeechain &lt;- sqlFetch(con, 'CoffeeChain Query')\nwrite_rds(coffeechain, \"data/CoffeeChain.rds\")\nodbcClose(con)\n\nNote: Before running the code chunk, you need to change the R system to 32bit version. This is because the odbcConnectAccess() is based on 32bit and not 64bit\n\n\n\nThe code chunk below is used to import CoffeeChain.rds into R.\n\ncoffeechain &lt;- read_rds(\"data/rds/CoffeeChain.rds\")\n\nNote: This step is optional if coffeechain is already available in R.\nThe code chunk below is used to aggregate Sales and Budgeted Sales at the Product level.\n\nproduct &lt;- coffeechain %&gt;%\n  group_by(`Product`) %&gt;%\n  summarise(`target` = sum(`Budget Sales`),\n            `current` = sum(`Sales`)) %&gt;%\n  ungroup()\n\n\n\n\nThe code chunk below is used to plot the bullet charts using ggplot2 functions.\n\nggplot(product, aes(Product, current)) + \n  geom_col(aes(Product, max(target) * 1.01),\n           fill=\"grey85\", width=0.85) +\n  geom_col(aes(Product, target * 0.75),\n           fill=\"grey60\", width=0.85) +\n  geom_col(aes(Product, target * 0.5),\n           fill=\"grey50\", width=0.85) +\n  geom_col(aes(Product, current), \n           width=0.35,\n           fill = \"black\") + \n  geom_errorbar(aes(y = target,\n                    x = Product, \n                    ymin = target,\n                    ymax= target), \n                width = .4,\n                colour = \"red\",\n                size = 1) +\n  coord_flip()\n\n\n\n\n\n\n\n\n\n\n\n\nIn this section, you will learn how to plot sparklines by using ggplot2.\n\n\n\nsales_report &lt;- coffeechain %&gt;%\n  filter(Date &gt;= \"2013-01-01\") %&gt;%\n  mutate(Month = month(Date)) %&gt;%\n  group_by(Month, Product) %&gt;%\n  summarise(Sales = sum(Sales)) %&gt;%\n  ungroup() %&gt;%\n  select(Month, Product, Sales)\n\nThe code chunk below is used to compute the minimum, maximum and end othe the month sales.\n\nmins &lt;- group_by(sales_report, Product) %&gt;% \n  slice(which.min(Sales))\nmaxs &lt;- group_by(sales_report, Product) %&gt;% \n  slice(which.max(Sales))\nends &lt;- group_by(sales_report, Product) %&gt;% \n  filter(Month == max(Month))\n\nThe code chunk below is used to compute the 25 and 75 quantiles.\n\nquarts &lt;- sales_report %&gt;%\n  group_by(Product) %&gt;%\n  summarise(quart1 = quantile(Sales, \n                              0.25),\n            quart2 = quantile(Sales, \n                              0.75)) %&gt;%\n  right_join(sales_report)\n\n\n\n\nThe code chunk used.\n\nggplot(sales_report, aes(x=Month, y=Sales)) + \n  facet_grid(Product ~ ., scales = \"free_y\") + \n  geom_ribbon(data = quarts, aes(ymin = quart1, max = quart2), \n              fill = 'grey90') +\n  geom_line(size=0.3) +\n  geom_point(data = mins, col = 'red') +\n  geom_point(data = maxs, col = 'blue') +\n  geom_text(data = mins, aes(label = Sales), vjust = -1) +\n  geom_text(data = maxs, aes(label = Sales), vjust = 2.5) +\n  geom_text(data = ends, aes(label = Sales), hjust = 0, nudge_x = 0.5) +\n  geom_text(data = ends, aes(label = Product), hjust = 0, nudge_x = 1.0) +\n  expand_limits(x = max(sales_report$Month) + \n                  (0.25 * (max(sales_report$Month) - min(sales_report$Month)))) +\n  scale_x_continuous(breaks = seq(1, 12, 1)) +\n  scale_y_continuous(expand = c(0.1, 0)) +\n  theme_tufte(base_size = 3, base_family = \"Helvetica\") +\n  theme(axis.title=element_blank(), axis.text.y = element_blank(), \n        axis.ticks = element_blank(), strip.text = element_blank())\n\n\n\n\n\n\n\n\n\n\n\n\nIn this section, you will learn how to create static information dashboard by using gt and gtExtras packages. Before getting started, it is highly recommended for you to visit the webpage of these two packages and review all the materials provided on the webpages at least once. You done not have to understand and remember everything provided but at least have an overview of the purposes and functions provided by them.\n\n\nIn this section, you will learn how to prepare a bullet chart report by using functions of gt and gtExtras packages.\n\nproduct %&gt;%\n  gt::gt() %&gt;%\n  gt_plt_bullet(column = current, \n              target = target, \n              width = 60,\n              palette = c(\"lightblue\", \n                          \"black\")) %&gt;%\n  gt_theme_538()\n\n\n\n\n\n\n\n\n\n\n\nProduct\ncurrent\n\n\n\n\nAmaretto\n\n\n\n   \n\n\n\nCaffe Latte\n\n\n\n   \n\n\n\nCaffe Mocha\n\n\n\n   \n\n\n\nChamomile\n\n\n\n   \n\n\n\nColombian\n\n\n\n   \n\n\n\nDarjeeling\n\n\n\n   \n\n\n\nDecaf Espresso\n\n\n\n   \n\n\n\nDecaf Irish Cream\n\n\n\n   \n\n\n\nEarl Grey\n\n\n\n   \n\n\n\nGreen Tea\n\n\n\n   \n\n\n\nLemon\n\n\n\n   \n\n\n\nMint\n\n\n\n   \n\n\n\nRegular Espresso\n\n\n\n   \n\n\n\n\n\n\n\n\n\n\n\n\nBefore we can prepare the sales report by product by using gtExtras functions, code chunk below will be used to prepare the data.\n\nreport &lt;- coffeechain %&gt;%\n  mutate(Year = year(Date)) %&gt;%\n  filter(Year == \"2013\") %&gt;%\n  mutate (Month = month(Date, \n                        label = TRUE, \n                        abbr = TRUE)) %&gt;%\n  group_by(Product, Month) %&gt;%\n  summarise(Sales = sum(Sales)) %&gt;%\n  ungroup()\n\nIt is important to note that one of the requirement of gtExtras functions is that almost exclusively they require you to pass data.frame with list columns. In view of this, code chunk below will be used to convert the report data.frame into list columns.\n\nreport %&gt;%\n  group_by(Product) %&gt;%\n  summarize('Monthly Sales' = list(Sales), \n            .groups = \"drop\")\n\n# A tibble: 13 × 2\n   Product           `Monthly Sales`\n   &lt;chr&gt;             &lt;list&gt;         \n 1 Amaretto          &lt;dbl [12]&gt;     \n 2 Caffe Latte       &lt;dbl [12]&gt;     \n 3 Caffe Mocha       &lt;dbl [12]&gt;     \n 4 Chamomile         &lt;dbl [12]&gt;     \n 5 Colombian         &lt;dbl [12]&gt;     \n 6 Darjeeling        &lt;dbl [12]&gt;     \n 7 Decaf Espresso    &lt;dbl [12]&gt;     \n 8 Decaf Irish Cream &lt;dbl [12]&gt;     \n 9 Earl Grey         &lt;dbl [12]&gt;     \n10 Green Tea         &lt;dbl [12]&gt;     \n11 Lemon             &lt;dbl [12]&gt;     \n12 Mint              &lt;dbl [12]&gt;     \n13 Regular Espresso  &lt;dbl [12]&gt;     \n\n\n\n\n\nreport %&gt;%\n  group_by(Product) %&gt;%\n  summarize('Monthly Sales' = list(Sales), \n            .groups = \"drop\") %&gt;%\n   gt() %&gt;%\n   gt_plt_sparkline('Monthly Sales',\n                    same_limit = FALSE)\n\n\n\n\n\n\n\n\n\n\n\nProduct\nMonthly Sales\n\n\n\n\nAmaretto\n\n\n\n   1.2K\n\n\n\nCaffe Latte\n\n\n\n   1.5K\n\n\n\nCaffe Mocha\n\n\n\n   3.7K\n\n\n\nChamomile\n\n\n\n   3.3K\n\n\n\nColombian\n\n\n\n   5.5K\n\n\n\nDarjeeling\n\n\n\n   3.0K\n\n\n\nDecaf Espresso\n\n\n\n   3.2K\n\n\n\nDecaf Irish Cream\n\n\n\n   2.7K\n\n\n\nEarl Grey\n\n\n\n   3.0K\n\n\n\nGreen Tea\n\n\n\n   1.5K\n\n\n\nLemon\n\n\n\n   4.4K\n\n\n\nMint\n\n\n\n   1.5K\n\n\n\nRegular Espresso\n\n\n\n   1.1K\n\n\n\n\n\n\n\n\n\n\n\nFirst, calculate summary statistics by using the code chunk below.\n\nreport %&gt;% \n  group_by(Product) %&gt;% \n  summarise(\"Min\" = min(Sales, na.rm = T),\n            \"Max\" = max(Sales, na.rm = T),\n            \"Average\" = mean(Sales, na.rm = T)\n            ) %&gt;%\n  gt() %&gt;%\n  fmt_number(columns = 4,\n    decimals = 2)\n\n\n\n\n\n\n\nProduct\nMin\nMax\nAverage\n\n\n\n\nAmaretto\n1016\n1210\n1,119.00\n\n\nCaffe Latte\n1398\n1653\n1,528.33\n\n\nCaffe Mocha\n3322\n3828\n3,613.92\n\n\nChamomile\n2967\n3395\n3,217.42\n\n\nColombian\n5132\n5961\n5,457.25\n\n\nDarjeeling\n2926\n3281\n3,112.67\n\n\nDecaf Espresso\n3181\n3493\n3,326.83\n\n\nDecaf Irish Cream\n2463\n2901\n2,648.25\n\n\nEarl Grey\n2730\n3005\n2,841.83\n\n\nGreen Tea\n1339\n1476\n1,398.75\n\n\nLemon\n3851\n4418\n4,080.83\n\n\nMint\n1388\n1669\n1,519.17\n\n\nRegular Espresso\n890\n1218\n1,023.42\n\n\n\n\n\n\n\n\n\n\nNext, use the code chunk below to add the statistics on the table.\n\nspark &lt;- report %&gt;%\n  group_by(Product) %&gt;%\n  summarize('Monthly Sales' = list(Sales), \n            .groups = \"drop\")\n\n\nsales &lt;- report %&gt;% \n  group_by(Product) %&gt;% \n  summarise(\"Min\" = min(Sales, na.rm = T),\n            \"Max\" = max(Sales, na.rm = T),\n            \"Average\" = mean(Sales, na.rm = T)\n            )\n\n\nsales_data = left_join(sales, spark)\n\n\n\n\n\nsales_data %&gt;%\n  gt() %&gt;%\n  gt_plt_sparkline('Monthly Sales',\n                   same_limit = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProduct\nMin\nMax\nAverage\nMonthly Sales\n\n\n\n\nAmaretto\n1016\n1210\n1119.000\n\n\n\n   1.2K\n\n\n\nCaffe Latte\n1398\n1653\n1528.333\n\n\n\n   1.5K\n\n\n\nCaffe Mocha\n3322\n3828\n3613.917\n\n\n\n   3.7K\n\n\n\nChamomile\n2967\n3395\n3217.417\n\n\n\n   3.3K\n\n\n\nColombian\n5132\n5961\n5457.250\n\n\n\n   5.5K\n\n\n\nDarjeeling\n2926\n3281\n3112.667\n\n\n\n   3.0K\n\n\n\nDecaf Espresso\n3181\n3493\n3326.833\n\n\n\n   3.2K\n\n\n\nDecaf Irish Cream\n2463\n2901\n2648.250\n\n\n\n   2.7K\n\n\n\nEarl Grey\n2730\n3005\n2841.833\n\n\n\n   3.0K\n\n\n\nGreen Tea\n1339\n1476\n1398.750\n\n\n\n   1.5K\n\n\n\nLemon\n3851\n4418\n4080.833\n\n\n\n   4.4K\n\n\n\nMint\n1388\n1669\n1519.167\n\n\n\n   1.5K\n\n\n\nRegular Espresso\n890\n1218\n1023.417\n\n\n\n   1.1K\n\n\n\n\n\n\n\n\n\n\n\nSimilarly, we can combining the bullet chart and sparklines using the steps below.\n\nbullet &lt;- coffeechain %&gt;%\n  filter(Date &gt;= \"2013-01-01\") %&gt;%\n  group_by(`Product`) %&gt;%\n  summarise(`Target` = sum(`Budget Sales`),\n            `Actual` = sum(`Sales`)) %&gt;%\n  ungroup() \n\n\nsales_data = sales_data %&gt;%\n  left_join(bullet)\n\n\nsales_data %&gt;%\n  gt() %&gt;%\n  gt_plt_sparkline('Monthly Sales') %&gt;%\n  gt_plt_bullet(column = Actual, \n                target = Target, \n                width = 28,\n                palette = c(\"lightblue\", \n                          \"black\")) %&gt;%\n  gt_theme_538()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProduct\nMin\nMax\nAverage\nMonthly Sales\nActual\n\n\n\n\nAmaretto\n1016\n1210\n1119.000\n\n\n\n   1.2K\n\n\n\n\n   \n\n\n\nCaffe Latte\n1398\n1653\n1528.333\n\n\n\n   1.5K\n\n\n\n\n   \n\n\n\nCaffe Mocha\n3322\n3828\n3613.917\n\n\n\n   3.7K\n\n\n\n\n   \n\n\n\nChamomile\n2967\n3395\n3217.417\n\n\n\n   3.3K\n\n\n\n\n   \n\n\n\nColombian\n5132\n5961\n5457.250\n\n\n\n   5.5K\n\n\n\n\n   \n\n\n\nDarjeeling\n2926\n3281\n3112.667\n\n\n\n   3.0K\n\n\n\n\n   \n\n\n\nDecaf Espresso\n3181\n3493\n3326.833\n\n\n\n   3.2K\n\n\n\n\n   \n\n\n\nDecaf Irish Cream\n2463\n2901\n2648.250\n\n\n\n   2.7K\n\n\n\n\n   \n\n\n\nEarl Grey\n2730\n3005\n2841.833\n\n\n\n   3.0K\n\n\n\n\n   \n\n\n\nGreen Tea\n1339\n1476\n1398.750\n\n\n\n   1.5K\n\n\n\n\n   \n\n\n\nLemon\n3851\n4418\n4080.833\n\n\n\n   4.4K\n\n\n\n\n   \n\n\n\nMint\n1388\n1669\n1519.167\n\n\n\n   1.5K\n\n\n\n\n   \n\n\n\nRegular Espresso\n890\n1218\n1023.417\n\n\n\n   1.1K\n\n\n\n\n   \n\n\n\n\n\n\n\n\n\n\n\n\nIn this section, you will learn how to create interactive information dashboard by using reactable and reactablefmtr packages. Before getting started, it is highly recommended for you to visit the webpage of these two packages and review all the materials provided on the webpages at least once. You done not have to understand and remember everything provided but at least have an overview of the purposes and functions provided by them.\nIn order to build an interactive sparklines, we need to install dataui R package by using the code chunk below.{r} library(dataui)\n\n\nSimilar to gtExtras, to plot an interactive sparklines by using reactablefmtr package we need to prepare the list field by using the code chunk below.\nreport &lt;- report %&gt;% group_by(Product) %&gt;% summarize(Monthly Sales = list(Sales))\nNext, react_sparkline will be to plot the sparklines as shown below.\nreactable( report, columns = list( Product = colDef(maxWidth = 200), Monthly Sales = colDef( cell = react_sparkline(report) ) ) )\n\n\n\nBy default the pagesize is 10. In the code chunk below, arguments defaultPageSize is used to change the default setting.\n{r} reactable( report, defaultPageSize = 13, columns = list( Product = colDef(maxWidth = 200), Monthly Sales = colDef( cell = react_sparkline(report) ) ) )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09/Hands_on_ex9.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex09/Hands_on_ex9.html#overview",
    "title": "Hand-on exercise 9 Visual Analytics",
    "section": "",
    "text": "By the end of this hands-on exercise, you will be able to:\n\ncreate bullet chart by using ggplot2,\ncreate sparklines by using ggplot2 ,\nbuild industry standard dashboard by using R Shiny."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09/Hands_on_ex9.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex09/Hands_on_ex9.html#getting-started",
    "title": "Hand-on exercise 9 Visual Analytics",
    "section": "",
    "text": "For the purpose of this hands-on exercise, the following R packages will be used.\n\npacman::p_load(lubridate, ggthemes, reactable,\nreactablefmtr, gt, gtExtras, tidyverse,RODBC,svglite,dataui)\n\n\ntidyverse provides a collection of functions for performing data science task such as importing, tidying, wrangling data and visualising data. It is not a single package but a collection of modern R packages including but not limited to readr, tidyr, dplyr, ggplot, tibble, stringr, forcats and purrr.\nlubridate provides functions to work with dates and times more efficiently.\nggthemes is an extension of ggplot2. It provides additional themes beyond the basic themes of ggplot2.\ngtExtras provides some additional helper functions to assist in creating beautiful tables with gt, an R package specially designed for anyone to make wonderful-looking tables using the R programming language.\nreactable provides functions to create interactive data tables for R, based on the React Table library and made with reactR.\nreactablefmtr provides various features to streamline and enhance the styling of interactive reactable tables with easy-to-use and highly-customizable functions and themes."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09/Hands_on_ex9.html#importing-microsoft-access-database",
    "href": "Hands-on_Ex/Hands-on_Ex09/Hands_on_ex9.html#importing-microsoft-access-database",
    "title": "Hand-on exercise 9 Visual Analytics",
    "section": "",
    "text": "For the purpose of this study, a personal database in Microsoft Access mdb format called Coffee Chain will be used.\n\n\n\nIn the code chunk below, odbcConnectAccess() of RODBC package is used used to import a database query table into R.\n\nlibrary(RODBC)\ncon &lt;- odbcConnectAccess2007('data/Coffee Chain.mdb')\ncoffeechain &lt;- sqlFetch(con, 'CoffeeChain Query')\nwrite_rds(coffeechain, \"data/CoffeeChain.rds\")\nodbcClose(con)\n\nNote: Before running the code chunk, you need to change the R system to 32bit version. This is because the odbcConnectAccess() is based on 32bit and not 64bit\n\n\n\nThe code chunk below is used to import CoffeeChain.rds into R.\n\ncoffeechain &lt;- read_rds(\"data/rds/CoffeeChain.rds\")\n\nNote: This step is optional if coffeechain is already available in R.\nThe code chunk below is used to aggregate Sales and Budgeted Sales at the Product level.\n\nproduct &lt;- coffeechain %&gt;%\n  group_by(`Product`) %&gt;%\n  summarise(`target` = sum(`Budget Sales`),\n            `current` = sum(`Sales`)) %&gt;%\n  ungroup()\n\n\n\n\nThe code chunk below is used to plot the bullet charts using ggplot2 functions.\n\nggplot(product, aes(Product, current)) + \n  geom_col(aes(Product, max(target) * 1.01),\n           fill=\"grey85\", width=0.85) +\n  geom_col(aes(Product, target * 0.75),\n           fill=\"grey60\", width=0.85) +\n  geom_col(aes(Product, target * 0.5),\n           fill=\"grey50\", width=0.85) +\n  geom_col(aes(Product, current), \n           width=0.35,\n           fill = \"black\") + \n  geom_errorbar(aes(y = target,\n                    x = Product, \n                    ymin = target,\n                    ymax= target), \n                width = .4,\n                colour = \"red\",\n                size = 1) +\n  coord_flip()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09/Hands_on_ex9.html#plotting-sparklines-using-ggplot2",
    "href": "Hands-on_Ex/Hands-on_Ex09/Hands_on_ex9.html#plotting-sparklines-using-ggplot2",
    "title": "Hand-on exercise 9 Visual Analytics",
    "section": "",
    "text": "In this section, you will learn how to plot sparklines by using ggplot2.\n\n\n\nsales_report &lt;- coffeechain %&gt;%\n  filter(Date &gt;= \"2013-01-01\") %&gt;%\n  mutate(Month = month(Date)) %&gt;%\n  group_by(Month, Product) %&gt;%\n  summarise(Sales = sum(Sales)) %&gt;%\n  ungroup() %&gt;%\n  select(Month, Product, Sales)\n\nThe code chunk below is used to compute the minimum, maximum and end othe the month sales.\n\nmins &lt;- group_by(sales_report, Product) %&gt;% \n  slice(which.min(Sales))\nmaxs &lt;- group_by(sales_report, Product) %&gt;% \n  slice(which.max(Sales))\nends &lt;- group_by(sales_report, Product) %&gt;% \n  filter(Month == max(Month))\n\nThe code chunk below is used to compute the 25 and 75 quantiles.\n\nquarts &lt;- sales_report %&gt;%\n  group_by(Product) %&gt;%\n  summarise(quart1 = quantile(Sales, \n                              0.25),\n            quart2 = quantile(Sales, \n                              0.75)) %&gt;%\n  right_join(sales_report)\n\n\n\n\nThe code chunk used.\n\nggplot(sales_report, aes(x=Month, y=Sales)) + \n  facet_grid(Product ~ ., scales = \"free_y\") + \n  geom_ribbon(data = quarts, aes(ymin = quart1, max = quart2), \n              fill = 'grey90') +\n  geom_line(size=0.3) +\n  geom_point(data = mins, col = 'red') +\n  geom_point(data = maxs, col = 'blue') +\n  geom_text(data = mins, aes(label = Sales), vjust = -1) +\n  geom_text(data = maxs, aes(label = Sales), vjust = 2.5) +\n  geom_text(data = ends, aes(label = Sales), hjust = 0, nudge_x = 0.5) +\n  geom_text(data = ends, aes(label = Product), hjust = 0, nudge_x = 1.0) +\n  expand_limits(x = max(sales_report$Month) + \n                  (0.25 * (max(sales_report$Month) - min(sales_report$Month)))) +\n  scale_x_continuous(breaks = seq(1, 12, 1)) +\n  scale_y_continuous(expand = c(0.1, 0)) +\n  theme_tufte(base_size = 3, base_family = \"Helvetica\") +\n  theme(axis.title=element_blank(), axis.text.y = element_blank(), \n        axis.ticks = element_blank(), strip.text = element_blank())"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09/Hands_on_ex9.html#static-information-dashboard-design-gt-and-gtextras-methods",
    "href": "Hands-on_Ex/Hands-on_Ex09/Hands_on_ex9.html#static-information-dashboard-design-gt-and-gtextras-methods",
    "title": "Hand-on exercise 9 Visual Analytics",
    "section": "",
    "text": "In this section, you will learn how to create static information dashboard by using gt and gtExtras packages. Before getting started, it is highly recommended for you to visit the webpage of these two packages and review all the materials provided on the webpages at least once. You done not have to understand and remember everything provided but at least have an overview of the purposes and functions provided by them.\n\n\nIn this section, you will learn how to prepare a bullet chart report by using functions of gt and gtExtras packages.\n\nproduct %&gt;%\n  gt::gt() %&gt;%\n  gt_plt_bullet(column = current, \n              target = target, \n              width = 60,\n              palette = c(\"lightblue\", \n                          \"black\")) %&gt;%\n  gt_theme_538()\n\n\n\n\n\n\n\n\n\n\n\nProduct\ncurrent\n\n\n\n\nAmaretto\n\n\n\n   \n\n\n\nCaffe Latte\n\n\n\n   \n\n\n\nCaffe Mocha\n\n\n\n   \n\n\n\nChamomile\n\n\n\n   \n\n\n\nColombian\n\n\n\n   \n\n\n\nDarjeeling\n\n\n\n   \n\n\n\nDecaf Espresso\n\n\n\n   \n\n\n\nDecaf Irish Cream\n\n\n\n   \n\n\n\nEarl Grey\n\n\n\n   \n\n\n\nGreen Tea\n\n\n\n   \n\n\n\nLemon\n\n\n\n   \n\n\n\nMint\n\n\n\n   \n\n\n\nRegular Espresso"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09/Hands_on_ex9.html#sparklines-gtextras-method",
    "href": "Hands-on_Ex/Hands-on_Ex09/Hands_on_ex9.html#sparklines-gtextras-method",
    "title": "Hand-on exercise 9 Visual Analytics",
    "section": "",
    "text": "Before we can prepare the sales report by product by using gtExtras functions, code chunk below will be used to prepare the data.\n\nreport &lt;- coffeechain %&gt;%\n  mutate(Year = year(Date)) %&gt;%\n  filter(Year == \"2013\") %&gt;%\n  mutate (Month = month(Date, \n                        label = TRUE, \n                        abbr = TRUE)) %&gt;%\n  group_by(Product, Month) %&gt;%\n  summarise(Sales = sum(Sales)) %&gt;%\n  ungroup()\n\nIt is important to note that one of the requirement of gtExtras functions is that almost exclusively they require you to pass data.frame with list columns. In view of this, code chunk below will be used to convert the report data.frame into list columns.\n\nreport %&gt;%\n  group_by(Product) %&gt;%\n  summarize('Monthly Sales' = list(Sales), \n            .groups = \"drop\")\n\n# A tibble: 13 × 2\n   Product           `Monthly Sales`\n   &lt;chr&gt;             &lt;list&gt;         \n 1 Amaretto          &lt;dbl [12]&gt;     \n 2 Caffe Latte       &lt;dbl [12]&gt;     \n 3 Caffe Mocha       &lt;dbl [12]&gt;     \n 4 Chamomile         &lt;dbl [12]&gt;     \n 5 Colombian         &lt;dbl [12]&gt;     \n 6 Darjeeling        &lt;dbl [12]&gt;     \n 7 Decaf Espresso    &lt;dbl [12]&gt;     \n 8 Decaf Irish Cream &lt;dbl [12]&gt;     \n 9 Earl Grey         &lt;dbl [12]&gt;     \n10 Green Tea         &lt;dbl [12]&gt;     \n11 Lemon             &lt;dbl [12]&gt;     \n12 Mint              &lt;dbl [12]&gt;     \n13 Regular Espresso  &lt;dbl [12]&gt;     \n\n\n\n\n\nreport %&gt;%\n  group_by(Product) %&gt;%\n  summarize('Monthly Sales' = list(Sales), \n            .groups = \"drop\") %&gt;%\n   gt() %&gt;%\n   gt_plt_sparkline('Monthly Sales',\n                    same_limit = FALSE)\n\n\n\n\n\n\n\n\n\n\n\nProduct\nMonthly Sales\n\n\n\n\nAmaretto\n\n\n\n   1.2K\n\n\n\nCaffe Latte\n\n\n\n   1.5K\n\n\n\nCaffe Mocha\n\n\n\n   3.7K\n\n\n\nChamomile\n\n\n\n   3.3K\n\n\n\nColombian\n\n\n\n   5.5K\n\n\n\nDarjeeling\n\n\n\n   3.0K\n\n\n\nDecaf Espresso\n\n\n\n   3.2K\n\n\n\nDecaf Irish Cream\n\n\n\n   2.7K\n\n\n\nEarl Grey\n\n\n\n   3.0K\n\n\n\nGreen Tea\n\n\n\n   1.5K\n\n\n\nLemon\n\n\n\n   4.4K\n\n\n\nMint\n\n\n\n   1.5K\n\n\n\nRegular Espresso\n\n\n\n   1.1K\n\n\n\n\n\n\n\n\n\n\n\nFirst, calculate summary statistics by using the code chunk below.\n\nreport %&gt;% \n  group_by(Product) %&gt;% \n  summarise(\"Min\" = min(Sales, na.rm = T),\n            \"Max\" = max(Sales, na.rm = T),\n            \"Average\" = mean(Sales, na.rm = T)\n            ) %&gt;%\n  gt() %&gt;%\n  fmt_number(columns = 4,\n    decimals = 2)\n\n\n\n\n\n\n\nProduct\nMin\nMax\nAverage\n\n\n\n\nAmaretto\n1016\n1210\n1,119.00\n\n\nCaffe Latte\n1398\n1653\n1,528.33\n\n\nCaffe Mocha\n3322\n3828\n3,613.92\n\n\nChamomile\n2967\n3395\n3,217.42\n\n\nColombian\n5132\n5961\n5,457.25\n\n\nDarjeeling\n2926\n3281\n3,112.67\n\n\nDecaf Espresso\n3181\n3493\n3,326.83\n\n\nDecaf Irish Cream\n2463\n2901\n2,648.25\n\n\nEarl Grey\n2730\n3005\n2,841.83\n\n\nGreen Tea\n1339\n1476\n1,398.75\n\n\nLemon\n3851\n4418\n4,080.83\n\n\nMint\n1388\n1669\n1,519.17\n\n\nRegular Espresso\n890\n1218\n1,023.42\n\n\n\n\n\n\n\n\n\n\nNext, use the code chunk below to add the statistics on the table.\n\nspark &lt;- report %&gt;%\n  group_by(Product) %&gt;%\n  summarize('Monthly Sales' = list(Sales), \n            .groups = \"drop\")\n\n\nsales &lt;- report %&gt;% \n  group_by(Product) %&gt;% \n  summarise(\"Min\" = min(Sales, na.rm = T),\n            \"Max\" = max(Sales, na.rm = T),\n            \"Average\" = mean(Sales, na.rm = T)\n            )\n\n\nsales_data = left_join(sales, spark)\n\n\n\n\n\nsales_data %&gt;%\n  gt() %&gt;%\n  gt_plt_sparkline('Monthly Sales',\n                   same_limit = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProduct\nMin\nMax\nAverage\nMonthly Sales\n\n\n\n\nAmaretto\n1016\n1210\n1119.000\n\n\n\n   1.2K\n\n\n\nCaffe Latte\n1398\n1653\n1528.333\n\n\n\n   1.5K\n\n\n\nCaffe Mocha\n3322\n3828\n3613.917\n\n\n\n   3.7K\n\n\n\nChamomile\n2967\n3395\n3217.417\n\n\n\n   3.3K\n\n\n\nColombian\n5132\n5961\n5457.250\n\n\n\n   5.5K\n\n\n\nDarjeeling\n2926\n3281\n3112.667\n\n\n\n   3.0K\n\n\n\nDecaf Espresso\n3181\n3493\n3326.833\n\n\n\n   3.2K\n\n\n\nDecaf Irish Cream\n2463\n2901\n2648.250\n\n\n\n   2.7K\n\n\n\nEarl Grey\n2730\n3005\n2841.833\n\n\n\n   3.0K\n\n\n\nGreen Tea\n1339\n1476\n1398.750\n\n\n\n   1.5K\n\n\n\nLemon\n3851\n4418\n4080.833\n\n\n\n   4.4K\n\n\n\nMint\n1388\n1669\n1519.167\n\n\n\n   1.5K\n\n\n\nRegular Espresso\n890\n1218\n1023.417\n\n\n\n   1.1K\n\n\n\n\n\n\n\n\n\n\n\nSimilarly, we can combining the bullet chart and sparklines using the steps below.\n\nbullet &lt;- coffeechain %&gt;%\n  filter(Date &gt;= \"2013-01-01\") %&gt;%\n  group_by(`Product`) %&gt;%\n  summarise(`Target` = sum(`Budget Sales`),\n            `Actual` = sum(`Sales`)) %&gt;%\n  ungroup() \n\n\nsales_data = sales_data %&gt;%\n  left_join(bullet)\n\n\nsales_data %&gt;%\n  gt() %&gt;%\n  gt_plt_sparkline('Monthly Sales') %&gt;%\n  gt_plt_bullet(column = Actual, \n                target = Target, \n                width = 28,\n                palette = c(\"lightblue\", \n                          \"black\")) %&gt;%\n  gt_theme_538()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProduct\nMin\nMax\nAverage\nMonthly Sales\nActual\n\n\n\n\nAmaretto\n1016\n1210\n1119.000\n\n\n\n   1.2K\n\n\n\n\n   \n\n\n\nCaffe Latte\n1398\n1653\n1528.333\n\n\n\n   1.5K\n\n\n\n\n   \n\n\n\nCaffe Mocha\n3322\n3828\n3613.917\n\n\n\n   3.7K\n\n\n\n\n   \n\n\n\nChamomile\n2967\n3395\n3217.417\n\n\n\n   3.3K\n\n\n\n\n   \n\n\n\nColombian\n5132\n5961\n5457.250\n\n\n\n   5.5K\n\n\n\n\n   \n\n\n\nDarjeeling\n2926\n3281\n3112.667\n\n\n\n   3.0K\n\n\n\n\n   \n\n\n\nDecaf Espresso\n3181\n3493\n3326.833\n\n\n\n   3.2K\n\n\n\n\n   \n\n\n\nDecaf Irish Cream\n2463\n2901\n2648.250\n\n\n\n   2.7K\n\n\n\n\n   \n\n\n\nEarl Grey\n2730\n3005\n2841.833\n\n\n\n   3.0K\n\n\n\n\n   \n\n\n\nGreen Tea\n1339\n1476\n1398.750\n\n\n\n   1.5K\n\n\n\n\n   \n\n\n\nLemon\n3851\n4418\n4080.833\n\n\n\n   4.4K\n\n\n\n\n   \n\n\n\nMint\n1388\n1669\n1519.167\n\n\n\n   1.5K\n\n\n\n\n   \n\n\n\nRegular Espresso\n890\n1218\n1023.417\n\n\n\n   1.1K"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09/Hands_on_ex9.html#interactive-information-dashboard-design-reactable-and-reactablefmtr-methods",
    "href": "Hands-on_Ex/Hands-on_Ex09/Hands_on_ex9.html#interactive-information-dashboard-design-reactable-and-reactablefmtr-methods",
    "title": "Hand-on exercise 9 Visual Analytics",
    "section": "",
    "text": "In this section, you will learn how to create interactive information dashboard by using reactable and reactablefmtr packages. Before getting started, it is highly recommended for you to visit the webpage of these two packages and review all the materials provided on the webpages at least once. You done not have to understand and remember everything provided but at least have an overview of the purposes and functions provided by them.\nIn order to build an interactive sparklines, we need to install dataui R package by using the code chunk below.{r} library(dataui)\n\n\nSimilar to gtExtras, to plot an interactive sparklines by using reactablefmtr package we need to prepare the list field by using the code chunk below.\nreport &lt;- report %&gt;% group_by(Product) %&gt;% summarize(Monthly Sales = list(Sales))\nNext, react_sparkline will be to plot the sparklines as shown below.\nreactable( report, columns = list( Product = colDef(maxWidth = 200), Monthly Sales = colDef( cell = react_sparkline(report) ) ) )\n\n\n\nBy default the pagesize is 10. In the code chunk below, arguments defaultPageSize is used to change the default setting.\n{r} reactable( report, defaultPageSize = 13, columns = list( Product = colDef(maxWidth = 200), Monthly Sales = colDef( cell = react_sparkline(report) ) ) )"
  },
  {
    "objectID": "Take-home_Ex/Take-home_ex04/data/geospatial/MPSZ-2019.html",
    "href": "Take-home_Ex/Take-home_ex04/data/geospatial/MPSZ-2019.html",
    "title": "ISSS608-Zhongchao",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "Take-home_Ex/Take-home_ex04/Take-home_ex4.html#inverse-distance-weighted-idw",
    "href": "Take-home_Ex/Take-home_ex04/Take-home_ex4.html#inverse-distance-weighted-idw",
    "title": "Take-home exercise 4 Visual Analytics",
    "section": "Inverse Distance Weighted (IDW)",
    "text": "Inverse Distance Weighted (IDW)\n\nThe method\nIn the IDW interpolation method, the sample points are weighted during interpolation such that the influence of one point relative to another declines with distance from the unknown point you want to create.\n\n\nWorking with gstat\nWe are going to use three parameters of the gstat function:\n\nformula: The prediction “formula” specifying the dependent and the independent variables (covariates)\ndata: The calibration data\nmodel: The variogram model\n\nKeep in mind that we need to specify parameter names, because these three parameters are not the first three in the gstat function definition.\nFor example, to interpolate using the IDW method we create the following gstat object, specifying just the formula and data:\ng = gstat(formula = annual ~ 1, data = rainfall)\n\n#res &lt;- gstat(formula = MONTHSUM ~ 1, \n#             locations = rfdata_sf, \n#             nmax = 5,\n#             set = list(idp = 0))\n\n#resp &lt;- predict(res, coop)\n\n#resp$x &lt;- st_coordinates(resp)[,1]\n#resp$y &lt;- st_coordinates(resp)[,2]\n#resp$pred &lt;- resp$var1.pred\n\n#pred &lt;- terra::rasterize(resp, grid, \n#                         field = \"pred\", \n#                         fun = \"mean\")\n\n#tmap_options(check.and.fix = TRUE)\n#tmap_mode(\"plot\")\n#tm_shape(pred) + \n#  tm_raster(alpha = 0.6, \n#            palette = \"viridis\")\n\n\n\nWorking with gstat\nFirstly, we will calculate and examine the empirical variogram by using variogram() of gstat package. The function requires two arguments:\n\nformula, the dependent variable and the covariates (same as in gstat, see Section 12.2.1)\ndata, a point layer with the dependent variable and covariates as attributes\n\nas shown in the code chunk below.\n\n#v &lt;- variogram(MONTHSUM ~ 1, \n#               data = rfdata_sf)\n#plot(v)\n\nside by side compare for tempurature and rainfall map.\nNEED TO IN CLUDE THE STATSTICAL TEST RESULTS\nheatmap interactive\nrain station:\nuncertainty, boxplot compare distribution, compare mean, median\nHorizon Plot\nDistribution of categorical data: ‘Station’, ‘Month’ ‘Weekday’\nDistribution of Numerical data:"
  },
  {
    "objectID": "Take-home_Ex/Take-home_ex04/Take-home_ex4.html#does-singapores-weather-change-across-different-years-shows-statistical-significant",
    "href": "Take-home_Ex/Take-home_ex04/Take-home_ex4.html#does-singapores-weather-change-across-different-years-shows-statistical-significant",
    "title": "Take-home exercise 4 Visual Analytics",
    "section": "6.1Does Singapore’s Weather Change Across Different Years Shows Statistical Significant?",
    "text": "6.1Does Singapore’s Weather Change Across Different Years Shows Statistical Significant?\nIn this part, the weather change accounts for both the temperature and the rainfall as given in the data used.\n\n6.1.1 Temperature\n\ntemp_year1 &lt;- weather_data_imputed %&gt;%\n  group_by(Station,Year) %&gt;%\n  summarise(median_mean_temp = median(Mean_Temperature),\n            median_max_temp = median(Max_Temperature),\n            median_min_temp = median(Min_Temperature))\n\nDT::datatable(temp_year1,class = \"compact\")\n\n\n\n\n\n\nglimpse(temp_year1)\n\nRows: 70\nColumns: 5\nGroups: Station [7]\n$ Station          &lt;chr&gt; \"Changi\", \"Changi\", \"Changi\", \"Changi\", \"Changi\", \"Ch…\n$ Year             &lt;fct&gt; 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022,…\n$ median_mean_temp &lt;dbl&gt; 28.00, 28.40, 28.60, 27.70, 27.90, 28.60, 28.10, 28.0…\n$ median_max_temp  &lt;dbl&gt; 31.9, 32.0, 32.1, 31.3, 31.9, 32.6, 31.9, 32.1, 31.8,…\n$ median_min_temp  &lt;dbl&gt; 25.20, 25.80, 25.90, 25.10, 25.40, 26.00, 25.40, 25.2…\n\n\nSave the output as csv:\n\nwrite_csv(temp_year1, \"data/temp_year1.csv\")\n\nTo check out the median mean, max and min temperature:\n\nMedian Daily Mean TemperatureMedian Daily MAX TemperatureMedian Daily MIN Temperature\n\n\n\nplot_list &lt;- lapply(unique(temp_year1$Station), function(stn) {\n  station_data &lt;- subset(temp_year1, Station == stn)\n  \n  plot_ly(data = station_data, x = ~Year, y = ~median_mean_temp, name = stn, type = 'scatter', mode = 'lines',\n          hoverinfo = 'text', text = ~paste(\"Station:\", stn, \"&lt;br&gt;Year:\", Year, \"&lt;br&gt;Temp:\", median_mean_temp)) %&gt;%\n    layout(title = paste(\"Median Mean Temperature - Station:\", stn),\n           xaxis = list(title = \"Year\"),\n           yaxis = list(title = \"Temperature (°C)\"))\n})\n\np2 &lt;- subplot(plot_list, nrows = length(unique(temp_year1$Station)), shareX = TRUE, titleX = FALSE)\n\np2 &lt;- layout(p2,\n                       title = \"Median Daily Mean Temperature Across Weather Stations (2014-2023)\",\n                       xaxis = list(tickangle = 90),\n                       margin = list(b = 80)) # Increase bottom margin to accommodate angled x-axis labels\np2\n\n\n\n\n\nBased on the line graph, we can not say that there the mean temperature across selected weather stations have significant changes from year 2014 to 2023.\n\n\n\nplot_list &lt;- lapply(unique(temp_year1$Station), function(stn) {\n  station_data &lt;- subset(temp_year1, Station == stn)\n  \n  plot_ly(data = station_data, x = ~Year, y = ~median_max_temp, name = stn, type = 'scatter', mode = 'lines',\n          hoverinfo = 'text', text = ~paste(\"Station:\", stn, \"&lt;br&gt;Year:\", Year, \"&lt;br&gt;Temp:\", median_max_temp)) %&gt;%\n    layout(title = paste(\"Median Max Temperature - Station:\", stn),\n           xaxis = list(title = \"Year\"),\n           yaxis = list(title = \"Temperature (°C)\"))\n})\n\np3 &lt;- subplot(plot_list, nrows = length(unique(temp_year1$Station)), shareX = TRUE, titleX = FALSE)\n\np3 &lt;- layout(p3,\n                       title = \"Median Daily Maximum Temperature Across Weather Stations (2014-2023)\",\n                       xaxis = list(tickangle = 90),\n                       margin = list(b = 80)) # Increase bottom margin to accommodate angled x-axis labels\np3\n\n\n\n\n\n\n\n\np4 &lt;- lapply(unique(temp_year1$Station), function(stn) {\n  station_data &lt;- subset(temp_year1, Station == stn)\n  \n  plot_ly(data = station_data, x = ~Year, y = ~median_min_temp, name = stn, type = 'scatter', mode = 'lines',\n          hoverinfo = 'text', text = ~paste(\"Station:\", stn, \"&lt;br&gt;Year:\", Year, \"&lt;br&gt;Temp:\", median_min_temp)) %&gt;%\n    layout(title = paste(\"Median Min Temperature - Station:\", stn),\n           xaxis = list(title = \"Year\"),\n           yaxis = list(title = \"Temperature (°C)\"))\n})\n\np4 &lt;- subplot(p4, nrows = length(unique(temp_year1$Station)), shareX = TRUE, titleX = FALSE)\n\np4 &lt;- layout(p4,\n                       title = \"Median Daily Minimum Temperature Across Weather Stations (2014-2023)\",\n                       xaxis = list(tickangle = 90),\n                       margin = list(b = 80)) # Increase bottom margin to accommodate angled x-axis labels\np4\n\n\n\n\n\n\n\n\nBefore performing statistical test on the significant level, is best to determine how the temperature data is distributed in the data. We can observe the normality of the data using ridgeline plots, using the code chunk below:\n\nNormality Daily Mean TemperatureNormality Daily Max TemperatureNormality Daily Min Temperature\n\n\n\np5 &lt;- ggplot(weather_data_imputed, \n       aes(x = Mean_Temperature, \n           y = as.factor(Year), \n           fill = 0.5 - abs(0.5 - ..ecdf..))) +\n  stat_density_ridges(geom = \"density_ridges_gradient\", \n                      calc_ecdf = TRUE) +\n  scale_fill_viridis_c(name = \"Tail probability\",\n                       direction = -1,\n                       option=\"turbo\") +\n  facet_wrap(~Station, scales = \"free_y\") + \n  theme_ridges(font_size = 12) + # Adjusted for smaller text\n  coord_cartesian(xlim = c(0,50)) +\n  labs(title=\"Distribution of Mean Temperature from 2014 to 2023\",\n       y=\"Station\",\n       x=\"Mean Temperature (°C)\")\n\np5\n\n\n\n\n\n\n\n\nBased on the above observation, as the mean temperature is not normally distributed, non-parametric test will be used.\n\n\n\np6 &lt;- ggplot(weather_data_imputed, \n       aes(x = Max_Temperature, \n           y = as.factor(Year), \n           fill = 0.5 - abs(0.5 - ..ecdf..))) +\n  stat_density_ridges(geom = \"density_ridges_gradient\", \n                      calc_ecdf = TRUE) +\n  scale_fill_viridis_c(name = \"Tail probability\",\n                       direction = -1,\n                       option=\"turbo\")+\n  facet_wrap(~Station, scales = \"free_y\") + \n  theme_ridges(font_size = 12)+\n  coord_cartesian(xlim = c(0,50))+\n  labs(title=\"Distribution of Maximum Temperature from 2014 to 2023\",\n       y=\"Station\",\n       x=\"Maximum Temperature (°C)\")\n\np6\n\n\n\n\n\n\n\n\nBased on the above observation, as the maximum temperature is not normally distributed, non-parametric test will be used.\n\n\n\np8 &lt;- ggplot(weather_data_imputed, \n       aes(x = Min_Temperature, \n           y = as.factor(Year), \n           fill = 0.5 - abs(0.5 - ..ecdf..))) +\n  stat_density_ridges(geom = \"density_ridges_gradient\", \n                      calc_ecdf = TRUE) +\n  scale_fill_viridis_c(name = \"Tail probability\",\n                       direction = -1,\n                       option=\"turbo\")+\n  facet_wrap(~Station, scales = \"free_y\") + \n  theme_ridges(font_size = 12)+\n  coord_cartesian(xlim = c(0,50))+\n  labs(title=\"Distribution of Minimum Temperature from 2014 to 2023\",\n       y=\"Station\",\n       x=\"Minimum Temperature (°C)\")\n\np8\n\n\n\n\n\n\n\n\nBased on the above observation, as the minimum temperature is not normally distributed, non-parametric test will be used.\n\n\n\nDefault Non-Parametric tests temperature different per year:\n\nMedian Mean temperature Per YearMedian Max temperature Per YearMedian Min temperature Per Year\n\n\nThe hypothesis is as follows:\nH0: There is no statistical difference between yearly median mean temperature from 2014-2023.\nH1: There is statistical difference between yearly median mean temperature from 2014-2023.\n\np9 &lt;- ggbetweenstats(\n  data = temp_year1,\n  x = Year, \n  y = median_mean_temp,\n  type = \"np\",\n  messages = FALSE,\n  title=\"Distribution of Yearly Median Mean Temperature from 2014 to 2023\",\n  ylab = \"Temperature (°C)\",\n  xlab = \"Year\",\n  ggsignif.args = list(textsize = 4)\n) +\n  theme(text = element_text(size = 12),plot.title=element_text(size=12))\np9\n\n\n\n\n\n\n\n\nKruskal-Wallis Test: The test has a x^2 value of 19.82 and a p-value of 0.02, which is below the conventional alpha level of 0.05. This suggests that there is statistically significant difference in median maximum temperatures across the years.\n\n\nThe hypothesis is as follows:\nH0: There is no statistical difference between yearly median max temperature from 2014-2023.\nH1: There is statistical difference between yearly median max temperature from 2014-2023.\n\np10 &lt;- ggbetweenstats(\n  data = temp_year1,\n  x = Year, \n  y = median_max_temp,\n  type = \"np\",\n  messages = FALSE,\n  title=\"Distribution of Yearly Median Maximum Temperature from 2014 to 2023\",\n  ylab = \"Temperature (°C)\",\n  xlab = \"Year\",\n  ggsignif.args = list(textsize = 4)\n) +\n  theme(text = element_text(size = 12),plot.title=element_text(size=12))\np10\n\n\n\n\n\n\n\n\nKruskal-Wallis Test: The test has a x^2 value of 10.02 and a p-value of 0.35, which is above the conventional alpha level of 0.05. This suggests that there is no statistically significant difference in median maximum temperatures across the years. But from yearly difference we can further dig into the daily difference to see if there is a statistical difference.\n\n\nThe hypothesis is as follows:\nH0: There is no statistical difference between yearly median min temperature from 2014-2023.\nH1: There is statistical difference between yearly median min temperature from 2014-2023.\n\np11 &lt;- ggbetweenstats(\n  data = temp_year1,\n  x = Year, \n  y = median_min_temp,\n  type = \"np\",\n  messages = FALSE,\n  title=\"Distribution of Yearly Median Minimum Temperature from 2014 to 2023\",\n  ylab = \"Temperature (°C)\",\n  xlab = \"Year\",\n  ggsignif.args = list(textsize = 4)\n) +\n  theme(text = element_text(size = 12),plot.title=element_text(size=12))\np11\n\n\n\n\n\n\n\n\nKruskal-Wallis Test: The test has a x^2 value of 14.72 and a p-value of 0.10, which is above the conventional alpha level of 0.05. This suggests that there is no statistically significant difference in median maximum temperatures across the years. But from yearly difference we can further dig into the daily difference to see if there is a statistical difference.\n\n\n\n\nDaily Mean temperatureDaily Max temperatureDaily Min temperature\n\n\nHypothesis :\nH0: There is no statistical difference in daily mean temperature from 2014-2023.\nH1: There is statistical difference in daily mean temperature from 2014-2023.\n\np12 &lt;- ggbetweenstats(\n  data = weather_data_imputed,\n  x = Year, \n  y = Mean_Temperature,\n  type = \"np\",\n  messages = FALSE,\n  title=\"Distribution of Daily Minimum Temperature from 2014 to 2023\",\n  ylab = \"Temperature (°C)\",\n  xlab = \"Year\",\n  ggsignif.args = list(textsize = 4)\n) +\n  theme(text = element_text(size = 12),plot.title=element_text(size=12))\np12\n\n\n\n\n\n\n\n\n\n\nHypothesis :\nH0: There is no statistical difference in daily max temperature from 2014-2023.\nH1: There is statistical difference in daily max temperature from 2014-2023.\n\np13 &lt;- ggbetweenstats(\n  data = weather_data_imputed,\n  x = Year, \n  y = Max_Temperature,\n  type = \"np\",\n  messages = FALSE,\n  title=\"Distribution of Daily Maximum Temperature from 2014 to 2023\",\n  ylab = \"Temperature (°C)\",\n  xlab = \"Year\",\n  ggsignif.args = list(textsize = 4)\n) +\n  theme(text = element_text(size = 12),plot.title=element_text(size=12))\np13\n\n\n\n\n\n\n\n\n\n\nHypothesis :\nH0: There is no statistical difference in daily min temperature from 2014-2023.\nH1: There is statistical difference in daily min temperature from 2014-2023.\n\np14 &lt;- ggbetweenstats(\n  data = weather_data_imputed,\n  x = Year, \n  y = Min_Temperature,\n  type = \"np\",\n  messages = FALSE,\n  title=\"Distribution of Daily Minimum Temperature from 2014 to 2023\",\n  ylab = \"Temperature (°C)\",\n  xlab = \"Year\",\n  ggsignif.args = list(textsize = 4)\n) +\n  theme(text = element_text(size = 12),plot.title=element_text(size=12))\np14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nAll daily temperatures (mean, maximum and minimum) have p-value lower than 0.05 which means they all shows statistical significant. Meaning there are statistical different in the mean, maximum and minimum daily temperature in Singapore.\n\nEven though the yearly median max temperature and median min temperature appears no statistical significant, but the daily max and min are statistically different.\n\n\n\n\n\n6.1.2 Rainfall\nFor rainfall we will be using the daily rainfall total to test the hypothesis\n\nrainfall_year &lt;- weather_data_imputed %&gt;%\n  group_by(Station,Year) %&gt;%\n  summarise(yearly_rainfall = sum(Daily_Rainfall_Total_mm))\n\nDT::datatable(rainfall_year,class = \"compact\")\n\n\n\n\n\n\nwrite_csv(rainfall_year, \"data/rainfall_year.csv\")\n\n\nplot_list &lt;- lapply(unique(rainfall_year$Station), function(stn) {\n  station_data &lt;- subset(rainfall_year, Station == stn)\n  \n  plot_ly(data = station_data, x = ~Year, y = ~yearly_rainfall, name = stn, type = 'scatter', mode = 'lines') %&gt;%\n    layout(title = paste(\"Yearly Rainfall - Station:\", stn),\n           xaxis = list(title = \"Year\", tickangle = 90),\n           yaxis = list(title = \"Rainfall Volume (mm)\"))\n})\n\nfaceted_plot &lt;- subplot(plot_list, nrows = length(unique(rainfall_year$Station)), shareX = TRUE, titleX = FALSE)\n\nfaceted_plot &lt;- layout(faceted_plot,\n                       title = \"Yearly Rainfall Across Weather Stations (2014-2023)\")\nfaceted_plot\n\n\n\n\n\nFrom the observations above, over the pass 10 years from 2014 to 2023 the total rainfall for Singapore captured by different stations indicates there is a volume increase. And every a few years it tend to drop to a low point(2015 and 2019) and will bounce back with even higher volume.\nWe have to check if the different in years of the total rainfall are statistically different, before making any conclusions. But first lets see if the data follows a normal distribution or not in determine the method for test later.\n\np7 &lt;- ggplot(weather_data_imputed, \n       aes(x = Daily_Rainfall_Total_mm, \n           y = as.factor(Year), \n           fill = 0.5 - abs(0.5 - ..ecdf..))) +\n  stat_density_ridges(geom = \"density_ridges_gradient\", \n                      calc_ecdf = TRUE) +\n  scale_fill_viridis_c(name = \"Tail probability\",\n                       direction = -1,\n                       option=\"turbo\")+\n  facet_wrap(~Station) + \n  theme_ridges(font_size = 12)+\n  coord_cartesian(xlim = c(0,50))+\n  labs(title=\"Distribution of Daily Rainfall from 2014 to 2023\",\n       y=\"Station\",\n       x=\"Rainfall Volume (mm)\")\n\np7\n\n\n\n\n\n\n\n\nFrom the distribution graph using sstat function called stat_density_ridges()of ggplot2. We can see that the rainfall distribution is not normally distributed, so non-parametric test will be used.\n\nMedian Rainfall Per YearMedian Daily Rainfall 2014-2023\n\n\nHypothesis:\nH0: There is no statistical difference between median rainfall per year from 2014-2023.\nH1: There is statistical difference between median rainfall per year across 2014-2023.\n\np8 &lt;- ggbetweenstats(\n  data = rainfall_year,\n  x = Year, \n  y = yearly_rainfall,\n  type = \"np\",\n  pairwise.display = \"non-significant\",\n  messages = FALSE,\n  title=\"Distribution of Rainfall from 2014 to 2023\",\n  ylab = \"Rainfall volume (mm)\",\n  xlab = \"Year\",\n  ggsignif.args = list(textsize = 4)\n) +\n  theme(text = element_text(size = 12), plot.title=element_text(size=12))\n\np8\n\n\n\n\n\n\n\n\nKruskal-Wallis test result at the top indicates a significant difference in rainfall distribution across the years (p-value &lt; 0.01), suggesting that at least one year has a statistically different total rainfall volume compared to the others.\nfrom the lines connecting the years we can observe that some years trend towards significance when the pHolm-adj is less than 1.00. Hence we can reject the null hypothesis and say that the rainfall over the years is statistically significant.\n\n\nHypothesis:\nH0: There is no statistical difference between median daily rainfall from 2014-2023.\nH1: There is statistical difference between median daily rainfall from 2014-2023.\n\np9 &lt;- ggbetweenstats(\n  data = weather_data_imputed,\n  x = Year, \n  y = Daily_Rainfall_Total_mm,\n  type = \"np\",\n  pairwise.display = \"non-significant\",\n  messages = FALSE,\n  title=\"Distribution of Rainfall from 2014 to 2023\",\n  ylab = \"Rainfall volume (mm)\",\n  xlab = \"Year\",\n  ggsignif.args = list(textsize = 4)\n) +\n  theme(text = element_text(size = 12), plot.title=element_text(size=12))\n\np9\n\n\n\n\n\n\n\n\nWe can dig further into the daily rainfall total to see if there is statistical significant. After undergo the test above, we can observe the test result that overall Kruskal-Wallis test is highly significant (p=2.84e−89), indicating there are differences in the distributions of daily rainfall volumes across the years.\nWe can also observe that some years the median daily rainfall is 0.00 or 0.20 (2014, 2015, 2019, etc.) suggesting low rainfall volume."
  },
  {
    "objectID": "Take-home_Ex/Take-home_ex04/Take-home_ex4.html#can-we-clearly-identify-the-dry-and-wet-month",
    "href": "Take-home_Ex/Take-home_ex04/Take-home_ex4.html#can-we-clearly-identify-the-dry-and-wet-month",
    "title": "Take-home exercise 4 Visual Analytics",
    "section": "6.2 Can We Clearly Identify the ‘Dry’ and ‘Wet’ Month?",
    "text": "6.2 Can We Clearly Identify the ‘Dry’ and ‘Wet’ Month?"
  },
  {
    "objectID": "Take-home_Ex/Take-home_ex04/Take-home_ex4.html#cda-hypothesis",
    "href": "Take-home_Ex/Take-home_ex04/Take-home_ex4.html#cda-hypothesis",
    "title": "Take-home exercise 4 Visual Analytics",
    "section": "5.2 CDA Hypothesis",
    "text": "5.2 CDA Hypothesis\nThis confirmatory data analysis section will check the data based on the certain hypothesis made from the exploratory & descriptive analysis in the above sections.\nAs of now, two hypothesis are made:\n\nDoes Singapore’s weather change across different years shows statistical significant?\ncan we clearly identify the ‘dry’ and ‘wet’ month?\n\n\n5.2.1Does Singapore’s Weather Change Across Different Years Shows Statistical Significant?\nIn this part, the weather change accounts for both the temperature and the rainfall as given in the data used.\n\n5.2.1.1 Temperature\n\ntemp_year1 &lt;- weather_data_imputed %&gt;%\n  group_by(Station,Year) %&gt;%\n  summarise(median_mean_temp = median(Mean_Temperature),\n            median_max_temp = median(Max_Temperature),\n            median_min_temp = median(Min_Temperature))\n\nDT::datatable(temp_year1,class = \"compact\")\n\n\n\n\n\n\nglimpse(temp_year1)\n\nRows: 70\nColumns: 5\nGroups: Station [7]\n$ Station          &lt;chr&gt; \"Changi\", \"Changi\", \"Changi\", \"Changi\", \"Changi\", \"Ch…\n$ Year             &lt;fct&gt; 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022,…\n$ median_mean_temp &lt;dbl&gt; 28.00, 28.40, 28.60, 27.70, 27.90, 28.60, 28.10, 28.0…\n$ median_max_temp  &lt;dbl&gt; 31.9, 32.0, 32.1, 31.3, 31.9, 32.6, 31.9, 32.1, 31.8,…\n$ median_min_temp  &lt;dbl&gt; 25.20, 25.80, 25.90, 25.10, 25.40, 26.00, 25.40, 25.2…\n\n\nSave the output as csv:\n\nwrite_csv(temp_year1, \"data/temp_year1.csv\")\n\nTo check out the median mean, max and min temperature:\n\nMedian Daily Mean TemperatureMedian Daily MAX TemperatureMedian Daily MIN Temperature\n\n\n\nplot_list &lt;- lapply(unique(temp_year1$Station), function(stn) {\n  station_data &lt;- subset(temp_year1, Station == stn)\n  \n  plot_ly(data = station_data, x = ~Year, y = ~median_mean_temp, name = stn, type = 'scatter', mode = 'lines',\n          hoverinfo = 'text', text = ~paste(\"Station:\", stn, \"&lt;br&gt;Year:\", Year, \"&lt;br&gt;Temp:\", median_mean_temp)) %&gt;%\n    layout(title = paste(\"Median Mean Temperature - Station:\", stn),\n           xaxis = list(title = \"Year\"),\n           yaxis = list(title = \"Temperature (°C)\"))\n})\n\np2 &lt;- subplot(plot_list, nrows = length(unique(temp_year1$Station)), shareX = TRUE, titleX = FALSE)\n\np2 &lt;- layout(p2,\n                       title = \"Median Daily Mean Temperature Across Weather Stations (2014-2023)\",\n                       xaxis = list(tickangle = 90),\n                       margin = list(b = 80)) # Increase bottom margin to accommodate angled x-axis labels\np2\n\n\n\n\n\nBased on the line graph, we can not say that there the mean temperature across selected weather stations have significant changes from year 2014 to 2023.\n\n\n\nplot_list &lt;- lapply(unique(temp_year1$Station), function(stn) {\n  station_data &lt;- subset(temp_year1, Station == stn)\n  \n  plot_ly(data = station_data, x = ~Year, y = ~median_max_temp, name = stn, type = 'scatter', mode = 'lines',\n          hoverinfo = 'text', text = ~paste(\"Station:\", stn, \"&lt;br&gt;Year:\", Year, \"&lt;br&gt;Temp:\", median_max_temp)) %&gt;%\n    layout(title = paste(\"Median Max Temperature - Station:\", stn),\n           xaxis = list(title = \"Year\"),\n           yaxis = list(title = \"Temperature (°C)\"))\n})\n\np3 &lt;- subplot(plot_list, nrows = length(unique(temp_year1$Station)), shareX = TRUE, titleX = FALSE)\n\np3 &lt;- layout(p3,\n                       title = \"Median Daily Maximum Temperature Across Weather Stations (2014-2023)\",\n                       xaxis = list(tickangle = 90),\n                       margin = list(b = 80)) # Increase bottom margin to accommodate angled x-axis labels\np3\n\n\n\n\n\n\n\n\np4 &lt;- lapply(unique(temp_year1$Station), function(stn) {\n  station_data &lt;- subset(temp_year1, Station == stn)\n  \n  plot_ly(data = station_data, x = ~Year, y = ~median_min_temp, name = stn, type = 'scatter', mode = 'lines',\n          hoverinfo = 'text', text = ~paste(\"Station:\", stn, \"&lt;br&gt;Year:\", Year, \"&lt;br&gt;Temp:\", median_min_temp)) %&gt;%\n    layout(title = paste(\"Median Min Temperature - Station:\", stn),\n           xaxis = list(title = \"Year\"),\n           yaxis = list(title = \"Temperature (°C)\"))\n})\n\np4 &lt;- subplot(p4, nrows = length(unique(temp_year1$Station)), shareX = TRUE, titleX = FALSE)\n\np4 &lt;- layout(p4,\n                       title = \"Median Daily Minimum Temperature Across Weather Stations (2014-2023)\",\n                       xaxis = list(tickangle = 90),\n                       margin = list(b = 80)) # Increase bottom margin to accommodate angled x-axis labels\np4\n\n\n\n\n\n\n\n\nBefore performing statistical test on the significant level, is best to determine how the temperature data is distributed in the data. We can observe the normality of the data using ridgeline plots, using the code chunk below:\n\nNormality Daily Mean TemperatureNormality Daily Max TemperatureNormality Daily Min Temperature\n\n\n\np5 &lt;- ggplot(weather_data_imputed, \n       aes(x = Mean_Temperature, \n           y = as.factor(Year), \n           fill = 0.5 - abs(0.5 - ..ecdf..))) +\n  stat_density_ridges(geom = \"density_ridges_gradient\", \n                      calc_ecdf = TRUE) +\n  scale_fill_viridis_c(name = \"Tail probability\",\n                       direction = -1,\n                       option=\"turbo\") +\n  facet_wrap(~Station, scales = \"free_y\") + \n  theme_ridges(font_size = 12) + # Adjusted for smaller text\n  coord_cartesian(xlim = c(0,50)) +\n  labs(title=\"Distribution of Mean Temperature from 2014 to 2023\",\n       y=\"Station\",\n       x=\"Mean Temperature (°C)\")\n\np5\n\n\n\n\n\n\n\n\nBased on the above observation, as the mean temperature is not normally distributed, non-parametric test will be used.\n\n\n\np6 &lt;- ggplot(weather_data_imputed, \n       aes(x = Max_Temperature, \n           y = as.factor(Year), \n           fill = 0.5 - abs(0.5 - ..ecdf..))) +\n  stat_density_ridges(geom = \"density_ridges_gradient\", \n                      calc_ecdf = TRUE) +\n  scale_fill_viridis_c(name = \"Tail probability\",\n                       direction = -1,\n                       option=\"turbo\")+\n  facet_wrap(~Station, scales = \"free_y\") + \n  theme_ridges(font_size = 12)+\n  coord_cartesian(xlim = c(0,50))+\n  labs(title=\"Distribution of Maximum Temperature from 2014 to 2023\",\n       y=\"Station\",\n       x=\"Maximum Temperature (°C)\")\n\np6\n\n\n\n\n\n\n\n\nBased on the above observation, as the maximum temperature is not normally distributed, non-parametric test will be used.\n\n\n\np8 &lt;- ggplot(weather_data_imputed, \n       aes(x = Min_Temperature, \n           y = as.factor(Year), \n           fill = 0.5 - abs(0.5 - ..ecdf..))) +\n  stat_density_ridges(geom = \"density_ridges_gradient\", \n                      calc_ecdf = TRUE) +\n  scale_fill_viridis_c(name = \"Tail probability\",\n                       direction = -1,\n                       option=\"turbo\")+\n  facet_wrap(~Station, scales = \"free_y\") + \n  theme_ridges(font_size = 12)+\n  coord_cartesian(xlim = c(0,50))+\n  labs(title=\"Distribution of Minimum Temperature from 2014 to 2023\",\n       y=\"Station\",\n       x=\"Minimum Temperature (°C)\")\n\np8\n\n\n\n\n\n\n\n\nBased on the above observation, as the minimum temperature is not normally distributed, non-parametric test will be used.\n\n\n\nDefault Non-Parametric tests temperature different per year:\n\nMedian Mean temperature Per YearMedian Max temperature Per YearMedian Min temperature Per Year\n\n\nThe hypothesis is as follows:\nH0: There is no statistical difference between yearly median mean temperature from 2014-2023.\nH1: There is statistical difference between yearly median mean temperature from 2014-2023.\n\np9 &lt;- ggbetweenstats(\n  data = temp_year1,\n  x = Year, \n  y = median_mean_temp,\n  type = \"np\",\n  messages = FALSE,\n  title=\"Distribution of Yearly Median Mean Temperature from 2014 to 2023\",\n  ylab = \"Temperature (°C)\",\n  xlab = \"Year\",\n  ggsignif.args = list(textsize = 4)\n) +\n  theme(text = element_text(size = 12),plot.title=element_text(size=12))\np9\n\n\n\n\n\n\n\n\nKruskal-Wallis Test: The test has a x^2 value of 19.82 and a p-value of 0.02, which is below the conventional alpha level of 0.05. This suggests that there is statistically significant difference in median maximum temperatures across the years.\n\n\nThe hypothesis is as follows:\nH0: There is no statistical difference between yearly median max temperature from 2014-2023.\nH1: There is statistical difference between yearly median max temperature from 2014-2023.\n\np10 &lt;- ggbetweenstats(\n  data = temp_year1,\n  x = Year, \n  y = median_max_temp,\n  type = \"np\",\n  messages = FALSE,\n  title=\"Distribution of Yearly Median Maximum Temperature from 2014 to 2023\",\n  ylab = \"Temperature (°C)\",\n  xlab = \"Year\",\n  ggsignif.args = list(textsize = 4)\n) +\n  theme(text = element_text(size = 12),plot.title=element_text(size=12))\np10\n\n\n\n\n\n\n\n\nKruskal-Wallis Test: The test has a x^2 value of 10.02 and a p-value of 0.35, which is above the conventional alpha level of 0.05. This suggests that there is no statistically significant difference in median maximum temperatures across the years. But from yearly difference we can further dig into the daily difference to see if there is a statistical difference.\n\n\nThe hypothesis is as follows:\nH0: There is no statistical difference between yearly median min temperature from 2014-2023.\nH1: There is statistical difference between yearly median min temperature from 2014-2023.\n\np11 &lt;- ggbetweenstats(\n  data = temp_year1,\n  x = Year, \n  y = median_min_temp,\n  type = \"np\",\n  messages = FALSE,\n  title=\"Distribution of Yearly Median Minimum Temperature from 2014 to 2023\",\n  ylab = \"Temperature (°C)\",\n  xlab = \"Year\",\n  ggsignif.args = list(textsize = 4)\n) +\n  theme(text = element_text(size = 12),plot.title=element_text(size=12))\np11\n\n\n\n\n\n\n\n\nKruskal-Wallis Test: The test has a x^2 value of 14.72 and a p-value of 0.10, which is above the conventional alpha level of 0.05. This suggests that there is no statistically significant difference in median maximum temperatures across the years. But from yearly difference we can further dig into the daily difference to see if there is a statistical difference.\n\n\n\n\nDaily Mean temperatureDaily Max temperatureDaily Min temperature\n\n\nHypothesis :\nH0: There is no statistical difference in daily mean temperature from 2014-2023.\nH1: There is statistical difference in daily mean temperature from 2014-2023.\n\np12 &lt;- ggbetweenstats(\n  data = weather_data_imputed,\n  x = Year, \n  y = Mean_Temperature,\n  type = \"np\",\n  messages = FALSE,\n  title=\"Distribution of Daily Minimum Temperature from 2014 to 2023\",\n  ylab = \"Temperature (°C)\",\n  xlab = \"Year\",\n  ggsignif.args = list(textsize = 4)\n) +\n  theme(text = element_text(size = 12),plot.title=element_text(size=12))\np12\n\n\n\n\n\n\n\n\n\n\nHypothesis :\nH0: There is no statistical difference in daily max temperature from 2014-2023.\nH1: There is statistical difference in daily max temperature from 2014-2023.\n\np13 &lt;- ggbetweenstats(\n  data = weather_data_imputed,\n  x = Year, \n  y = Max_Temperature,\n  type = \"np\",\n  messages = FALSE,\n  title=\"Distribution of Daily Maximum Temperature from 2014 to 2023\",\n  ylab = \"Temperature (°C)\",\n  xlab = \"Year\",\n  ggsignif.args = list(textsize = 4)\n) +\n  theme(text = element_text(size = 12),plot.title=element_text(size=12))\np13\n\n\n\n\n\n\n\n\n\n\nHypothesis :\nH0: There is no statistical difference in daily min temperature from 2014-2023.\nH1: There is statistical difference in daily min temperature from 2014-2023.\n\np14 &lt;- ggbetweenstats(\n  data = weather_data_imputed,\n  x = Year, \n  y = Min_Temperature,\n  type = \"np\",\n  messages = FALSE,\n  title=\"Distribution of Daily Minimum Temperature from 2014 to 2023\",\n  ylab = \"Temperature (°C)\",\n  xlab = \"Year\",\n  ggsignif.args = list(textsize = 4)\n) +\n  theme(text = element_text(size = 12),plot.title=element_text(size=12))\np14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nAll daily temperatures (mean, maximum and minimum) have p-value lower than 0.05 which means they all shows statistical significant. Meaning there are statistical different in the mean, maximum and minimum daily temperature in Singapore.\n\nEven though the yearly median max temperature and median min temperature appears no statistical significant, but the daily max and min are statistically different.\n\n\n\n\n\n5.2.1.2 Rainfall\nFor rainfall we will be using the daily rainfall total to test the hypothesis\n\nrainfall_year &lt;- weather_data_imputed %&gt;%\n  group_by(Station,Year) %&gt;%\n  summarise(yearly_rainfall = sum(Daily_Rainfall_Total_mm))\n\nDT::datatable(rainfall_year,class = \"compact\")\n\n\n\n\n\n\nwrite_csv(rainfall_year, \"data/rainfall_year.csv\")\n\n\nplot_list &lt;- lapply(unique(rainfall_year$Station), function(stn) {\n  station_data &lt;- subset(rainfall_year, Station == stn)\n  \n  plot_ly(data = station_data, x = ~Year, y = ~yearly_rainfall, name = stn, type = 'scatter', mode = 'lines') %&gt;%\n    layout(title = paste(\"Yearly Rainfall - Station:\", stn),\n           xaxis = list(title = \"Year\", tickangle = 90),\n           yaxis = list(title = \"Rainfall Volume (mm)\"))\n})\n\nfaceted_plot &lt;- subplot(plot_list, nrows = length(unique(rainfall_year$Station)), shareX = TRUE, titleX = FALSE)\n\nfaceted_plot &lt;- layout(faceted_plot,\n                       title = \"Yearly Rainfall Across Weather Stations (2014-2023)\")\nfaceted_plot\n\n\n\n\n\nFrom the observations above, over the pass 10 years from 2014 to 2023 the total rainfall for Singapore captured by different stations indicates there is a volume increase. And every a few years it tend to drop to a low point(2015 and 2019) and will bounce back with even higher volume.\nWe have to check if the different in years of the total rainfall are statistically different, before making any conclusions. But first lets see if the data follows a normal distribution or not in determine the method for test later.\n\np7 &lt;- ggplot(weather_data_imputed, \n       aes(x = Daily_Rainfall_Total_mm, \n           y = as.factor(Year), \n           fill = 0.5 - abs(0.5 - ..ecdf..))) +\n  stat_density_ridges(geom = \"density_ridges_gradient\", \n                      calc_ecdf = TRUE) +\n  scale_fill_viridis_c(name = \"Tail probability\",\n                       direction = -1,\n                       option=\"turbo\")+\n  facet_wrap(~Station) + \n  theme_ridges(font_size = 12)+\n  coord_cartesian(xlim = c(0,50))+\n  labs(title=\"Distribution of Daily Rainfall from 2014 to 2023\",\n       y=\"Station\",\n       x=\"Rainfall Volume (mm)\")\n\np7\n\n\n\n\n\n\n\n\nFrom the distribution graph using sstat function called stat_density_ridges()of ggplot2. We can see that the rainfall distribution is not normally distributed, so non-parametric test will be used.\n\nMedian Rainfall Per YearMedian Daily Rainfall 2014-2023\n\n\nHypothesis:\nH0: There is no statistical difference between median rainfall per year from 2014-2023.\nH1: There is statistical difference between median rainfall per year across 2014-2023.\n\np8 &lt;- ggbetweenstats(\n  data = rainfall_year,\n  x = Year, \n  y = yearly_rainfall,\n  type = \"np\",\n  pairwise.display = \"non-significant\",\n  messages = FALSE,\n  title=\"Distribution of Rainfall from 2014 to 2023\",\n  ylab = \"Rainfall volume (mm)\",\n  xlab = \"Year\",\n  ggsignif.args = list(textsize = 4)\n) +\n  theme(text = element_text(size = 12), plot.title=element_text(size=12))\n\np8\n\n\n\n\n\n\n\n\n\n# Filter the data for a specific year\nrainfall_year_filtered &lt;- rainfall_year %&gt;%\n  filter(Year &gt;= 2014, Year &lt;= 2023)\n\n# Create the plotly violin plot\np8_plotly &lt;- plot_ly(data = rainfall_year_filtered,\n                     x = ~Year,\n                     y = ~yearly_rainfall,\n                     type = 'violin',\n                     spanmode = 'hard',\n                     marker = list(opacity = 0.5, line = list(width = 2)),\n                     box = list(visible = T),\n                     points = 'all',\n                     scalemode = 'count',\n                     meanline = list(visible = T, color = \"red\"),\n                     color = I('#caced8'),\n                     marker = list(line = list(width = 2, color = '#caced8'))\n                    ) %&gt;%\n  layout(title = \"Distribution of Rainfall from 2014 to 2023\",\n         yaxis = list(title = \"Rainfall volume (mm)\"),\n         xaxis = list(title = \"Year\"))\n\n# Show the plot\np8_plotly\n\n\n\n\n\nKruskal-Wallis test result at the top indicates a significant difference in rainfall distribution across the years (p-value &lt; 0.01), suggesting that at least one year has a statistically different total rainfall volume compared to the others.\nfrom the lines connecting the years we can observe that some years trend towards significance when the pHolm-adj is less than 1.00. Hence we can reject the null hypothesis and say that the rainfall over the years is statistically significant.\n\n\nHypothesis:\nH0: There is no statistical difference between median daily rainfall from 2014-2023.\nH1: There is statistical difference between median daily rainfall from 2014-2023.\n\np9 &lt;- ggbetweenstats(\n  data = weather_data_imputed,\n  x = Year, \n  y = Daily_Rainfall_Total_mm,\n  type = \"np\",\n  pairwise.display = \"non-significant\",\n  messages = FALSE,\n  title=\"Distribution of Rainfall from 2014 to 2023\",\n  ylab = \"Rainfall volume (mm)\",\n  xlab = \"Year\",\n  ggsignif.args = list(textsize = 4)\n) +\n  theme(text = element_text(size = 12), plot.title=element_text(size=12))\n\np9\n\n\n\n\n\n\n\n\nWe can dig further into the daily rainfall total to see if there is statistical significant. After undergo the test above, we can observe the test result that overall Kruskal-Wallis test is highly significant (p=2.84e−89), indicating there are differences in the distributions of daily rainfall volumes across the years.\nWe can also observe that some years the median daily rainfall is 0.00 or 0.20 (2014, 2015, 2019, etc.) suggesting low rainfall volume.\n\n\n\n\n\n\n5.2.2 Can We Clearly Identify the ‘Dry’ and ‘Wet’ Month?\nSingapore which is a tropical country, means have no clear identification of the four seasons, but there are certain months which the ‘cooler’ compare to some months. In this section we will be testing the hypothesis on ‘can we clearly identify the ’Dry’ and ‘Wet’ Month’?\nFirst we need to filter the data to get monthly records, code chunk below:\n\nrf_data_month &lt;- weather_data_imputed %&gt;%\n  group_by(Year,Month) %&gt;%\n  summarise(monthly_rainfall = sum(Daily_Rainfall_Total_mm))\n\nrf_data_month$Year &lt;- factor(rf_data_month$Year)\nrf_data_month$Month &lt;- factor(rf_data_month$Month, levels = as.character(1:12))\n\nDT::datatable(rf_data_month,class = \"compact\")\n\n\n\n\n\n\nglimpse(rf_data_month)\n\nRows: 120\nColumns: 3\nGroups: Year [10]\n$ Year             &lt;fct&gt; 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014,…\n$ Month            &lt;fct&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2, 3, 4, 5,…\n$ monthly_rainfall &lt;dbl&gt; 450.6, 124.0, 728.1, 1296.2, 1621.8, 1026.9, 1070.8, …\n\n\n\ntemp_month &lt;- weather_data_imputed %&gt;%\n  group_by(Year,Month) %&gt;%\n  summarise(median_mean_temp = median(Mean_Temperature),\n            median_max_temp = median(Max_Temperature),\n            median_min_temp = median(Min_Temperature))\n\nDT::datatable(temp_month,class = \"compact\")\n\n\n\n\n\nTo see the distribution of the monthly rainfall and Temperature, code chunk below:\n\nDistribution of Monthly RainfallDistribution of Monthly Mean TemperatureDistribution of Monthly Max TemperatureDistribution of Monthly Min Temperature\n\n\n\ncolor_palette &lt;- brewer.pal(\"Set3\", n = length(unique(rf_data_month$Year)))\n\np18 &lt;- ggplot(rf_data_month, aes(x = Month, y = monthly_rainfall, fill = Year)) +\n  geom_bar(stat = \"identity\") +\n  facet_wrap(~Year, scales = \"free_x\") +\n  labs(title = \"Monthly Rainfall From Year 2014 to 2023\",\n       y = \"Rainfall volume (mm)\",\n       x = \"Month\") +\n  theme_minimal() +\n  scale_fill_manual(values = color_palette) +\n  theme(panel.spacing.y = unit(0.5, \"lines\")) # Adjusted for less spacing\n\np18\n\n\n\n\n\n\n\n\nFrom the distribution of monthly rainfall from year 2014 to 2023, we can observe that the data is not normally distributed, hence non-parametric test will be used later.\n\n\n\np19 &lt;- ggplot(temp_month,\n       aes(y = median_mean_temp,\n           x = Month,\n           colour = Year)) +\n  geom_line(size = 1.5)+\n  facet_wrap(~Year, scales = \"free_x\") +\n  labs(title=\"Monthly mean Temperature From 2014 to 2023\",\n       y = \"Temperature (°C)\",\n       x = \"Month\")+\n  coord_cartesian(ylim = c(20,35))+\n  scale_x_discrete(limits = 1:12) + # Assuming Month is numeric already\n  theme_minimal() +\n  theme(panel.spacing.y = unit(0.5,\"lines\"))\n\np19\n\n\n\n\n\n\n\n\nFrom the distribution of monthly mean temperature from year 2014 to 2023, we can observe that the data is not normally distributed, hence non-parametric test will be used later.\n\n\n\np20 &lt;- ggplot(temp_month,\n       aes(y = median_max_temp,\n           x = Month,\n           colour = Year)) +\n  geom_line(size = 1.5)+\n  facet_wrap(~Year, scales = \"free_x\") +\n  labs(title=\"Monthly Maximum Temperature From 2014 to 2023\",\n       y = \"Temperature (°C)\",\n       x = \"Month\")+\n  coord_cartesian(ylim = c(20,35))+\n  scale_x_discrete(limits = 1:12) + # Assuming Month is numeric already\n  theme_minimal() +\n  theme(panel.spacing.y = unit(0.5,\"lines\"))\n\np20\n\n\n\n\n\n\n\n\nFrom the distribution of monthly maximum temperature from year 2014 to 2023, we can observe that the data is not normally distributed, hence non-parametric test will be used later.\n\n\n\np21 &lt;- ggplot(temp_month,\n       aes(y = median_min_temp,\n           x = Month,\n           colour = Year)) +\n  geom_line(size = 1.5)+\n  facet_wrap(~Year, scales = \"free_x\") +\n  labs(title=\"Monthly Minimum Temperature From 2014 to 2023\",\n       y = \"Temperature (°C)\",\n       x = \"Month\")+\n  coord_cartesian(ylim = c(20,35))+\n  scale_x_discrete(limits = 1:12) + # Assuming Month is numeric already\n  theme_minimal() +\n  theme(panel.spacing.y = unit(0.5,\"lines\"))\n\np21\n\n\n\n\n\n\n\n\nFrom the distribution of monthly minimum temperature from year 2014 to 2023, we can observe that the data is not normally distributed, hence non-parametric test will be used later.\n\n\n\n\n5.2.2.1 Hypothesis testing\nTo test Monthly Rainfall From Year 2014 to 2023:\nThe hypothesis is as follows:\nH0: There is no statistical difference between minimum temperature across months.\nH1: There is statistical difference between minimum temperature across months.\n\nMonthly Rainfall over yearsMonthly Mean TemperatureMonthly Maximum TemperatureMonthly Minimum Temperature\n\n\nHypothesis:\nH0: There is no statistical difference in rainfall volume across months.\nH1: There is statistical difference in rainfall volume across months.\n\np22 &lt;- ggbetweenstats(\n  data = rf_data_month,\n  x = Month, \n  y = monthly_rainfall,\n  type = \"np\",\n  messages = FALSE,\n  title=\"Distribution of Rainfall across months 2014 to 2023\",\n  ylab = \"Rainfall volume (mm)\",\n  xlab = \"Month\",\n  ggsignif.args = list(textsize = 4)\n) +\n  theme(text = element_text(size = 12), plot.title=element_text(size=12))\np22\n\n\n\n\n\n\n\n\nAt CI of 95%, the Kruskal-Wallis test results give a p-value &lt; 0.05, which indicates there is statistical difference in the rainfall volume across different month in Singapore.\n\nFeb has significant different in rainfall volume comparing with Nov and Dec.\n2, 3, 7, 8, 9, 10 can consider ‘Dry’ as median rainfall volume lower or equal to around 1000mm per month.\n4, 5, 6, 11, 12 can consider a ‘Wet’ as median rainfall volume mainly higher than 1400mm per month.\n\n\n\nHypothesis:\nH0: There is no statistical difference between mean temperature across months.\nH1: There is statistical difference between mean temperature across months.\n\np23 &lt;- ggbetweenstats(data = temp_month,\n                      x = Month,\n                      y = median_mean_temp,\n                      type = \"np\",\n                      messages = FALSE,\n                      title = \"Distribution of Mean Temperature by month from 2014 to 2023\",\n                      ylab = \"Temperature (C)\",\n                      xlab = \"Month\",\n                      ggsignif.args = list(textsize =4)) +\n  theme(text = element_text(size = 11),plot.title = element_text(size = 11))\np23\n\n\n\n\n\n\n\n\nAt CI of 95%, the Kruskal-Wallis test results give a p-value &lt; 0.05, which indicates there is statistical difference in the mean temperature across different month in Singapore.\n\nIt is observed that towards the middle of the 12 months, 5, 6, 7 ,8 ,9 has the highest median mean temperature .\nWhile months 1, 2, 11, 12 has the lowest median mean temperature.\n\n\n\nHypothesis:\nH0: There is no statistical difference between maximum temperature across months.\nH1: There is statistical difference between maximum temperature across months.\n\np24 &lt;- ggbetweenstats(data = temp_month,\n                      x = Month,\n                      y = median_max_temp,\n                      type = \"np\",\n                      messages = FALSE,\n                      title = \"Distribution of Maximum Temperature by month from 2014 to 2023\",\n                      ylab = \"Temperature (C)\",\n                      xlab = \"Month\",\n                      ggsignif.args = list(textsize =4)) +\n  theme(text = element_text(size = 11),plot.title = element_text(size = 11))\np24\n\n\n\n\n\n\n\n\nAt CI of 95%, the Kruskal-Wallis test results give a p-value &lt; 0.05, which indicates there is statistical difference in the maximum temperature across different month in Singapore.\n\nThe month with highest daily temperature is month 3, 4, 5 which average maximum temperature more than 32 Degree Celsius.\nMonth with lowest daily temperature is January, with only average maximum temperature of 30.85 Degree Celsius.\n\n\n\nHypothesis:\nH0: There is no statistical difference between minimum temperature across months.\nH1: There is statistical difference between minimum temperature across months.\n\np25 &lt;- ggbetweenstats(data = temp_month,\n                      x = Month,\n                      y = median_min_temp,\n                      type = \"np\",\n                      messages = FALSE,\n                      title = \"Distribution of Minimum Temperature by month from 2014 to 2023\",\n                      ylab = \"Temperature (C)\",\n                      xlab = \"Month\",\n                      ggsignif.args = list(textsize =4)) +\n  theme(text = element_text(size = 11),plot.title = element_text(size = 11))\np25\n\n\n\n\n\n\n\n\nAt CI of 95%, the Kruskal-Wallis test results give a p-value &lt; 0.05, which indicates there is statistical difference in the minimum temperature across different month in Singapore.\n\n\n\nFrom the hypothesis testing, result indicates that we can clearly identify the ‘Dry’ or ‘Wet’ months, and also the ‘Hot’ and ‘Cool’ months from the date used.\nFrom the hypothesis testing, several result can be shown:\n\nDry Month: 2, 3, 7, 8, 9\nWet Month :1, 6, 11, 12\nHot Month: 3, 4, 5, 9, 10\nCool Month: 1, 2, 11, 12\nDry & Hot : 3, 9\nWet & Cool : 1, 11, 12\n\nBy identifying this can help Singapore Government to develop strategies to deal with different situations, and also to see the trend in future if there is shift in different type of month"
  }
]